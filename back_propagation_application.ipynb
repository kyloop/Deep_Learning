{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Kai Chung, Ying\n",
    "\n",
    "Class: CSC 578\n",
    "\n",
    "Section: 710 - 15961\n",
    "\n",
    "Assignment #2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 578 HW#2 Support code (more added on 10/13/2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import NN578_network2 as network2\n",
    "import numpy as np\n",
    "\n",
    "# Load the iris train-test (separate) data files\n",
    "def my_load_csv(fname, no_trainfeatures, no_testfeatures):\n",
    "    ret = np.genfromtxt(fname, delimiter=',')\n",
    "    data = np.array([(entry[:no_trainfeatures],entry[no_trainfeatures:]) for entry in ret])\n",
    "    temp_inputs = [np.reshape(x, (no_trainfeatures, 1)) for x in data[:,0]]\n",
    "    temp_results = [np.reshape(y, (no_testfeatures, 1)) for y in data[:,1]]\n",
    "    dataset = list(zip(temp_inputs, temp_results))\n",
    "    return dataset\n",
    "\n",
    "iris_train = my_load_csv('./data/iris-train-1.csv', 4, 3)\n",
    "iris_test = my_load_csv('./data/iris-test-1.csv', 4, 3)\n",
    "#iris_train = my_load_csv('./data/iris.csv', 4, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Small experiment with One-training-One-test Iris data\n",
    "\n",
    "### [You can run the start-up code as is to generate these.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.sizes [4, 2, 3]\n",
      "Epoch 0 training complete\n",
      "Cost on training data: 0.14585442495280249\n",
      "Accuracy on training data: 1 / 1\n",
      "Cost on evaluation data: 1.5242727719037343\n",
      "Accuracy on evaluation data: 0 / 1\n",
      "\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 0.27034549594460733\n",
      "Accuracy on training data: 1 / 1\n",
      "Cost on evaluation data: 0.7432847742145561\n",
      "Accuracy on evaluation data: 0 / 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1.5242727719037343, 0.7432847742145561],\n",
       " [0, 0],\n",
       " [0.14585442495280249, 0.27034549594460733],\n",
       " [1, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import NN578_network2 as network2\n",
    "import numpy as np\n",
    "\n",
    "# Test with one-data Iris data\n",
    "\n",
    "inst1 = (np.array([5.7, 3, 4.2, 1.2]), np.array([0., 1., 0.]))\n",
    "x1 = np.reshape(inst1[0], (4, 1))\n",
    "y1 = np.reshape(inst1[1], (3, 1))\n",
    "sample1 = [(x1, y1)]\n",
    "inst2 = (np.array([4.8, 3.4, 1.6, 0.2]), np.array([1., 0., 0.]))\n",
    "x2 = np.reshape(inst2[0], (4, 1))\n",
    "y2 = np.reshape(inst2[1], (3, 1))\n",
    "sample2 = [(x2, y2)]\n",
    "\n",
    "net4 = network2.load_network(\"./data/iris-423.dat\")\n",
    "net4.set_parameters(cost=network2.QuadraticCost,act_output=network2.Tanh, act_hidden=network2.Tanh)\n",
    "\n",
    "\n",
    "net4.SGD(sample1, 2, 1, 1.0, \n",
    "         evaluation_data=sample2, \n",
    "         monitor_evaluation_cost=True, \n",
    "         monitor_evaluation_accuracy=True,\n",
    "         monitor_training_cost=True,\n",
    "         monitor_training_accuracy=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Experiments with Iris train-test-1 datasets, with random-shuffle suppressed training\n",
    "##   (Test1) Sigmoid hidden + Sigmoid Output +  QuadraticCost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.sizes [4, 2, 3]\n",
      "Epoch 0 training complete\n",
      "Cost on training data: 0.23511052948021774\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.23887527012577628\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 0.18455113694871086\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.19259471015802\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 0.1746888670684831\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.1835496893958853\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 0.17088787461067578\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.17937927946978807\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 0.16990642674912537\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.17822748490175794\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 0.16952151503854623\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.1778135860232122\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 0.16359259470352616\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.1706324018202625\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 0.16617108130713076\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.17390190909821354\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 0.16877864308913074\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.17715961328304014\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 0.16812677240211796\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.17636324096051592\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net2 = network2.load_network(\"./data/iris-423.dat\")\n",
    "\n",
    "# Set hyper-parameter values individually after the network\n",
    "net2.set_parameters(cost=network2.QuadraticCost,\n",
    "                    act_hidden = network2.Sigmoid,\n",
    "                    act_output = network2.Sigmoid,\n",
    "                    regularization=None, \n",
    "                    dropoutpercent=0.0)\n",
    "\n",
    "evaluation_cost1, evaluation_accuracy1, training_cost1, training_accuracy1 = net2.SGD(iris_train, 10, 1, 1.0,\n",
    "                             lmbda=0.0,\n",
    "                             evaluation_data=iris_test, \n",
    "                             monitor_evaluation_cost=True, \n",
    "                             monitor_evaluation_accuracy=True,\n",
    "                             monitor_training_cost=True,\n",
    "                             monitor_training_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Test2) Sigmoid hidden + Sogmoid output + CrossEntropyCost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.sizes [4, 2, 3]\n",
      "Epoch 0 training complete\n",
      "Cost on training data: 1.9263678377944575\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.9432631693053442\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 1.926529576230227\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.9436026308833534\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 1.9265258643832714\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.9435835110281197\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 1.9265258643832475\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.9435835110280708\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 1.9265258643832308\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.9435835110280477\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 1.9265258643832144\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.9435835110280253\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 1.9265258643831964\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.9435835110280022\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 1.9265258643831784\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.9435835110279793\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 1.926525864383162\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.943583511027956\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 1.9265258643831449\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.9435835110279336\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net2 = network2.load_network(\"./data/iris-423.dat\")\n",
    "\n",
    "# Set hyper-parameter values individually after the network\n",
    "net2.set_parameters(cost=network2.CrossEntropyCost,\n",
    "                    act_hidden = network2.Sigmoid,\n",
    "                    act_output = network2.Sigmoid,\n",
    "                    regularization=None, \n",
    "                    dropoutpercent=0.0)\n",
    "\n",
    "evaluation_cost2, evaluation_accuracy2, training_cost2, training_accuracy2 = net2.SGD(iris_train, 10, 1, 1.0,\n",
    "         lmbda=0.0,\n",
    "         evaluation_data=iris_test, \n",
    "         monitor_evaluation_cost=True, \n",
    "         monitor_evaluation_accuracy=True,\n",
    "         monitor_training_cost=True,\n",
    "         monitor_training_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Test3) ReLU hidden + Softmax output + CrossEntropyCost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.sizes [4, 2, 3]\n",
      "Cost Function is overridded to LogLikelihood because act_output is set as Softmax function\n",
      "Epoch 0 training complete\n",
      "Cost on training data: 1.1669307963212423\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.184405803283747\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net2 = network2.load_network(\"./data/iris-423.dat\")\n",
    "\n",
    "# Set hyper-parameter values individually after the network\n",
    "net2.set_parameters(cost=network2.CrossEntropyCost,\n",
    "                    act_hidden = network2.ReLU,\n",
    "                    act_output = network2.Softmax,\n",
    "                    regularization=None, \n",
    "                    dropoutpercent=0.0)\n",
    "\n",
    "evaluation_cost3, evaluation_accuracy3, training_cost3, training_accuracy3 = net2.SGD(iris_train, 10, 1, 1.0,\n",
    "         lmbda=0.0,\n",
    "         evaluation_data=iris_test, \n",
    "         monitor_evaluation_cost=True, \n",
    "         monitor_evaluation_accuracy=True,\n",
    "         monitor_training_cost=True,\n",
    "         monitor_training_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2-4) ReLU hidden + Softmax output + LogLikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.sizes [4, 2, 3]\n",
      "Epoch 0 training complete\n",
      "Cost on training data: 1.1669307963212423\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.184405803283747\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net2 = network2.load_network(\"./data/iris-423.dat\")\n",
    "\n",
    "# Set hyper-parameter values individually after the network\n",
    "net2.set_parameters(cost=network2.LogLikelihood,\n",
    "                    act_hidden = network2.ReLU,\n",
    "                    act_output = network2.Softmax,\n",
    "                    regularization=None, \n",
    "                    dropoutpercent=0.0)\n",
    "\n",
    "evaluation_cost4, evaluation_accuracy4, training_cost4, training_accuracy4 = net2.SGD(iris_train, 10, 1, 1.0,\n",
    "         lmbda=0.0,\n",
    "         evaluation_data=iris_test, \n",
    "         monitor_evaluation_cost=True, \n",
    "         monitor_evaluation_accuracy=True,\n",
    "         monitor_training_cost=True,\n",
    "         monitor_training_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Test5) Tanh hidden + Tanh output + CrossEntropyCost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.sizes [4, 2, 3]\n",
      "Activation function of Output Layer is overrided to Sigmoid function\n",
      "Epoch 0 training complete\n",
      "Cost on training data: 2.4626489005442567\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 2.5399319341564066\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 2.4626597971114186\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 2.5399444183983864\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 2.4626645293839027\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 2.539949744825968\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 2.4626672431713525\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 2.539952771327394\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 2.462669025882277\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 2.539954747177492\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 2.4626702965400593\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 2.5399561489584004\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 2.4626712531651287\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 2.539957200384023\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 2.462672002263134\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 2.539958021176445\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 2.462672606524668\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 2.5399586815242463\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 2.462673105397776\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 2.5399592254473013\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net2 = network2.load_network(\"./data/iris-423.dat\")\n",
    "\n",
    "# Set hyper-parameter values individually after the network\n",
    "net2.set_parameters(cost=network2.CrossEntropyCost,\n",
    "                    act_hidden = network2.Tanh,\n",
    "                    act_output = network2.Tanh,\n",
    "                    regularization=None, \n",
    "                    dropoutpercent=0.0)\n",
    "\n",
    "evaluation_cost5, evaluation_accuracy5, training_cost5, training_accuracy5 =net2.SGD(iris_train, 10, 1, 1.0,\n",
    "         lmbda=0.0,\n",
    "         evaluation_data=iris_test, \n",
    "         monitor_evaluation_cost=True, \n",
    "         monitor_evaluation_accuracy=True,\n",
    "         monitor_training_cost=True,\n",
    "         monitor_training_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancelled (2-6) ReLU hidden + Softmax output + CrossEntropyCost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.sizes [4, 2, 3]\n",
      "Cost Function is overridded to LogLikelihood because act_output is set as Softmax function\n",
      "Epoch 0 training complete\n",
      "Cost on training data: 1.1669307963212423\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.184405803283747\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1.184405803283747,\n",
       "  1.1844058032833626,\n",
       "  1.1844058032833626,\n",
       "  1.1844058032833626,\n",
       "  1.1844058032833626,\n",
       "  1.1844058032833626,\n",
       "  1.1844058032833626,\n",
       "  1.1844058032833626,\n",
       "  1.1844058032833626,\n",
       "  1.1844058032833626],\n",
       " [17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
       " [1.1669307963212423,\n",
       "  1.1669307963209965,\n",
       "  1.1669307963209965,\n",
       "  1.1669307963209965,\n",
       "  1.1669307963209965,\n",
       "  1.1669307963209965,\n",
       "  1.1669307963209965,\n",
       "  1.1669307963209965,\n",
       "  1.1669307963209965,\n",
       "  1.1669307963209965],\n",
       " [33, 33, 33, 33, 33, 33, 33, 33, 33, 33])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2 = network2.load_network(\"./data/iris-423.dat\")\n",
    "\n",
    "# Set hyper-parameter values individually after the network\n",
    "net2.set_parameters(cost=network2.CrossEntropyCost,\n",
    "                    act_hidden = network2.ReLU,\n",
    "                    act_output = network2.Softmax,\n",
    "                    regularization=None, \n",
    "                    dropoutpercent=0.0)\n",
    "\n",
    "net2.SGD(iris_train, 10, 1, 1.0,\n",
    "         lmbda=0.0,\n",
    "         evaluation_data=iris_test, \n",
    "         monitor_evaluation_cost=True, \n",
    "         monitor_evaluation_accuracy=True,\n",
    "         monitor_training_cost=True,\n",
    "         monitor_training_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2-7) ReLU hidden + Softmax output + LogLikelihood + L2 + lmbda=3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.sizes [4, 2, 3]\n",
      "Epoch 0 training complete\n",
      "Cost on training data: 1.2515777525067482\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.3306141821496207\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 1.1671212605998658\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1847347870377731\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 1.1669312248850365\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.184406543530341\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 1.1669307972853094\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058049489938\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 1.1669307963231663\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032871105\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 1.1669307963210014\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.184405803283371\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net2 = network2.load_network(\"./data/iris-423.dat\")\n",
    "\n",
    "# Set hyper-parameter values individually after the network\n",
    "net2.set_parameters(cost=network2.LogLikelihood,\n",
    "                    act_hidden = network2.ReLU,\n",
    "                    act_output = network2.Softmax,\n",
    "                    regularization=\"L2\", \n",
    "                    dropoutpercent=0.0)\n",
    "\n",
    "evaluation_cost7, evaluation_accuracy7, training_cost7, training_accuracy7 = net2.SGD(iris_train, 10, 1, 1.0,\n",
    "         lmbda=3.0,\n",
    "         evaluation_data=iris_test, \n",
    "         monitor_evaluation_cost=True, \n",
    "         monitor_evaluation_accuracy=True,\n",
    "         monitor_training_cost=True,\n",
    "         monitor_training_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2-8) ) ReLU hidden + Softmax output + LogLikelihood + L1 + lmbda=3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.sizes [4, 2, 3]\n",
      "Epoch 0 training complete\n",
      "Cost on training data: 3.657026899941137\n",
      "Accuracy on training data: 30 / 95\n",
      "Cost on evaluation data: 5.475927838541125\n",
      "Accuracy on evaluation data: 20 / 55\n",
      "\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 3.0548493895376785\n",
      "Accuracy on training data: 30 / 95\n",
      "Cost on evaluation data: 4.435803047844241\n",
      "Accuracy on evaluation data: 20 / 55\n",
      "\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 2.608402325715704\n",
      "Accuracy on training data: 30 / 95\n",
      "Cost on evaluation data: 3.6646672103335582\n",
      "Accuracy on evaluation data: 20 / 55\n",
      "\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 2.2633854682574435\n",
      "Accuracy on training data: 30 / 95\n",
      "Cost on evaluation data: 3.068729001996563\n",
      "Accuracy on evaluation data: 20 / 55\n",
      "\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 1.9806191822207226\n",
      "Accuracy on training data: 30 / 95\n",
      "Cost on evaluation data: 2.580314507933136\n",
      "Accuracy on evaluation data: 20 / 55\n",
      "\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 1.7573298493040912\n",
      "Accuracy on training data: 30 / 95\n",
      "Cost on evaluation data: 2.194632932895318\n",
      "Accuracy on evaluation data: 20 / 55\n",
      "\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 1.5697604564589458\n",
      "Accuracy on training data: 30 / 95\n",
      "Cost on evaluation data: 1.870649436162794\n",
      "Accuracy on evaluation data: 20 / 55\n",
      "\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 1.3783824808830385\n",
      "Accuracy on training data: 30 / 95\n",
      "Cost on evaluation data: 1.5400874783498635\n",
      "Accuracy on evaluation data: 20 / 55\n",
      "\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 1.2177031129146834\n",
      "Accuracy on training data: 30 / 95\n",
      "Cost on evaluation data: 1.2625503882227047\n",
      "Accuracy on evaluation data: 20 / 55\n",
      "\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 1.133063921326252\n",
      "Accuracy on training data: 30 / 95\n",
      "Cost on evaluation data: 1.1163554209335957\n",
      "Accuracy on evaluation data: 20 / 55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net2 = network2.load_network(\"./data/iris-423.dat\")\n",
    "\n",
    "# Set hyper-parameter values individually after the network\n",
    "net2.set_parameters(cost=network2.LogLikelihood,\n",
    "                    act_hidden = network2.ReLU,\n",
    "                    act_output = network2.Softmax,\n",
    "                    regularization=\"L1\", \n",
    "                    dropoutpercent=0.0)\n",
    "\n",
    "evaluation_cost8, evaluation_accuracy8, training_cost8, training_accuracy8 = net2.SGD(iris_train, 10, 1, 1.0,\n",
    "         lmbda=3.0,\n",
    "         evaluation_data=iris_test, \n",
    "         monitor_evaluation_cost=True, \n",
    "         monitor_evaluation_accuracy=True,\n",
    "         monitor_training_cost=True,\n",
    "         monitor_training_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Test9) ReLU hidden + Softmax output + LogLikelihood  + dropout=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.sizes [4, 20, 7, 3]\n",
      "Epoch 0 training complete\n",
      "Cost on training data: 1.1669307963234508\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032871783\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net2 = network2.load_network(\"./data/iris-4-20-7-3.dat\")\n",
    "\n",
    "# Set hyper-parameter values individually after the network\n",
    "net2.set_parameters(cost=network2.LogLikelihood,\n",
    "                    act_hidden = network2.ReLU,\n",
    "                    act_output = network2.Softmax,\n",
    "                    regularization=None, \n",
    "                    dropoutpercent=0.1)\n",
    "\n",
    "evaluation_cost9, evaluation_accuracy9, training_cost9, training_accuracy9 = net2.SGD(iris_train, 10, 1, 1.0,\n",
    "         lmbda=0.0,\n",
    "         evaluation_data=iris_test, \n",
    "         monitor_evaluation_cost=True, \n",
    "         monitor_evaluation_accuracy=True,\n",
    "         monitor_training_cost=True,\n",
    "         monitor_training_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2-10) ReLU hidden + Softmax output + LogLikelihood + dropout=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.sizes [4, 20, 7, 3]\n",
      "Epoch 0 training complete\n",
      "Cost on training data: 1.1669307963490136\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058033265357\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net2 = network2.load_network(\"./data/iris-4-20-7-3.dat\")\n",
    "\n",
    "# Set hyper-parameter values individually after the network\n",
    "net2.set_parameters(cost=network2.LogLikelihood,\n",
    "                    act_hidden = network2.ReLU,\n",
    "                    act_output = network2.Softmax,\n",
    "                    regularization=None, \n",
    "                    dropoutpercent=0.5)\n",
    "\n",
    "evaluation_cost10, evaluation_accuracy10, training_cost10, training_accuracy10 =net2.SGD(iris_train, 10, 1, 1.0,\n",
    "         lmbda=0.0,\n",
    "         evaluation_data=iris_test, \n",
    "         monitor_evaluation_cost=True, \n",
    "         monitor_evaluation_accuracy=True,\n",
    "         monitor_training_cost=True,\n",
    "         monitor_training_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2-11) ReLU hidden + Softmax output + LogLikelihood + L2+ lmbda=3.0 +dropout=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.sizes [4, 20, 7, 3]\n",
      "Epoch 0 training complete\n",
      "Cost on training data: 1.3704736335191618\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.535979794807281\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 1.1673887884503387\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1851968805976811\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 1.1669318268499673\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844075832879486\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 1.1669307986397917\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058072885544\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 1.166930796326214\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032923748\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 1.1669307963210083\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833829\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net2 = network2.load_network(\"./data/iris-4-20-7-3.dat\")\n",
    "\n",
    "# Set hyper-parameter values individually after the network\n",
    "net2.set_parameters(cost=network2.LogLikelihood,\n",
    "                    act_hidden = network2.ReLU,\n",
    "                    act_output = network2.Softmax,\n",
    "                    regularization=\"L2\", \n",
    "                    dropoutpercent=0.1)\n",
    "\n",
    "evaluation_cost11, evaluation_accuracy11, training_cost11, training_accuracy11 = net2.SGD(iris_train, 10, 1, 1.0,\n",
    "         lmbda=3.0,\n",
    "         evaluation_data=iris_test, \n",
    "         monitor_evaluation_cost=True, \n",
    "         monitor_evaluation_accuracy=True,\n",
    "         monitor_training_cost=True,\n",
    "         monitor_training_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2-12) ReLU hidden + Softmax output + LogLikelihood + L2+ lmbda=3.0 +dropout=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.sizes [4, 20, 7, 3]\n",
      "Epoch 0 training complete\n",
      "Cost on training data: 33.82506538678303\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 57.59391099171434\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 1.2404149275975764\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.3113329391247281\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 1.1670961431060747\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1846914022757704\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 1.1669311683681156\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844064459102046\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 1.1669307971581404\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058047293384\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 1.16693079632288\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032866163\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 1.1669307963210007\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.18440580328337\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 1.1669307963209965\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1844058032833626\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net2 = network2.load_network(\"./data/iris-4-20-7-3.dat\")\n",
    "\n",
    "# Set hyper-parameter values individually after the network\n",
    "net2.set_parameters(cost=network2.LogLikelihood,\n",
    "                    act_hidden = network2.ReLU,\n",
    "                    act_output = network2.Softmax,\n",
    "                    regularization=\"L2\", \n",
    "                    dropoutpercent=0.5)\n",
    "\n",
    "evaluation_cost12, evaluation_accuracy12, training_cost12, training_accuracy12 = net2.SGD(iris_train, 10, 1, 1.0,\n",
    "         lmbda=3.0,\n",
    "         evaluation_data=iris_test, \n",
    "         monitor_evaluation_cost=True, \n",
    "         monitor_evaluation_accuracy=True,\n",
    "         monitor_training_cost=True,\n",
    "         monitor_training_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from plotly import __version__\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "#Set plotly offline\n",
    "init_notebook_mode(connected=True)\n",
    "print (__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_lst=list(np.linspace(0,9,10, dtype=int))\n",
    "epoch_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "lines",
         "name": "Quadratic",
         "type": "scatter",
         "uid": "6240ea86-d347-11e8-8707-a820660c6deb",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9
         ],
         "y": [
          0.23511052948021774,
          0.18455113694871086,
          0.1746888670684831,
          0.17088787461067578,
          0.16990642674912537,
          0.16952151503854623,
          0.16359259470352616,
          0.16617108130713076,
          0.16877864308913074,
          0.16812677240211796
         ]
        },
        {
         "mode": "lines",
         "name": "CrossEntropy",
         "type": "scatter",
         "uid": "62422fe2-d347-11e8-ad12-a820660c6deb",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9
         ],
         "y": [
          1.9263678377944575,
          1.926529576230227,
          1.9265258643832714,
          1.9265258643832475,
          1.9265258643832308,
          1.9265258643832144,
          1.9265258643831964,
          1.9265258643831784,
          1.926525864383162,
          1.9265258643831449
         ]
        }
       ],
       "layout": {
        "title": "Cost: Hidden: Sigmoid / Output: Sigmoid / Network:4-2-3 (Train)",
        "xaxis": {
         "title": "Epoch"
        },
        "yaxis": {
         "range": [
          0,
          2
         ],
         "title": "Cost"
        }
       }
      },
      "text/html": [
       "<div id=\"d60e448a-72de-4581-979d-943cafcf5a16\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "        Plotly.plot(\n",
       "            'd60e448a-72de-4581-979d-943cafcf5a16',\n",
       "            [{\"mode\": \"lines\", \"name\": \"Quadratic\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [0.23511052948021774, 0.18455113694871086, 0.1746888670684831, 0.17088787461067578, 0.16990642674912537, 0.16952151503854623, 0.16359259470352616, 0.16617108130713076, 0.16877864308913074, 0.16812677240211796], \"type\": \"scatter\", \"uid\": \"6240ea86-d347-11e8-8707-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"CrossEntropy\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [1.9263678377944575, 1.926529576230227, 1.9265258643832714, 1.9265258643832475, 1.9265258643832308, 1.9265258643832144, 1.9265258643831964, 1.9265258643831784, 1.926525864383162, 1.9265258643831449], \"type\": \"scatter\", \"uid\": \"62422fe2-d347-11e8-ad12-a820660c6deb\"}],\n",
       "            {\"title\": \"Cost: Hidden: Sigmoid / Output: Sigmoid / Network:4-2-3 (Train)\", \"xaxis\": {\"title\": \"Epoch\"}, \"yaxis\": {\"range\": [0, 2], \"title\": \"Cost\"}},\n",
       "            {\"showLink\": true, \"linkText\": \"Export to plot.ly\"}\n",
       "        ).then(function () {return Plotly.addFrames('d60e448a-72de-4581-979d-943cafcf5a16',{});}).then(function(){Plotly.animate('d60e448a-72de-4581-979d-943cafcf5a16');})\n",
       "        });</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"d60e448a-72de-4581-979d-943cafcf5a16\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "        Plotly.plot(\n",
       "            'd60e448a-72de-4581-979d-943cafcf5a16',\n",
       "            [{\"mode\": \"lines\", \"name\": \"Quadratic\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [0.23511052948021774, 0.18455113694871086, 0.1746888670684831, 0.17088787461067578, 0.16990642674912537, 0.16952151503854623, 0.16359259470352616, 0.16617108130713076, 0.16877864308913074, 0.16812677240211796], \"type\": \"scatter\", \"uid\": \"6240ea86-d347-11e8-8707-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"CrossEntropy\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [1.9263678377944575, 1.926529576230227, 1.9265258643832714, 1.9265258643832475, 1.9265258643832308, 1.9265258643832144, 1.9265258643831964, 1.9265258643831784, 1.926525864383162, 1.9265258643831449], \"type\": \"scatter\", \"uid\": \"62422fe2-d347-11e8-ad12-a820660c6deb\"}],\n",
       "            {\"title\": \"Cost: Hidden: Sigmoid / Output: Sigmoid / Network:4-2-3 (Train)\", \"xaxis\": {\"title\": \"Epoch\"}, \"yaxis\": {\"range\": [0, 2], \"title\": \"Cost\"}},\n",
       "            {\"showLink\": true, \"linkText\": \"Export to plot.ly\"}\n",
       "        ).then(function () {return Plotly.addFrames('d60e448a-72de-4581-979d-943cafcf5a16',{});}).then(function(){Plotly.animate('d60e448a-72de-4581-979d-943cafcf5a16');})\n",
       "        });</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Set up plotly data\n",
    "trace0 = go.Scatter(\n",
    "    x = epoch_lst,\n",
    "    y = training_cost1,\n",
    "    mode = 'lines',\n",
    "    name = 'Quadratic'\n",
    ")\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x = epoch_lst,\n",
    "    y = training_cost2,\n",
    "    mode = 'lines',\n",
    "    name = 'CrossEntropy'\n",
    ")\n",
    "\n",
    "\n",
    "data = [trace0, trace1]\n",
    "\n",
    "# Edit the layout\n",
    "layout = dict(title = 'Cost: Hidden: Sigmoid / Output: Sigmoid / Network:4-2-3 (Train)',\n",
    "              xaxis = dict(title = 'Epoch'),\n",
    "              yaxis = dict(title = 'Cost', range=[0,2]),\n",
    "              )\n",
    "\n",
    "#Plot \n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "lines",
         "name": "Training Accuracy - Quadratic",
         "type": "scatter",
         "uid": "62a12806-d347-11e8-a5e8-a820660c6deb",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9
         ],
         "y": [
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65
         ]
        },
        {
         "mode": "lines",
         "name": "Training Accuracy - CrossEntropy",
         "type": "scatter",
         "uid": "62a12b4c-d347-11e8-a57a-a820660c6deb",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9
         ],
         "y": [
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33
         ]
        }
       ],
       "layout": {
        "title": "Accuracy: Hidden: Sigmoid / Output: Sigmoid / Network: 4-2-3 (Train: 100 instances)",
        "xaxis": {
         "title": "Epoch"
        },
        "yaxis": {
         "range": [
          20,
          70
         ],
         "title": "Cost"
        }
       }
      },
      "text/html": [
       "<div id=\"06f8642e-7419-4ae4-8ec5-e866e2201e7c\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "        Plotly.plot(\n",
       "            '06f8642e-7419-4ae4-8ec5-e866e2201e7c',\n",
       "            [{\"mode\": \"lines\", \"name\": \"Training Accuracy - Quadratic\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [65, 65, 65, 65, 65, 65, 65, 65, 65, 65], \"type\": \"scatter\", \"uid\": \"62a12806-d347-11e8-a5e8-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"Training Accuracy - CrossEntropy\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [33, 33, 33, 33, 33, 33, 33, 33, 33, 33], \"type\": \"scatter\", \"uid\": \"62a12b4c-d347-11e8-a57a-a820660c6deb\"}],\n",
       "            {\"title\": \"Accuracy: Hidden: Sigmoid / Output: Sigmoid / Network: 4-2-3 (Train: 100 instances)\", \"xaxis\": {\"title\": \"Epoch\"}, \"yaxis\": {\"range\": [20, 70], \"title\": \"Cost\"}},\n",
       "            {\"showLink\": true, \"linkText\": \"Export to plot.ly\"}\n",
       "        ).then(function () {return Plotly.addFrames('06f8642e-7419-4ae4-8ec5-e866e2201e7c',{});}).then(function(){Plotly.animate('06f8642e-7419-4ae4-8ec5-e866e2201e7c');})\n",
       "        });</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"06f8642e-7419-4ae4-8ec5-e866e2201e7c\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "        Plotly.plot(\n",
       "            '06f8642e-7419-4ae4-8ec5-e866e2201e7c',\n",
       "            [{\"mode\": \"lines\", \"name\": \"Training Accuracy - Quadratic\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [65, 65, 65, 65, 65, 65, 65, 65, 65, 65], \"type\": \"scatter\", \"uid\": \"62a12806-d347-11e8-a5e8-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"Training Accuracy - CrossEntropy\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [33, 33, 33, 33, 33, 33, 33, 33, 33, 33], \"type\": \"scatter\", \"uid\": \"62a12b4c-d347-11e8-a57a-a820660c6deb\"}],\n",
       "            {\"title\": \"Accuracy: Hidden: Sigmoid / Output: Sigmoid / Network: 4-2-3 (Train: 100 instances)\", \"xaxis\": {\"title\": \"Epoch\"}, \"yaxis\": {\"range\": [20, 70], \"title\": \"Cost\"}},\n",
       "            {\"showLink\": true, \"linkText\": \"Export to plot.ly\"}\n",
       "        ).then(function () {return Plotly.addFrames('06f8642e-7419-4ae4-8ec5-e866e2201e7c',{});}).then(function(){Plotly.animate('06f8642e-7419-4ae4-8ec5-e866e2201e7c');})\n",
       "        });</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Set up plotly data\n",
    "trace0 = go.Scatter(\n",
    "    x = epoch_lst,\n",
    "    y = training_accuracy1,\n",
    "    mode = 'lines',\n",
    "    name = 'Training Accuracy - Quadratic'\n",
    ")\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x = epoch_lst,\n",
    "    y = training_accuracy2,\n",
    "    mode = 'lines',\n",
    "    name = 'Training Accuracy - CrossEntropy'\n",
    ")\n",
    "\n",
    "\n",
    "data = [trace0, trace1]\n",
    "\n",
    "# Edit the layout\n",
    "layout = dict(title = 'Accuracy: Hidden: Sigmoid / Output: Sigmoid / Network: 4-2-3 (Train: 100 instances)',\n",
    "              xaxis = dict(title = 'Epoch'),\n",
    "              yaxis = dict(title = 'Cost', range=[20, 70]),\n",
    "              )\n",
    "\n",
    "#Plot \n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "lines",
         "name": "CrossEntropy",
         "type": "scatter",
         "uid": "62e757b8-d347-11e8-b979-a820660c6deb",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9
         ],
         "y": [
          1.1669307963212423,
          1.1669307963209965,
          1.1669307963209965,
          1.1669307963209965,
          1.1669307963209965,
          1.1669307963209965,
          1.1669307963209965,
          1.1669307963209965,
          1.1669307963209965,
          1.1669307963209965
         ]
        },
        {
         "mode": "lines",
         "name": "LogLikelihood - Reg: Default",
         "type": "scatter",
         "uid": "62e75b78-d347-11e8-a798-a820660c6deb",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9
         ],
         "y": [
          1.1669307963212423,
          1.1669307963209965,
          1.1669307963209965,
          1.1669307963209965,
          1.1669307963209965,
          1.1669307963209965,
          1.1669307963209965,
          1.1669307963209965,
          1.1669307963209965,
          1.1669307963209965
         ]
        },
        {
         "mode": "lines",
         "name": "LogLikelihood - Reg: L2, lmbda:3.0",
         "type": "scatter",
         "uid": "62e75dba-d347-11e8-8007-a820660c6deb",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9
         ],
         "y": [
          1.2515777525067482,
          1.1671212605998658,
          1.1669312248850365,
          1.1669307972853094,
          1.1669307963231663,
          1.1669307963210014,
          1.1669307963209965,
          1.1669307963209965,
          1.1669307963209965,
          1.1669307963209965
         ]
        },
        {
         "mode": "lines",
         "name": "LogLikelihood - Reg: L1, lmbda:3.0",
         "type": "scatter",
         "uid": "62e75f74-d347-11e8-995f-a820660c6deb",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9
         ],
         "y": [
          3.657026899941137,
          3.0548493895376785,
          2.608402325715704,
          2.2633854682574435,
          1.9806191822207226,
          1.7573298493040912,
          1.5697604564589458,
          1.3783824808830385,
          1.2177031129146834,
          1.133063921326252
         ]
        }
       ],
       "layout": {
        "title": "Cost: hidden: ReLU / Output: Softmax / Network: 4-2-3 (Train)",
        "xaxis": {
         "title": "Epoch"
        },
        "yaxis": {
         "range": [
          0.5,
          3
         ],
         "title": "Cost"
        }
       }
      },
      "text/html": [
       "<div id=\"6b95fc44-de44-41b0-8524-93f689a8fd21\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "        Plotly.plot(\n",
       "            '6b95fc44-de44-41b0-8524-93f689a8fd21',\n",
       "            [{\"mode\": \"lines\", \"name\": \"CrossEntropy\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [1.1669307963212423, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965], \"type\": \"scatter\", \"uid\": \"62e757b8-d347-11e8-b979-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: Default\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [1.1669307963212423, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965], \"type\": \"scatter\", \"uid\": \"62e75b78-d347-11e8-a798-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: L2, lmbda:3.0\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [1.2515777525067482, 1.1671212605998658, 1.1669312248850365, 1.1669307972853094, 1.1669307963231663, 1.1669307963210014, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965], \"type\": \"scatter\", \"uid\": \"62e75dba-d347-11e8-8007-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: L1, lmbda:3.0\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [3.657026899941137, 3.0548493895376785, 2.608402325715704, 2.2633854682574435, 1.9806191822207226, 1.7573298493040912, 1.5697604564589458, 1.3783824808830385, 1.2177031129146834, 1.133063921326252], \"type\": \"scatter\", \"uid\": \"62e75f74-d347-11e8-995f-a820660c6deb\"}],\n",
       "            {\"title\": \"Cost: hidden: ReLU / Output: Softmax / Network: 4-2-3 (Train)\", \"xaxis\": {\"title\": \"Epoch\"}, \"yaxis\": {\"range\": [0.5, 3], \"title\": \"Cost\"}},\n",
       "            {\"showLink\": true, \"linkText\": \"Export to plot.ly\"}\n",
       "        ).then(function () {return Plotly.addFrames('6b95fc44-de44-41b0-8524-93f689a8fd21',{});}).then(function(){Plotly.animate('6b95fc44-de44-41b0-8524-93f689a8fd21');})\n",
       "        });</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"6b95fc44-de44-41b0-8524-93f689a8fd21\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "        Plotly.plot(\n",
       "            '6b95fc44-de44-41b0-8524-93f689a8fd21',\n",
       "            [{\"mode\": \"lines\", \"name\": \"CrossEntropy\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [1.1669307963212423, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965], \"type\": \"scatter\", \"uid\": \"62e757b8-d347-11e8-b979-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: Default\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [1.1669307963212423, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965], \"type\": \"scatter\", \"uid\": \"62e75b78-d347-11e8-a798-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: L2, lmbda:3.0\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [1.2515777525067482, 1.1671212605998658, 1.1669312248850365, 1.1669307972853094, 1.1669307963231663, 1.1669307963210014, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965], \"type\": \"scatter\", \"uid\": \"62e75dba-d347-11e8-8007-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: L1, lmbda:3.0\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [3.657026899941137, 3.0548493895376785, 2.608402325715704, 2.2633854682574435, 1.9806191822207226, 1.7573298493040912, 1.5697604564589458, 1.3783824808830385, 1.2177031129146834, 1.133063921326252], \"type\": \"scatter\", \"uid\": \"62e75f74-d347-11e8-995f-a820660c6deb\"}],\n",
       "            {\"title\": \"Cost: hidden: ReLU / Output: Softmax / Network: 4-2-3 (Train)\", \"xaxis\": {\"title\": \"Epoch\"}, \"yaxis\": {\"range\": [0.5, 3], \"title\": \"Cost\"}},\n",
       "            {\"showLink\": true, \"linkText\": \"Export to plot.ly\"}\n",
       "        ).then(function () {return Plotly.addFrames('6b95fc44-de44-41b0-8524-93f689a8fd21',{});}).then(function(){Plotly.animate('6b95fc44-de44-41b0-8524-93f689a8fd21');})\n",
       "        });</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Set up plotly data\n",
    "trace0 = go.Scatter(\n",
    "    x = epoch_lst,\n",
    "    y = training_cost3,\n",
    "    mode = 'lines',\n",
    "    name = 'CrossEntropy'\n",
    ")\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x = epoch_lst,\n",
    "    y = training_cost4,\n",
    "    mode = 'lines',\n",
    "    name = 'LogLikelihood - Reg: Default'\n",
    ")\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "    x = epoch_lst,\n",
    "    y = training_cost7,\n",
    "    mode = 'lines',\n",
    "    name = 'LogLikelihood - Reg: L2, lmbda:3.0'\n",
    ")\n",
    "\n",
    "trace3 = go.Scatter(\n",
    "    x = epoch_lst,\n",
    "    y = training_cost8,\n",
    "    mode = 'lines',\n",
    "    name = 'LogLikelihood - Reg: L1, lmbda:3.0'\n",
    ")\n",
    "\n",
    "data = [trace0, trace1, trace2, trace3]\n",
    "\n",
    "# Edit the layout\n",
    "layout = dict(title = 'Cost: hidden: ReLU / Output: Softmax / Network: 4-2-3 (Train)',\n",
    "              xaxis = dict(title = 'Epoch'),\n",
    "              yaxis = dict(title = 'Cost', range=[0.5,3]),\n",
    "              )\n",
    "\n",
    "#Plot \n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "lines",
         "name": "CrossEntropy",
         "type": "scatter",
         "uid": "134d88a2-d348-11e8-99b6-a820660c6deb",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9
         ],
         "y": [
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33
         ]
        },
        {
         "mode": "lines",
         "name": "LogLikelihood - Reg: Default",
         "type": "scatter",
         "uid": "134d8a78-d348-11e8-8c91-a820660c6deb",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9
         ],
         "y": [
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33
         ]
        },
        {
         "mode": "lines",
         "name": "LogLikelihood - Reg: L2, lmbda:3.0",
         "type": "scatter",
         "uid": "134d8b7a-d348-11e8-97c9-a820660c6deb",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9
         ],
         "y": [
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33
         ]
        },
        {
         "mode": "lines",
         "name": "LogLikelihood - Reg: L1, lmbda:3.0",
         "type": "scatter",
         "uid": "134d8c62-d348-11e8-a99c-a820660c6deb",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9
         ],
         "y": [
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30
         ]
        }
       ],
       "layout": {
        "title": "Accuracy: hidden: ReLU / Output: Softmax / Network: 4-2-3 (Train: 95 instances)",
        "xaxis": {
         "title": "Epoch"
        },
        "yaxis": {
         "range": [
          28,
          35
         ],
         "title": "Accuracy"
        }
       }
      },
      "text/html": [
       "<div id=\"e8ab1ad7-17f6-4919-9dca-497f291f9409\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "        Plotly.plot(\n",
       "            'e8ab1ad7-17f6-4919-9dca-497f291f9409',\n",
       "            [{\"mode\": \"lines\", \"name\": \"CrossEntropy\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [33, 33, 33, 33, 33, 33, 33, 33, 33, 33], \"type\": \"scatter\", \"uid\": \"134d88a2-d348-11e8-99b6-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: Default\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [33, 33, 33, 33, 33, 33, 33, 33, 33, 33], \"type\": \"scatter\", \"uid\": \"134d8a78-d348-11e8-8c91-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: L2, lmbda:3.0\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [33, 33, 33, 33, 33, 33, 33, 33, 33, 33], \"type\": \"scatter\", \"uid\": \"134d8b7a-d348-11e8-97c9-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: L1, lmbda:3.0\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [30, 30, 30, 30, 30, 30, 30, 30, 30, 30], \"type\": \"scatter\", \"uid\": \"134d8c62-d348-11e8-a99c-a820660c6deb\"}],\n",
       "            {\"title\": \"Accuracy: hidden: ReLU / Output: Softmax / Network: 4-2-3 (Train: 95 instances)\", \"xaxis\": {\"title\": \"Epoch\"}, \"yaxis\": {\"range\": [28, 35], \"title\": \"Accuracy\"}},\n",
       "            {\"showLink\": true, \"linkText\": \"Export to plot.ly\"}\n",
       "        ).then(function () {return Plotly.addFrames('e8ab1ad7-17f6-4919-9dca-497f291f9409',{});}).then(function(){Plotly.animate('e8ab1ad7-17f6-4919-9dca-497f291f9409');})\n",
       "        });</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"e8ab1ad7-17f6-4919-9dca-497f291f9409\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "        Plotly.plot(\n",
       "            'e8ab1ad7-17f6-4919-9dca-497f291f9409',\n",
       "            [{\"mode\": \"lines\", \"name\": \"CrossEntropy\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [33, 33, 33, 33, 33, 33, 33, 33, 33, 33], \"type\": \"scatter\", \"uid\": \"134d88a2-d348-11e8-99b6-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: Default\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [33, 33, 33, 33, 33, 33, 33, 33, 33, 33], \"type\": \"scatter\", \"uid\": \"134d8a78-d348-11e8-8c91-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: L2, lmbda:3.0\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [33, 33, 33, 33, 33, 33, 33, 33, 33, 33], \"type\": \"scatter\", \"uid\": \"134d8b7a-d348-11e8-97c9-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: L1, lmbda:3.0\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [30, 30, 30, 30, 30, 30, 30, 30, 30, 30], \"type\": \"scatter\", \"uid\": \"134d8c62-d348-11e8-a99c-a820660c6deb\"}],\n",
       "            {\"title\": \"Accuracy: hidden: ReLU / Output: Softmax / Network: 4-2-3 (Train: 95 instances)\", \"xaxis\": {\"title\": \"Epoch\"}, \"yaxis\": {\"range\": [28, 35], \"title\": \"Accuracy\"}},\n",
       "            {\"showLink\": true, \"linkText\": \"Export to plot.ly\"}\n",
       "        ).then(function () {return Plotly.addFrames('e8ab1ad7-17f6-4919-9dca-497f291f9409',{});}).then(function(){Plotly.animate('e8ab1ad7-17f6-4919-9dca-497f291f9409');})\n",
       "        });</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Set up plotly data\n",
    "trace0 = go.Scatter(\n",
    "    x = epoch_lst,\n",
    "    y = training_accuracy3,\n",
    "    mode = 'lines',\n",
    "    name = 'CrossEntropy'\n",
    ")\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x = epoch_lst,\n",
    "    y = training_accuracy4,\n",
    "    mode = 'lines',\n",
    "    name = 'LogLikelihood - Reg: Default'\n",
    ")\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "    x = epoch_lst,\n",
    "    y = training_accuracy7,\n",
    "    mode = 'lines',\n",
    "    name = 'LogLikelihood - Reg: L2, lmbda:3.0'\n",
    ")\n",
    "\n",
    "trace3 = go.Scatter(\n",
    "    x = epoch_lst,\n",
    "    y = training_accuracy8,\n",
    "    mode = 'lines',\n",
    "    name = 'LogLikelihood - Reg: L1, lmbda:3.0'\n",
    ")\n",
    "\n",
    "data = [trace0, trace1, trace2, trace3]\n",
    "\n",
    "# Edit the layout\n",
    "layout = dict(title = 'Accuracy: hidden: ReLU / Output: Softmax / Network: 4-2-3 (Train: 95 instances)',\n",
    "              xaxis = dict(title = 'Epoch'),\n",
    "              yaxis = dict(title = 'Accuracy', range=[28,35]),\n",
    "              )\n",
    "\n",
    "#Plot \n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "lines",
         "name": "LogLikelihood - Reg: Default, drop:0.1",
         "type": "scatter",
         "uid": "636870a8-d347-11e8-86bd-a820660c6deb",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9
         ],
         "y": [
          1.1669307963234508,
          1.1669307963209965,
          1.1669307963209965,
          1.1669307963209965,
          1.1669307963209965,
          1.1669307963209965,
          1.1669307963209965,
          1.1669307963209965,
          1.1669307963209965,
          1.1669307963209965
         ]
        },
        {
         "mode": "lines",
         "name": "LogLikelihood - Reg: Default, drop:0.5",
         "type": "scatter",
         "uid": "63687276-d347-11e8-84b9-a820660c6deb",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9
         ],
         "y": [
          1.1669307963490136,
          1.1669307963209965,
          1.1669307963209965,
          1.1669307963209965,
          1.1669307963209965,
          1.1669307963209965,
          1.1669307963209965,
          1.1669307963209965,
          1.1669307963209965,
          1.1669307963209965
         ]
        },
        {
         "mode": "lines",
         "name": "LogLikelihood - Reg: L2, lmbda:3.0, drop:0.1",
         "type": "scatter",
         "uid": "6368737a-d347-11e8-8981-a820660c6deb",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9
         ],
         "y": [
          1.3704736335191618,
          1.1673887884503387,
          1.1669318268499673,
          1.1669307986397917,
          1.166930796326214,
          1.1669307963210083,
          1.1669307963209965,
          1.1669307963209965,
          1.1669307963209965,
          1.1669307963209965
         ]
        },
        {
         "mode": "lines",
         "name": "LogLikelihood - Reg: L2, lmbda:3.0, drop:0.5",
         "type": "scatter",
         "uid": "63687468-d347-11e8-9760-a820660c6deb",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9
         ],
         "y": [
          33.82506538678303,
          1.2404149275975764,
          1.1670961431060747,
          1.1669311683681156,
          1.1669307971581404,
          1.16693079632288,
          1.1669307963210007,
          1.1669307963209965,
          1.1669307963209965,
          1.1669307963209965
         ]
        }
       ],
       "layout": {
        "title": "Cost: hidden: ReLU / Output: Softmax / Network: 4-20-7-3 (Train: 95 instances)",
        "xaxis": {
         "title": "Epoch"
        },
        "yaxis": {
         "range": [
          0.5,
          3
         ],
         "title": "Cost"
        }
       }
      },
      "text/html": [
       "<div id=\"1f3e0a78-4d33-429d-b7f8-2b1571d576c8\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "        Plotly.plot(\n",
       "            '1f3e0a78-4d33-429d-b7f8-2b1571d576c8',\n",
       "            [{\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: Default, drop:0.1\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [1.1669307963234508, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965], \"type\": \"scatter\", \"uid\": \"636870a8-d347-11e8-86bd-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: Default, drop:0.5\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [1.1669307963490136, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965], \"type\": \"scatter\", \"uid\": \"63687276-d347-11e8-84b9-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: L2, lmbda:3.0, drop:0.1\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [1.3704736335191618, 1.1673887884503387, 1.1669318268499673, 1.1669307986397917, 1.166930796326214, 1.1669307963210083, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965], \"type\": \"scatter\", \"uid\": \"6368737a-d347-11e8-8981-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: L2, lmbda:3.0, drop:0.5\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [33.82506538678303, 1.2404149275975764, 1.1670961431060747, 1.1669311683681156, 1.1669307971581404, 1.16693079632288, 1.1669307963210007, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965], \"type\": \"scatter\", \"uid\": \"63687468-d347-11e8-9760-a820660c6deb\"}],\n",
       "            {\"title\": \"Cost: hidden: ReLU / Output: Softmax / Network: 4-20-7-3 (Train: 95 instances)\", \"xaxis\": {\"title\": \"Epoch\"}, \"yaxis\": {\"range\": [0.5, 3], \"title\": \"Cost\"}},\n",
       "            {\"showLink\": true, \"linkText\": \"Export to plot.ly\"}\n",
       "        ).then(function () {return Plotly.addFrames('1f3e0a78-4d33-429d-b7f8-2b1571d576c8',{});}).then(function(){Plotly.animate('1f3e0a78-4d33-429d-b7f8-2b1571d576c8');})\n",
       "        });</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"1f3e0a78-4d33-429d-b7f8-2b1571d576c8\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "        Plotly.plot(\n",
       "            '1f3e0a78-4d33-429d-b7f8-2b1571d576c8',\n",
       "            [{\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: Default, drop:0.1\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [1.1669307963234508, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965], \"type\": \"scatter\", \"uid\": \"636870a8-d347-11e8-86bd-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: Default, drop:0.5\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [1.1669307963490136, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965], \"type\": \"scatter\", \"uid\": \"63687276-d347-11e8-84b9-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: L2, lmbda:3.0, drop:0.1\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [1.3704736335191618, 1.1673887884503387, 1.1669318268499673, 1.1669307986397917, 1.166930796326214, 1.1669307963210083, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965], \"type\": \"scatter\", \"uid\": \"6368737a-d347-11e8-8981-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: L2, lmbda:3.0, drop:0.5\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [33.82506538678303, 1.2404149275975764, 1.1670961431060747, 1.1669311683681156, 1.1669307971581404, 1.16693079632288, 1.1669307963210007, 1.1669307963209965, 1.1669307963209965, 1.1669307963209965], \"type\": \"scatter\", \"uid\": \"63687468-d347-11e8-9760-a820660c6deb\"}],\n",
       "            {\"title\": \"Cost: hidden: ReLU / Output: Softmax / Network: 4-20-7-3 (Train: 95 instances)\", \"xaxis\": {\"title\": \"Epoch\"}, \"yaxis\": {\"range\": [0.5, 3], \"title\": \"Cost\"}},\n",
       "            {\"showLink\": true, \"linkText\": \"Export to plot.ly\"}\n",
       "        ).then(function () {return Plotly.addFrames('1f3e0a78-4d33-429d-b7f8-2b1571d576c8',{});}).then(function(){Plotly.animate('1f3e0a78-4d33-429d-b7f8-2b1571d576c8');})\n",
       "        });</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Set up plotly data\n",
    "trace0 = go.Scatter(\n",
    "    x = epoch_lst,\n",
    "    y = training_cost9,\n",
    "    mode = 'lines',\n",
    "    name = 'LogLikelihood - Reg: Default, drop:0.1'\n",
    ")\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x = epoch_lst,\n",
    "    y = training_cost10,\n",
    "    mode = 'lines',\n",
    "    name = 'LogLikelihood - Reg: Default, drop:0.5'\n",
    ")\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "    x = epoch_lst,\n",
    "    y = training_cost11,\n",
    "    mode = 'lines',\n",
    "    name = 'LogLikelihood - Reg: L2, lmbda:3.0, drop:0.1'\n",
    ")\n",
    "\n",
    "trace3 = go.Scatter(\n",
    "    x = epoch_lst,\n",
    "    y = training_cost12,\n",
    "    mode = 'lines',\n",
    "    name = 'LogLikelihood - Reg: L2, lmbda:3.0, drop:0.5'\n",
    ")\n",
    "\n",
    "data = [trace0, trace1, trace2, trace3]\n",
    "\n",
    "# Edit the layout\n",
    "layout = dict(title = 'Cost: hidden: ReLU / Output: Softmax / Network: 4-20-7-3 (Train: 95 instances)',\n",
    "              xaxis = dict(title = 'Epoch'),\n",
    "              yaxis = dict(title = 'Cost', range=[0.5,3]),\n",
    "              )\n",
    "\n",
    "#Plot \n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "lines",
         "name": "LogLikelihood - Reg: Default, drop:0.1",
         "type": "scatter",
         "uid": "63a1a9d8-d347-11e8-85a9-a820660c6deb",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9
         ],
         "y": [
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33
         ]
        },
        {
         "mode": "lines",
         "name": "LogLikelihood - Reg: Default, drop:0.5",
         "type": "scatter",
         "uid": "63a1abfe-d347-11e8-8201-a820660c6deb",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9
         ],
         "y": [
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33
         ]
        },
        {
         "mode": "lines",
         "name": "LogLikelihood - Reg: L2, lmbda:3.0, drop:0.1",
         "type": "scatter",
         "uid": "63a1acf8-d347-11e8-90ed-a820660c6deb",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9
         ],
         "y": [
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33
         ]
        },
        {
         "mode": "lines",
         "name": "LogLikelihood - Reg: L2, lmbda:3.0, drop:0.5",
         "type": "scatter",
         "uid": "63a1adde-d347-11e8-a6f8-a820660c6deb",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9
         ],
         "y": [
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33
         ]
        }
       ],
       "layout": {
        "title": "Accuracy: Hidden: ReLU / Output: Softmax / Network: 4-20-7-3 (Train: 95 instances)",
        "xaxis": {
         "title": "Epoch"
        },
        "yaxis": {
         "range": [
          31,
          35
         ],
         "title": "Accuracy"
        }
       }
      },
      "text/html": [
       "<div id=\"51b662c7-2c5b-4489-982c-7f16c3c72155\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "        Plotly.plot(\n",
       "            '51b662c7-2c5b-4489-982c-7f16c3c72155',\n",
       "            [{\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: Default, drop:0.1\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [33, 33, 33, 33, 33, 33, 33, 33, 33, 33], \"type\": \"scatter\", \"uid\": \"63a1a9d8-d347-11e8-85a9-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: Default, drop:0.5\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [33, 33, 33, 33, 33, 33, 33, 33, 33, 33], \"type\": \"scatter\", \"uid\": \"63a1abfe-d347-11e8-8201-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: L2, lmbda:3.0, drop:0.1\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [33, 33, 33, 33, 33, 33, 33, 33, 33, 33], \"type\": \"scatter\", \"uid\": \"63a1acf8-d347-11e8-90ed-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: L2, lmbda:3.0, drop:0.5\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [33, 33, 33, 33, 33, 33, 33, 33, 33, 33], \"type\": \"scatter\", \"uid\": \"63a1adde-d347-11e8-a6f8-a820660c6deb\"}],\n",
       "            {\"title\": \"Accuracy: Hidden: ReLU / Output: Softmax / Network: 4-20-7-3 (Train: 95 instances)\", \"xaxis\": {\"title\": \"Epoch\"}, \"yaxis\": {\"range\": [31, 35], \"title\": \"Accuracy\"}},\n",
       "            {\"showLink\": true, \"linkText\": \"Export to plot.ly\"}\n",
       "        ).then(function () {return Plotly.addFrames('51b662c7-2c5b-4489-982c-7f16c3c72155',{});}).then(function(){Plotly.animate('51b662c7-2c5b-4489-982c-7f16c3c72155');})\n",
       "        });</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"51b662c7-2c5b-4489-982c-7f16c3c72155\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "        Plotly.plot(\n",
       "            '51b662c7-2c5b-4489-982c-7f16c3c72155',\n",
       "            [{\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: Default, drop:0.1\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [33, 33, 33, 33, 33, 33, 33, 33, 33, 33], \"type\": \"scatter\", \"uid\": \"63a1a9d8-d347-11e8-85a9-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: Default, drop:0.5\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [33, 33, 33, 33, 33, 33, 33, 33, 33, 33], \"type\": \"scatter\", \"uid\": \"63a1abfe-d347-11e8-8201-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: L2, lmbda:3.0, drop:0.1\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [33, 33, 33, 33, 33, 33, 33, 33, 33, 33], \"type\": \"scatter\", \"uid\": \"63a1acf8-d347-11e8-90ed-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: L2, lmbda:3.0, drop:0.5\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [33, 33, 33, 33, 33, 33, 33, 33, 33, 33], \"type\": \"scatter\", \"uid\": \"63a1adde-d347-11e8-a6f8-a820660c6deb\"}],\n",
       "            {\"title\": \"Accuracy: Hidden: ReLU / Output: Softmax / Network: 4-20-7-3 (Train: 95 instances)\", \"xaxis\": {\"title\": \"Epoch\"}, \"yaxis\": {\"range\": [31, 35], \"title\": \"Accuracy\"}},\n",
       "            {\"showLink\": true, \"linkText\": \"Export to plot.ly\"}\n",
       "        ).then(function () {return Plotly.addFrames('51b662c7-2c5b-4489-982c-7f16c3c72155',{});}).then(function(){Plotly.animate('51b662c7-2c5b-4489-982c-7f16c3c72155');})\n",
       "        });</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Set up plotly data\n",
    "trace0 = go.Scatter(\n",
    "    x = epoch_lst,\n",
    "    y = training_accuracy9,\n",
    "    mode = 'lines',\n",
    "    name = 'LogLikelihood - Reg: Default, drop:0.1'\n",
    ")\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x = epoch_lst,\n",
    "    y = training_accuracy10,\n",
    "    mode = 'lines',\n",
    "    name = 'LogLikelihood - Reg: Default, drop:0.5'\n",
    ")\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "    x = epoch_lst,\n",
    "    y = training_accuracy11,\n",
    "    mode = 'lines',\n",
    "    name = 'LogLikelihood - Reg: L2, lmbda:3.0, drop:0.1'\n",
    ")\n",
    "\n",
    "trace3 = go.Scatter(\n",
    "    x = epoch_lst,\n",
    "    y = training_accuracy12,\n",
    "    mode = 'lines',\n",
    "    name = 'LogLikelihood - Reg: L2, lmbda:3.0, drop:0.5'\n",
    ")\n",
    "\n",
    "data = [trace0, trace1, trace2, trace3]\n",
    "\n",
    "# Edit the layout\n",
    "layout = dict(title = 'Accuracy: Hidden: ReLU / Output: Softmax / Network: 4-20-7-3 (Train: 95 instances)',\n",
    "              xaxis = dict(title = 'Epoch'),\n",
    "              yaxis = dict(title = 'Accuracy', range=[31,35]),\n",
    "              )\n",
    "\n",
    "#Plot \n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "lines",
         "name": "Training Cost",
         "type": "scatter",
         "uid": "64271618-d347-11e8-a821-a820660c6deb",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9
         ],
         "y": [
          0.23511052948021774,
          0.18455113694871086,
          0.1746888670684831,
          0.17088787461067578,
          0.16990642674912537,
          0.16952151503854623,
          0.16359259470352616,
          0.16617108130713076,
          0.16877864308913074,
          0.16812677240211796
         ]
        },
        {
         "mode": "lines",
         "name": "Evaluation Cost",
         "type": "scatter",
         "uid": "64271886-d347-11e8-85f4-a820660c6deb",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9
         ],
         "y": [
          2.5399319341564066,
          2.5399444183983864,
          2.539949744825968,
          2.539952771327394,
          2.539954747177492,
          2.5399561489584004,
          2.539957200384023,
          2.539958021176445,
          2.5399586815242463,
          2.5399592254473013
         ]
        }
       ],
       "layout": {
        "title": "Cost: Hidden: Tanh / Output: Tanh / Network:4-2-3",
        "xaxis": {
         "title": "Epoch"
        },
        "yaxis": {
         "title": "Cost"
        }
       }
      },
      "text/html": [
       "<div id=\"8e3a1220-f21d-4970-903e-2c5497c87e56\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "        Plotly.plot(\n",
       "            '8e3a1220-f21d-4970-903e-2c5497c87e56',\n",
       "            [{\"mode\": \"lines\", \"name\": \"Training Cost\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [0.23511052948021774, 0.18455113694871086, 0.1746888670684831, 0.17088787461067578, 0.16990642674912537, 0.16952151503854623, 0.16359259470352616, 0.16617108130713076, 0.16877864308913074, 0.16812677240211796], \"type\": \"scatter\", \"uid\": \"64271618-d347-11e8-a821-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"Evaluation Cost\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [2.5399319341564066, 2.5399444183983864, 2.539949744825968, 2.539952771327394, 2.539954747177492, 2.5399561489584004, 2.539957200384023, 2.539958021176445, 2.5399586815242463, 2.5399592254473013], \"type\": \"scatter\", \"uid\": \"64271886-d347-11e8-85f4-a820660c6deb\"}],\n",
       "            {\"title\": \"Cost: Hidden: Tanh / Output: Tanh / Network:4-2-3\", \"xaxis\": {\"title\": \"Epoch\"}, \"yaxis\": {\"title\": \"Cost\"}},\n",
       "            {\"showLink\": true, \"linkText\": \"Export to plot.ly\"}\n",
       "        ).then(function () {return Plotly.addFrames('8e3a1220-f21d-4970-903e-2c5497c87e56',{});}).then(function(){Plotly.animate('8e3a1220-f21d-4970-903e-2c5497c87e56');})\n",
       "        });</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"8e3a1220-f21d-4970-903e-2c5497c87e56\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "        Plotly.plot(\n",
       "            '8e3a1220-f21d-4970-903e-2c5497c87e56',\n",
       "            [{\"mode\": \"lines\", \"name\": \"Training Cost\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [0.23511052948021774, 0.18455113694871086, 0.1746888670684831, 0.17088787461067578, 0.16990642674912537, 0.16952151503854623, 0.16359259470352616, 0.16617108130713076, 0.16877864308913074, 0.16812677240211796], \"type\": \"scatter\", \"uid\": \"64271618-d347-11e8-a821-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"Evaluation Cost\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [2.5399319341564066, 2.5399444183983864, 2.539949744825968, 2.539952771327394, 2.539954747177492, 2.5399561489584004, 2.539957200384023, 2.539958021176445, 2.5399586815242463, 2.5399592254473013], \"type\": \"scatter\", \"uid\": \"64271886-d347-11e8-85f4-a820660c6deb\"}],\n",
       "            {\"title\": \"Cost: Hidden: Tanh / Output: Tanh / Network:4-2-3\", \"xaxis\": {\"title\": \"Epoch\"}, \"yaxis\": {\"title\": \"Cost\"}},\n",
       "            {\"showLink\": true, \"linkText\": \"Export to plot.ly\"}\n",
       "        ).then(function () {return Plotly.addFrames('8e3a1220-f21d-4970-903e-2c5497c87e56',{});}).then(function(){Plotly.animate('8e3a1220-f21d-4970-903e-2c5497c87e56');})\n",
       "        });</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Set up plotly data\n",
    "trace0 = go.Scatter(\n",
    "    x = epoch_lst,\n",
    "    y = training_cost1,\n",
    "    mode = 'lines',\n",
    "    name = 'Training Cost'\n",
    ")\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x = epoch_lst,\n",
    "    y = evaluation_cost5,\n",
    "    mode = 'lines',\n",
    "    name = 'Evaluation Cost'\n",
    ")\n",
    "\n",
    "\n",
    "data = [trace0, trace1]\n",
    "\n",
    "# Edit the layout\n",
    "layout = dict(title = 'Cost: Hidden: Tanh / Output: Tanh / Network:4-2-3',\n",
    "              xaxis = dict(title = 'Epoch'),\n",
    "              yaxis = dict(title = 'Cost'),\n",
    "              )\n",
    "\n",
    "#Plot \n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Run epoch = 200, mini_batch=1, eta0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.sizes [4, 20, 7, 3]\n",
      "Epoch 0 training complete\n",
      "Cost on training data: 0.7164908656559965\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.7268634506188708\n",
      "Accuracy on evaluation data: 34 / 55\n",
      "\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 0.4622341006614181\n",
      "Accuracy on training data: 63 / 95\n",
      "Cost on evaluation data: 0.43934646730988713\n",
      "Accuracy on evaluation data: 38 / 55\n",
      "\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 0.46917873348657074\n",
      "Accuracy on training data: 71 / 95\n",
      "Cost on evaluation data: 0.47613794519252367\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 0.4697690494586621\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.49140037659514557\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 0.49739563805671505\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.5126240154103568\n",
      "Accuracy on evaluation data: 34 / 55\n",
      "\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 0.4315818691239112\n",
      "Accuracy on training data: 67 / 95\n",
      "Cost on evaluation data: 0.4377845254374138\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 0.4337622886123998\n",
      "Accuracy on training data: 72 / 95\n",
      "Cost on evaluation data: 0.43169756169524226\n",
      "Accuracy on evaluation data: 41 / 55\n",
      "\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 0.38596244587540984\n",
      "Accuracy on training data: 74 / 95\n",
      "Cost on evaluation data: 0.38202725341232935\n",
      "Accuracy on evaluation data: 45 / 55\n",
      "\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 0.42570316520700596\n",
      "Accuracy on training data: 71 / 95\n",
      "Cost on evaluation data: 0.40305790107039774\n",
      "Accuracy on evaluation data: 41 / 55\n",
      "\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 0.2974264932909909\n",
      "Accuracy on training data: 86 / 95\n",
      "Cost on evaluation data: 0.2665723518035734\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 10 training complete\n",
      "Cost on training data: 0.35253845329622696\n",
      "Accuracy on training data: 81 / 95\n",
      "Cost on evaluation data: 0.3223664355270325\n",
      "Accuracy on evaluation data: 49 / 55\n",
      "\n",
      "Epoch 11 training complete\n",
      "Cost on training data: 0.35619648724113057\n",
      "Accuracy on training data: 80 / 95\n",
      "Cost on evaluation data: 0.33759515265911344\n",
      "Accuracy on evaluation data: 48 / 55\n",
      "\n",
      "Epoch 12 training complete\n",
      "Cost on training data: 0.2710454270435016\n",
      "Accuracy on training data: 92 / 95\n",
      "Cost on evaluation data: 0.24300839400620702\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 13 training complete\n",
      "Cost on training data: 0.32140455659657124\n",
      "Accuracy on training data: 83 / 95\n",
      "Cost on evaluation data: 0.28620384201198773\n",
      "Accuracy on evaluation data: 52 / 55\n",
      "\n",
      "Epoch 14 training complete\n",
      "Cost on training data: 0.3742665368333511\n",
      "Accuracy on training data: 75 / 95\n",
      "Cost on evaluation data: 0.35715348952316267\n",
      "Accuracy on evaluation data: 45 / 55\n",
      "\n",
      "Epoch 15 training complete\n",
      "Cost on training data: 0.25080542314606175\n",
      "Accuracy on training data: 93 / 95\n",
      "Cost on evaluation data: 0.21519350694393286\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 16 training complete\n",
      "Cost on training data: 0.38196588509580065\n",
      "Accuracy on training data: 81 / 95\n",
      "Cost on evaluation data: 0.34679829710756677\n",
      "Accuracy on evaluation data: 48 / 55\n",
      "\n",
      "Epoch 17 training complete\n",
      "Cost on training data: 0.29718250744986996\n",
      "Accuracy on training data: 86 / 95\n",
      "Cost on evaluation data: 0.24872031625051946\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 18 training complete\n",
      "Cost on training data: 0.30155054061296593\n",
      "Accuracy on training data: 84 / 95\n",
      "Cost on evaluation data: 0.25286186614411016\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 19 training complete\n",
      "Cost on training data: 0.2677209851194511\n",
      "Accuracy on training data: 88 / 95\n",
      "Cost on evaluation data: 0.23307871649630083\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 20 training complete\n",
      "Cost on training data: 0.5572495528069465\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6037385495067298\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 21 training complete\n",
      "Cost on training data: 0.5058145999038826\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5350357053732515\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 22 training complete\n",
      "Cost on training data: 0.4885488239783015\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5045414870317303\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 23 training complete\n",
      "Cost on training data: 0.4831317385066748\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4965970615225402\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 24 training complete\n",
      "Cost on training data: 0.47954295471655356\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4921500147897701\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 25 training complete\n",
      "Cost on training data: 0.4779945090452997\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.490167197539573\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 26 training complete\n",
      "Cost on training data: 0.476914110181013\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.48930807400156284\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 27 training complete\n",
      "Cost on training data: 0.4752594562993959\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4869354868247533\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 28 training complete\n",
      "Cost on training data: 0.47394873563394413\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.48588526404527943\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 29 training complete\n",
      "Cost on training data: 0.4725939707260362\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4848645195082813\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 30 training complete\n",
      "Cost on training data: 0.4732396917922601\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4855224004286839\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 31 training complete\n",
      "Cost on training data: 0.4725803984233273\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.48473830815619734\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 32 training complete\n",
      "Cost on training data: 0.4715187875206039\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.48357284039441295\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 33 training complete\n",
      "Cost on training data: 0.471637277176977\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.48385140661463194\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 34 training complete\n",
      "Cost on training data: 0.4707012770465192\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4831049535391876\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 35 training complete\n",
      "Cost on training data: 0.47068075059415243\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4830844537539195\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 36 training complete\n",
      "Cost on training data: 0.46981441115356937\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4823117003598247\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 37 training complete\n",
      "Cost on training data: 0.4698102754963191\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.48234918681570815\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 38 training complete\n",
      "Cost on training data: 0.4697951379530958\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.48220796532722643\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 39 training complete\n",
      "Cost on training data: 0.4690476466133142\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.48151735385062505\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 40 training complete\n",
      "Cost on training data: 0.4683995698560495\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4808591726888317\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 41 training complete\n",
      "Cost on training data: 0.46847471857349227\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4808968233773677\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 42 training complete\n",
      "Cost on training data: 0.46792373851558305\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4803364768727863\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 43 training complete\n",
      "Cost on training data: 0.46743996831847473\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.47983614113328105\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 44 training complete\n",
      "Cost on training data: 0.4669846244109398\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.47940006253262496\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 45 training complete\n",
      "Cost on training data: 0.4671246916974736\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.47950745832074687\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 training complete\n",
      "Cost on training data: 0.46669858935797176\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.479115085477808\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 47 training complete\n",
      "Cost on training data: 0.4668454410151359\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4791793834937002\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 48 training complete\n",
      "Cost on training data: 0.4664517444353516\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.47885163494224253\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 49 training complete\n",
      "Cost on training data: 0.46608633750514117\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4785049491495874\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 50 training complete\n",
      "Cost on training data: 0.46634956432276736\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4785288212208685\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 51 training complete\n",
      "Cost on training data: 0.46601712179775634\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.47837668664314587\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 52 training complete\n",
      "Cost on training data: 0.46570090632170624\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4781035467300643\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 53 training complete\n",
      "Cost on training data: 0.4657761171953198\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.47807010433377894\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 54 training complete\n",
      "Cost on training data: 0.4655472097653393\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4778885560170502\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 55 training complete\n",
      "Cost on training data: 0.46526708639403636\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4776657424555743\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 56 training complete\n",
      "Cost on training data: 0.46544733451148357\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4778290749052528\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 57 training complete\n",
      "Cost on training data: 0.4651736955085655\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.47757515438104325\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 58 training complete\n",
      "Cost on training data: 0.46492357616251245\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4773290826961056\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 59 training complete\n",
      "Cost on training data: 0.46510190919247035\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.47742880464917964\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 60 training complete\n",
      "Cost on training data: 0.4648638051519727\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.477269960407566\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 61 training complete\n",
      "Cost on training data: 0.46463231377921616\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.47703335700402794\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 62 training complete\n",
      "Cost on training data: 0.46441997743645497\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4768162251487296\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 63 training complete\n",
      "Cost on training data: 0.4646998253612009\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.47680637686424016\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 64 training complete\n",
      "Cost on training data: 0.46450596973088065\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.476844086814357\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 65 training complete\n",
      "Cost on training data: 0.46431018526986617\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4766847663524306\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 66 training complete\n",
      "Cost on training data: 0.46412162228702597\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.47650764351363534\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 67 training complete\n",
      "Cost on training data: 0.4639494709619405\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.47629939700946883\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 68 training complete\n",
      "Cost on training data: 0.4638272122447828\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.47617418269005474\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 69 training complete\n",
      "Cost on training data: 0.463673000781709\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4760473893171594\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 70 training complete\n",
      "Cost on training data: 0.4635257707216055\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.47590331365093247\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 71 training complete\n",
      "Cost on training data: 0.4636685825220047\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4757794282610167\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 72 training complete\n",
      "Cost on training data: 0.4635694585715622\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.47583041802726067\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 73 training complete\n",
      "Cost on training data: 0.46343859786550917\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4757909607358936\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 74 training complete\n",
      "Cost on training data: 0.4617534245687541\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4744845529108751\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 75 training complete\n",
      "Cost on training data: 0.4616147687936419\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.47436347066423323\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 76 training complete\n",
      "Cost on training data: 1.8723620979900741\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.8429245369077845\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 77 training complete\n",
      "Cost on training data: 0.4820832507146503\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.49326241056732567\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 78 training complete\n",
      "Cost on training data: 0.4791533486677235\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.49106642443763315\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 79 training complete\n",
      "Cost on training data: 0.4780271914387797\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4904365016131822\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 80 training complete\n",
      "Cost on training data: 0.4755846341608667\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4880650208883202\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 81 training complete\n",
      "Cost on training data: 0.4737067099387562\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.486072841302348\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 82 training complete\n",
      "Cost on training data: 0.4547532470057133\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4621401670774243\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 83 training complete\n",
      "Cost on training data: 0.37147263810031517\n",
      "Accuracy on training data: 85 / 95\n",
      "Cost on evaluation data: 0.34549755738644694\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 84 training complete\n",
      "Cost on training data: 0.385113442008069\n",
      "Accuracy on training data: 87 / 95\n",
      "Cost on evaluation data: 0.3681753634229765\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 85 training complete\n",
      "Cost on training data: 0.47173470440739135\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.48908565531960957\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 86 training complete\n",
      "Cost on training data: 0.37145750938333505\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.3905630406759602\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 87 training complete\n",
      "Cost on training data: 0.36742037141949757\n",
      "Accuracy on training data: 85 / 95\n",
      "Cost on evaluation data: 0.31732321087052046\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 88 training complete\n",
      "Cost on training data: 0.46631270221304005\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.47797042300907366\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 89 training complete\n",
      "Cost on training data: 0.43422092319965994\n",
      "Accuracy on training data: 73 / 95\n",
      "Cost on evaluation data: 0.42537530832030507\n",
      "Accuracy on evaluation data: 44 / 55\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90 training complete\n",
      "Cost on training data: 0.24773822813649568\n",
      "Accuracy on training data: 89 / 95\n",
      "Cost on evaluation data: 0.20558317405872364\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 91 training complete\n",
      "Cost on training data: 0.4974168126945388\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5291497779634635\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 92 training complete\n",
      "Cost on training data: 0.367444301193115\n",
      "Accuracy on training data: 81 / 95\n",
      "Cost on evaluation data: 0.31353890693985864\n",
      "Accuracy on evaluation data: 50 / 55\n",
      "\n",
      "Epoch 93 training complete\n",
      "Cost on training data: 0.23481457547934434\n",
      "Accuracy on training data: 92 / 95\n",
      "Cost on evaluation data: 0.1874996127316174\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 94 training complete\n",
      "Cost on training data: 0.44869850428527647\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4661966351182412\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 95 training complete\n",
      "Cost on training data: 0.3942306944677563\n",
      "Accuracy on training data: 77 / 95\n",
      "Cost on evaluation data: 0.3755323091989094\n",
      "Accuracy on evaluation data: 44 / 55\n",
      "\n",
      "Epoch 96 training complete\n",
      "Cost on training data: 0.37234021747542584\n",
      "Accuracy on training data: 80 / 95\n",
      "Cost on evaluation data: 0.34343566351042465\n",
      "Accuracy on evaluation data: 49 / 55\n",
      "\n",
      "Epoch 97 training complete\n",
      "Cost on training data: 0.33682784948715133\n",
      "Accuracy on training data: 80 / 95\n",
      "Cost on evaluation data: 0.3218525684831648\n",
      "Accuracy on evaluation data: 48 / 55\n",
      "\n",
      "Epoch 98 training complete\n",
      "Cost on training data: 0.29317220744438227\n",
      "Accuracy on training data: 85 / 95\n",
      "Cost on evaluation data: 0.2743385349773472\n",
      "Accuracy on evaluation data: 52 / 55\n",
      "\n",
      "Epoch 99 training complete\n",
      "Cost on training data: 0.35016766991649756\n",
      "Accuracy on training data: 78 / 95\n",
      "Cost on evaluation data: 0.3068958718291028\n",
      "Accuracy on evaluation data: 46 / 55\n",
      "\n",
      "Epoch 100 training complete\n",
      "Cost on training data: 0.29876024535528883\n",
      "Accuracy on training data: 82 / 95\n",
      "Cost on evaluation data: 0.25457322980061786\n",
      "Accuracy on evaluation data: 50 / 55\n",
      "\n",
      "Epoch 101 training complete\n",
      "Cost on training data: 0.19634290347161987\n",
      "Accuracy on training data: 86 / 95\n",
      "Cost on evaluation data: 0.12697884860890868\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 102 training complete\n",
      "Cost on training data: 0.27762788493745977\n",
      "Accuracy on training data: 90 / 95\n",
      "Cost on evaluation data: 0.2431323722420801\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 103 training complete\n",
      "Cost on training data: 0.26044057483155686\n",
      "Accuracy on training data: 84 / 95\n",
      "Cost on evaluation data: 0.2129435993559831\n",
      "Accuracy on evaluation data: 51 / 55\n",
      "\n",
      "Epoch 104 training complete\n",
      "Cost on training data: 0.19301747275913983\n",
      "Accuracy on training data: 92 / 95\n",
      "Cost on evaluation data: 0.14704962683218023\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 105 training complete\n",
      "Cost on training data: 0.33977750105458465\n",
      "Accuracy on training data: 84 / 95\n",
      "Cost on evaluation data: 0.28807721288210997\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 106 training complete\n",
      "Cost on training data: 0.22818273647392714\n",
      "Accuracy on training data: 93 / 95\n",
      "Cost on evaluation data: 0.18930415081994342\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 107 training complete\n",
      "Cost on training data: 0.34996534162432386\n",
      "Accuracy on training data: 81 / 95\n",
      "Cost on evaluation data: 0.289752238426487\n",
      "Accuracy on evaluation data: 49 / 55\n",
      "\n",
      "Epoch 108 training complete\n",
      "Cost on training data: 0.352369746047956\n",
      "Accuracy on training data: 80 / 95\n",
      "Cost on evaluation data: 0.30154058657889854\n",
      "Accuracy on evaluation data: 48 / 55\n",
      "\n",
      "Epoch 109 training complete\n",
      "Cost on training data: 0.29150431335282345\n",
      "Accuracy on training data: 86 / 95\n",
      "Cost on evaluation data: 0.2530926104015848\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 110 training complete\n",
      "Cost on training data: 0.3931645888570352\n",
      "Accuracy on training data: 67 / 95\n",
      "Cost on evaluation data: 0.38845904643495355\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 111 training complete\n",
      "Cost on training data: 0.3171112860755622\n",
      "Accuracy on training data: 85 / 95\n",
      "Cost on evaluation data: 0.2425915028260658\n",
      "Accuracy on evaluation data: 52 / 55\n",
      "\n",
      "Epoch 112 training complete\n",
      "Cost on training data: 0.24830922668169256\n",
      "Accuracy on training data: 91 / 95\n",
      "Cost on evaluation data: 0.22106389455534564\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 113 training complete\n",
      "Cost on training data: 0.3579918819235349\n",
      "Accuracy on training data: 78 / 95\n",
      "Cost on evaluation data: 0.3409334461579822\n",
      "Accuracy on evaluation data: 48 / 55\n",
      "\n",
      "Epoch 114 training complete\n",
      "Cost on training data: 0.5132885713679756\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5539382401554279\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 115 training complete\n",
      "Cost on training data: 0.37870248160161385\n",
      "Accuracy on training data: 83 / 95\n",
      "Cost on evaluation data: 0.33327722088787937\n",
      "Accuracy on evaluation data: 51 / 55\n",
      "\n",
      "Epoch 116 training complete\n",
      "Cost on training data: 0.2808233889516715\n",
      "Accuracy on training data: 83 / 95\n",
      "Cost on evaluation data: 0.22248514631678507\n",
      "Accuracy on evaluation data: 51 / 55\n",
      "\n",
      "Epoch 117 training complete\n",
      "Cost on training data: 0.2932556895613408\n",
      "Accuracy on training data: 83 / 95\n",
      "Cost on evaluation data: 0.22733053421733224\n",
      "Accuracy on evaluation data: 52 / 55\n",
      "\n",
      "Epoch 118 training complete\n",
      "Cost on training data: 0.14630852266693029\n",
      "Accuracy on training data: 89 / 95\n",
      "Cost on evaluation data: 0.07221529931240481\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 119 training complete\n",
      "Cost on training data: 0.3536521650968762\n",
      "Accuracy on training data: 80 / 95\n",
      "Cost on evaluation data: 0.2914850257937166\n",
      "Accuracy on evaluation data: 48 / 55\n",
      "\n",
      "Epoch 120 training complete\n",
      "Cost on training data: 0.31075026186292265\n",
      "Accuracy on training data: 81 / 95\n",
      "Cost on evaluation data: 0.27765738782193394\n",
      "Accuracy on evaluation data: 52 / 55\n",
      "\n",
      "Epoch 121 training complete\n",
      "Cost on training data: 0.2996245758648527\n",
      "Accuracy on training data: 72 / 95\n",
      "Cost on evaluation data: 0.28393654239694033\n",
      "Accuracy on evaluation data: 45 / 55\n",
      "\n",
      "Epoch 122 training complete\n",
      "Cost on training data: 0.22479804217133448\n",
      "Accuracy on training data: 89 / 95\n",
      "Cost on evaluation data: 0.18109790029162487\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 123 training complete\n",
      "Cost on training data: 0.25939239957466204\n",
      "Accuracy on training data: 80 / 95\n",
      "Cost on evaluation data: 0.2247292030057711\n",
      "Accuracy on evaluation data: 52 / 55\n",
      "\n",
      "Epoch 124 training complete\n",
      "Cost on training data: 0.17667490138784495\n",
      "Accuracy on training data: 87 / 95\n",
      "Cost on evaluation data: 0.12384627274221686\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 125 training complete\n",
      "Cost on training data: 0.24959305895008166\n",
      "Accuracy on training data: 82 / 95\n",
      "Cost on evaluation data: 0.19277894173311355\n",
      "Accuracy on evaluation data: 52 / 55\n",
      "\n",
      "Epoch 126 training complete\n",
      "Cost on training data: 0.24772715770434584\n",
      "Accuracy on training data: 83 / 95\n",
      "Cost on evaluation data: 0.20130575633525405\n",
      "Accuracy on evaluation data: 52 / 55\n",
      "\n",
      "Epoch 127 training complete\n",
      "Cost on training data: 0.20400218663186445\n",
      "Accuracy on training data: 88 / 95\n",
      "Cost on evaluation data: 0.15185183690579032\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 128 training complete\n",
      "Cost on training data: 0.28779879642178985\n",
      "Accuracy on training data: 81 / 95\n",
      "Cost on evaluation data: 0.18104669101702334\n",
      "Accuracy on evaluation data: 51 / 55\n",
      "\n",
      "Epoch 129 training complete\n",
      "Cost on training data: 0.18349238442053561\n",
      "Accuracy on training data: 90 / 95\n",
      "Cost on evaluation data: 0.14158044355859692\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 130 training complete\n",
      "Cost on training data: 0.15053316417303195\n",
      "Accuracy on training data: 93 / 95\n",
      "Cost on evaluation data: 0.10978065777250284\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 131 training complete\n",
      "Cost on training data: 0.3043833282513741\n",
      "Accuracy on training data: 80 / 95\n",
      "Cost on evaluation data: 0.2190805061959622\n",
      "Accuracy on evaluation data: 52 / 55\n",
      "\n",
      "Epoch 132 training complete\n",
      "Cost on training data: 0.19690621344549641\n",
      "Accuracy on training data: 87 / 95\n",
      "Cost on evaluation data: 0.14257331530893258\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 133 training complete\n",
      "Cost on training data: 0.1590753483803434\n",
      "Accuracy on training data: 89 / 95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on evaluation data: 0.11174598121090681\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 134 training complete\n",
      "Cost on training data: 0.22588918031789088\n",
      "Accuracy on training data: 86 / 95\n",
      "Cost on evaluation data: 0.16881727465075855\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 135 training complete\n",
      "Cost on training data: 0.16265484170254427\n",
      "Accuracy on training data: 90 / 95\n",
      "Cost on evaluation data: 0.12748680550608854\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 136 training complete\n",
      "Cost on training data: 0.1032281538643379\n",
      "Accuracy on training data: 93 / 95\n",
      "Cost on evaluation data: 0.07541088913439135\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 137 training complete\n",
      "Cost on training data: 0.21888442699526264\n",
      "Accuracy on training data: 87 / 95\n",
      "Cost on evaluation data: 0.1882420108495111\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 138 training complete\n",
      "Cost on training data: 0.23074542526381409\n",
      "Accuracy on training data: 87 / 95\n",
      "Cost on evaluation data: 0.19336819349987333\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 139 training complete\n",
      "Cost on training data: 0.295730938458594\n",
      "Accuracy on training data: 83 / 95\n",
      "Cost on evaluation data: 0.22670027143662427\n",
      "Accuracy on evaluation data: 52 / 55\n",
      "\n",
      "Epoch 140 training complete\n",
      "Cost on training data: 0.19290285011536087\n",
      "Accuracy on training data: 93 / 95\n",
      "Cost on evaluation data: 0.16066746303048593\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 141 training complete\n",
      "Cost on training data: 0.29143386692776724\n",
      "Accuracy on training data: 85 / 95\n",
      "Cost on evaluation data: 0.2470305048722136\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 142 training complete\n",
      "Cost on training data: 0.2700043443039865\n",
      "Accuracy on training data: 85 / 95\n",
      "Cost on evaluation data: 0.2251196264458364\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 143 training complete\n",
      "Cost on training data: 0.2794775444902732\n",
      "Accuracy on training data: 83 / 95\n",
      "Cost on evaluation data: 0.23043090475082648\n",
      "Accuracy on evaluation data: 52 / 55\n",
      "\n",
      "Epoch 144 training complete\n",
      "Cost on training data: 0.18736288957913172\n",
      "Accuracy on training data: 94 / 95\n",
      "Cost on evaluation data: 0.14087501761937815\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 145 training complete\n",
      "Cost on training data: 0.3709518954805212\n",
      "Accuracy on training data: 77 / 95\n",
      "Cost on evaluation data: 0.27166623661647477\n",
      "Accuracy on evaluation data: 49 / 55\n",
      "\n",
      "Epoch 146 training complete\n",
      "Cost on training data: 0.2947718747607091\n",
      "Accuracy on training data: 83 / 95\n",
      "Cost on evaluation data: 0.23277251416368594\n",
      "Accuracy on evaluation data: 52 / 55\n",
      "\n",
      "Epoch 147 training complete\n",
      "Cost on training data: 0.27315649429921884\n",
      "Accuracy on training data: 84 / 95\n",
      "Cost on evaluation data: 0.21001582253866463\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 148 training complete\n",
      "Cost on training data: 0.193810386096638\n",
      "Accuracy on training data: 87 / 95\n",
      "Cost on evaluation data: 0.13992921512234097\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 149 training complete\n",
      "Cost on training data: 0.17854533610159634\n",
      "Accuracy on training data: 89 / 95\n",
      "Cost on evaluation data: 0.1387838106280997\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 150 training complete\n",
      "Cost on training data: 0.14978452675033752\n",
      "Accuracy on training data: 87 / 95\n",
      "Cost on evaluation data: 0.10264338217196425\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 151 training complete\n",
      "Cost on training data: 0.22930759879287627\n",
      "Accuracy on training data: 88 / 95\n",
      "Cost on evaluation data: 0.1955105158199507\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 152 training complete\n",
      "Cost on training data: 0.1683279422790566\n",
      "Accuracy on training data: 90 / 95\n",
      "Cost on evaluation data: 0.12187872009430632\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 153 training complete\n",
      "Cost on training data: 0.13730822471801507\n",
      "Accuracy on training data: 88 / 95\n",
      "Cost on evaluation data: 0.08260702554595602\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 154 training complete\n",
      "Cost on training data: 0.15192172825140757\n",
      "Accuracy on training data: 89 / 95\n",
      "Cost on evaluation data: 0.10104599656405652\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 155 training complete\n",
      "Cost on training data: 0.17124841064055107\n",
      "Accuracy on training data: 87 / 95\n",
      "Cost on evaluation data: 0.12430414419298183\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 156 training complete\n",
      "Cost on training data: 0.0925569163475596\n",
      "Accuracy on training data: 94 / 95\n",
      "Cost on evaluation data: 0.0579636468882908\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 157 training complete\n",
      "Cost on training data: 0.1373347718686275\n",
      "Accuracy on training data: 90 / 95\n",
      "Cost on evaluation data: 0.0938140851061522\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 158 training complete\n",
      "Cost on training data: 0.181884579939211\n",
      "Accuracy on training data: 90 / 95\n",
      "Cost on evaluation data: 0.1404781945596566\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 159 training complete\n",
      "Cost on training data: 0.21444457453646693\n",
      "Accuracy on training data: 87 / 95\n",
      "Cost on evaluation data: 0.10077903726650456\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 160 training complete\n",
      "Cost on training data: 0.17626488697239986\n",
      "Accuracy on training data: 92 / 95\n",
      "Cost on evaluation data: 0.13660526195507527\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 161 training complete\n",
      "Cost on training data: 0.1636801053603809\n",
      "Accuracy on training data: 88 / 95\n",
      "Cost on evaluation data: 0.11672080732970244\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 162 training complete\n",
      "Cost on training data: 0.15673404282421055\n",
      "Accuracy on training data: 94 / 95\n",
      "Cost on evaluation data: 0.12362335488915256\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 163 training complete\n",
      "Cost on training data: 0.1088521452377157\n",
      "Accuracy on training data: 93 / 95\n",
      "Cost on evaluation data: 0.07929681166351848\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 164 training complete\n",
      "Cost on training data: 0.12103159836889618\n",
      "Accuracy on training data: 94 / 95\n",
      "Cost on evaluation data: 0.08399217325682325\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 165 training complete\n",
      "Cost on training data: 0.1373503342129476\n",
      "Accuracy on training data: 89 / 95\n",
      "Cost on evaluation data: 0.09002159430373177\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 166 training complete\n",
      "Cost on training data: 0.30104930709212907\n",
      "Accuracy on training data: 83 / 95\n",
      "Cost on evaluation data: 0.1736295171310217\n",
      "Accuracy on evaluation data: 52 / 55\n",
      "\n",
      "Epoch 167 training complete\n",
      "Cost on training data: 0.18281016796938743\n",
      "Accuracy on training data: 87 / 95\n",
      "Cost on evaluation data: 0.10623562390456302\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 168 training complete\n",
      "Cost on training data: 0.2130354540410634\n",
      "Accuracy on training data: 87 / 95\n",
      "Cost on evaluation data: 0.11910295392432253\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 169 training complete\n",
      "Cost on training data: 0.1287468994436223\n",
      "Accuracy on training data: 89 / 95\n",
      "Cost on evaluation data: 0.08315453350256688\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 170 training complete\n",
      "Cost on training data: 0.29912003564487316\n",
      "Accuracy on training data: 85 / 95\n",
      "Cost on evaluation data: 0.1513526497748859\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 171 training complete\n",
      "Cost on training data: 0.12054977203655601\n",
      "Accuracy on training data: 89 / 95\n",
      "Cost on evaluation data: 0.07518258748932\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 172 training complete\n",
      "Cost on training data: 0.14147800923537351\n",
      "Accuracy on training data: 92 / 95\n",
      "Cost on evaluation data: 0.1085255567117648\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 173 training complete\n",
      "Cost on training data: 0.21235635844616266\n",
      "Accuracy on training data: 87 / 95\n",
      "Cost on evaluation data: 0.16512137995061138\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 174 training complete\n",
      "Cost on training data: 0.09152529468636407\n",
      "Accuracy on training data: 93 / 95\n",
      "Cost on evaluation data: 0.06608847057416696\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 175 training complete\n",
      "Cost on training data: 0.7101372949195631\n",
      "Accuracy on training data: 71 / 95\n",
      "Cost on evaluation data: 0.5341197420494517\n",
      "Accuracy on evaluation data: 43 / 55\n",
      "\n",
      "Epoch 176 training complete\n",
      "Cost on training data: 0.17295400176663356\n",
      "Accuracy on training data: 89 / 95\n",
      "Cost on evaluation data: 0.13561422235003204\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 177 training complete\n",
      "Cost on training data: 0.16587426866975513\n",
      "Accuracy on training data: 88 / 95\n",
      "Cost on evaluation data: 0.10839970989923843\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178 training complete\n",
      "Cost on training data: 0.1949611404818274\n",
      "Accuracy on training data: 86 / 95\n",
      "Cost on evaluation data: 0.07185312294997033\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 179 training complete\n",
      "Cost on training data: 0.18482144539364748\n",
      "Accuracy on training data: 90 / 95\n",
      "Cost on evaluation data: 0.12830375944769204\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 180 training complete\n",
      "Cost on training data: 0.45090574434665537\n",
      "Accuracy on training data: 85 / 95\n",
      "Cost on evaluation data: 0.18819929023728027\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 181 training complete\n",
      "Cost on training data: 0.08958039012752726\n",
      "Accuracy on training data: 94 / 95\n",
      "Cost on evaluation data: 0.038336420196122885\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 182 training complete\n",
      "Cost on training data: 0.249053814732886\n",
      "Accuracy on training data: 93 / 95\n",
      "Cost on evaluation data: 0.21038301892422778\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 183 training complete\n",
      "Cost on training data: 0.3378400491415598\n",
      "Accuracy on training data: 82 / 95\n",
      "Cost on evaluation data: 0.1945041418834939\n",
      "Accuracy on evaluation data: 52 / 55\n",
      "\n",
      "Epoch 184 training complete\n",
      "Cost on training data: 0.11163592585479833\n",
      "Accuracy on training data: 93 / 95\n",
      "Cost on evaluation data: 0.08041616260012886\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 185 training complete\n",
      "Cost on training data: 0.10423722679811123\n",
      "Accuracy on training data: 91 / 95\n",
      "Cost on evaluation data: 0.06430722833560892\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 186 training complete\n",
      "Cost on training data: 0.1575058440221353\n",
      "Accuracy on training data: 90 / 95\n",
      "Cost on evaluation data: 0.10825012084833222\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 187 training complete\n",
      "Cost on training data: 0.3259257407677643\n",
      "Accuracy on training data: 80 / 95\n",
      "Cost on evaluation data: 0.23637031956929166\n",
      "Accuracy on evaluation data: 52 / 55\n",
      "\n",
      "Epoch 188 training complete\n",
      "Cost on training data: 0.24510299526291093\n",
      "Accuracy on training data: 87 / 95\n",
      "Cost on evaluation data: 0.16851224643434823\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 189 training complete\n",
      "Cost on training data: 0.1527373662058138\n",
      "Accuracy on training data: 93 / 95\n",
      "Cost on evaluation data: 0.12016181055789377\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 190 training complete\n",
      "Cost on training data: 0.15242926229432538\n",
      "Accuracy on training data: 93 / 95\n",
      "Cost on evaluation data: 0.1208337575733262\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 191 training complete\n",
      "Cost on training data: 0.21850544244235998\n",
      "Accuracy on training data: 87 / 95\n",
      "Cost on evaluation data: 0.1490443893661855\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 192 training complete\n",
      "Cost on training data: 0.17836415160045657\n",
      "Accuracy on training data: 87 / 95\n",
      "Cost on evaluation data: 0.1142453599346965\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 193 training complete\n",
      "Cost on training data: 0.1401045216562318\n",
      "Accuracy on training data: 88 / 95\n",
      "Cost on evaluation data: 0.09041374986533235\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 194 training complete\n",
      "Cost on training data: 0.10348405645640216\n",
      "Accuracy on training data: 93 / 95\n",
      "Cost on evaluation data: 0.0702068266433357\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 195 training complete\n",
      "Cost on training data: 0.17259922620217985\n",
      "Accuracy on training data: 87 / 95\n",
      "Cost on evaluation data: 0.09283905594156282\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 196 training complete\n",
      "Cost on training data: 0.18066709037530918\n",
      "Accuracy on training data: 87 / 95\n",
      "Cost on evaluation data: 0.1216763780370808\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 197 training complete\n",
      "Cost on training data: 0.08772878935622289\n",
      "Accuracy on training data: 94 / 95\n",
      "Cost on evaluation data: 0.05604347702619809\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 198 training complete\n",
      "Cost on training data: 0.25651565703277573\n",
      "Accuracy on training data: 85 / 95\n",
      "Cost on evaluation data: 0.15470225597855933\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 199 training complete\n",
      "Cost on training data: 0.21773404499120436\n",
      "Accuracy on training data: 86 / 95\n",
      "Cost on evaluation data: 0.1509911608006212\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net2 = network2.load_network(\"./data/iris-4-20-7-3.dat\")\n",
    "\n",
    "# Set hyper-parameter values individually after the network\n",
    "net2.set_parameters(cost=network2.LogLikelihood,\n",
    "                    act_hidden = network2.ReLU,\n",
    "                    act_output = network2.Softmax,\n",
    "                    regularization=None, \n",
    "                    dropoutpercent=0.1)\n",
    "\n",
    "evaluation_cost9b, evaluation_accuracy9b, training_cost9b, training_accuracy9b = net2.SGD(iris_train, 200, 1, 0.05,\n",
    "         lmbda=0.0,\n",
    "         evaluation_data=iris_test, \n",
    "         monitor_evaluation_cost=True, \n",
    "         monitor_evaluation_accuracy=True,\n",
    "         monitor_training_cost=True,\n",
    "         monitor_training_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2-10) ReLU hidden + Softmax output + LogLikelihood + dropout=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.sizes [4, 20, 7, 3]\n",
      "Epoch 0 training complete\n",
      "Cost on training data: 1.1629585567984275\n",
      "Accuracy on training data: 32 / 95\n",
      "Cost on evaluation data: 1.1864524800582021\n",
      "Accuracy on evaluation data: 18 / 55\n",
      "\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 1.1097333394900661\n",
      "Accuracy on training data: 32 / 95\n",
      "Cost on evaluation data: 1.1266645329974772\n",
      "Accuracy on evaluation data: 18 / 55\n",
      "\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 1.1006733710386802\n",
      "Accuracy on training data: 32 / 95\n",
      "Cost on evaluation data: 1.1097088698129811\n",
      "Accuracy on evaluation data: 18 / 55\n",
      "\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 1.0980600871259705\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.104495653116534\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 1.098328998314477\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1060156586996406\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 1.0987950863863687\n",
      "Accuracy on training data: 32 / 95\n",
      "Cost on evaluation data: 1.1061031745316046\n",
      "Accuracy on evaluation data: 18 / 55\n",
      "\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 1.098727087051333\n",
      "Accuracy on training data: 32 / 95\n",
      "Cost on evaluation data: 1.1052851273008888\n",
      "Accuracy on evaluation data: 18 / 55\n",
      "\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 1.1004312500835485\n",
      "Accuracy on training data: 32 / 95\n",
      "Cost on evaluation data: 1.1049615408812883\n",
      "Accuracy on evaluation data: 18 / 55\n",
      "\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 1.0989992155323531\n",
      "Accuracy on training data: 32 / 95\n",
      "Cost on evaluation data: 1.1032504403492815\n",
      "Accuracy on evaluation data: 18 / 55\n",
      "\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 1.0986117525614607\n",
      "Accuracy on training data: 32 / 95\n",
      "Cost on evaluation data: 1.1050380552674857\n",
      "Accuracy on evaluation data: 18 / 55\n",
      "\n",
      "Epoch 10 training complete\n",
      "Cost on training data: 1.0980019157490715\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1033983987351053\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 11 training complete\n",
      "Cost on training data: 1.0983562568523417\n",
      "Accuracy on training data: 32 / 95\n",
      "Cost on evaluation data: 1.1051719040632968\n",
      "Accuracy on evaluation data: 18 / 55\n",
      "\n",
      "Epoch 12 training complete\n",
      "Cost on training data: 1.0996112197668364\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.1043074878155714\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 13 training complete\n",
      "Cost on training data: 1.1021395944113368\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.10205301812201\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 14 training complete\n",
      "Cost on training data: 1.0632689073643629\n",
      "Accuracy on training data: 40 / 95\n",
      "Cost on evaluation data: 1.068884715769897\n",
      "Accuracy on evaluation data: 23 / 55\n",
      "\n",
      "Epoch 15 training complete\n",
      "Cost on training data: 0.8080247999508664\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.8191931234209716\n",
      "Accuracy on evaluation data: 34 / 55\n",
      "\n",
      "Epoch 16 training complete\n",
      "Cost on training data: 0.8246603403293616\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.8354141997822003\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 17 training complete\n",
      "Cost on training data: 0.6929984274934152\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7065444877175341\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 18 training complete\n",
      "Cost on training data: 0.8492300414544807\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.8593638376308274\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 19 training complete\n",
      "Cost on training data: 0.6687621859615464\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6864549776862952\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 20 training complete\n",
      "Cost on training data: 0.6406120786078874\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6616778192648207\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 21 training complete\n",
      "Cost on training data: 0.6253050897522108\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6414114523726436\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 22 training complete\n",
      "Cost on training data: 0.6012346782159245\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.618296096871817\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 23 training complete\n",
      "Cost on training data: 0.5564411606672567\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5752533843983484\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 24 training complete\n",
      "Cost on training data: 0.5399277942956558\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5576996912678794\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 25 training complete\n",
      "Cost on training data: 0.539630830956343\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5653735688501864\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 26 training complete\n",
      "Cost on training data: 0.5421933929398896\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5611993128620343\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 27 training complete\n",
      "Cost on training data: 0.49665422280376403\n",
      "Accuracy on training data: 87 / 95\n",
      "Cost on evaluation data: 0.4999310502882608\n",
      "Accuracy on evaluation data: 51 / 55\n",
      "\n",
      "Epoch 28 training complete\n",
      "Cost on training data: 0.5181397729965925\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5301073893755375\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 29 training complete\n",
      "Cost on training data: 0.4780716991569214\n",
      "Accuracy on training data: 70 / 95\n",
      "Cost on evaluation data: 0.48861038326268763\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 30 training complete\n",
      "Cost on training data: 0.43211549216302936\n",
      "Accuracy on training data: 88 / 95\n",
      "Cost on evaluation data: 0.4318050899419339\n",
      "Accuracy on evaluation data: 52 / 55\n",
      "\n",
      "Epoch 31 training complete\n",
      "Cost on training data: 0.5240198241968946\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5440267233537295\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 32 training complete\n",
      "Cost on training data: 0.5269971707494066\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5388590838921817\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 33 training complete\n",
      "Cost on training data: 0.5109061754215813\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5327137432591877\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 34 training complete\n",
      "Cost on training data: 0.45654062187389366\n",
      "Accuracy on training data: 71 / 95\n",
      "Cost on evaluation data: 0.4636143220201046\n",
      "Accuracy on evaluation data: 38 / 55\n",
      "\n",
      "Epoch 35 training complete\n",
      "Cost on training data: 0.47318699847827683\n",
      "Accuracy on training data: 67 / 95\n",
      "Cost on evaluation data: 0.4833299860583657\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 36 training complete\n",
      "Cost on training data: 0.4231202435451687\n",
      "Accuracy on training data: 72 / 95\n",
      "Cost on evaluation data: 0.4306534002726511\n",
      "Accuracy on evaluation data: 38 / 55\n",
      "\n",
      "Epoch 37 training complete\n",
      "Cost on training data: 0.47550759111147756\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.49335900330153765\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 38 training complete\n",
      "Cost on training data: 0.46305748376062467\n",
      "Accuracy on training data: 66 / 95\n",
      "Cost on evaluation data: 0.4742911016291861\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 39 training complete\n",
      "Cost on training data: 0.5129579362838796\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.542414271197485\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 40 training complete\n",
      "Cost on training data: 0.4502518200920423\n",
      "Accuracy on training data: 75 / 95\n",
      "Cost on evaluation data: 0.4524013828855125\n",
      "Accuracy on evaluation data: 45 / 55\n",
      "\n",
      "Epoch 41 training complete\n",
      "Cost on training data: 0.4830072790104559\n",
      "Accuracy on training data: 73 / 95\n",
      "Cost on evaluation data: 0.48751281991818346\n",
      "Accuracy on evaluation data: 39 / 55\n",
      "\n",
      "Epoch 42 training complete\n",
      "Cost on training data: 0.43053643134339636\n",
      "Accuracy on training data: 82 / 95\n",
      "Cost on evaluation data: 0.42781658424557156\n",
      "Accuracy on evaluation data: 48 / 55\n",
      "\n",
      "Epoch 43 training complete\n",
      "Cost on training data: 0.5124255732136886\n",
      "Accuracy on training data: 66 / 95\n",
      "Cost on evaluation data: 0.5260581932646866\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 44 training complete\n",
      "Cost on training data: 0.4715483073759084\n",
      "Accuracy on training data: 66 / 95\n",
      "Cost on evaluation data: 0.48258306772508297\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 45 training complete\n",
      "Cost on training data: 0.5205337571318357\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5418296035125926\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 training complete\n",
      "Cost on training data: 0.4222334394696623\n",
      "Accuracy on training data: 88 / 95\n",
      "Cost on evaluation data: 0.41578781202970827\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 47 training complete\n",
      "Cost on training data: 0.46534569142964644\n",
      "Accuracy on training data: 66 / 95\n",
      "Cost on evaluation data: 0.4722949697518876\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 48 training complete\n",
      "Cost on training data: 0.452448666749315\n",
      "Accuracy on training data: 75 / 95\n",
      "Cost on evaluation data: 0.4514738387944453\n",
      "Accuracy on evaluation data: 43 / 55\n",
      "\n",
      "Epoch 49 training complete\n",
      "Cost on training data: 0.3665300995513667\n",
      "Accuracy on training data: 92 / 95\n",
      "Cost on evaluation data: 0.34467960618354937\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 50 training complete\n",
      "Cost on training data: 0.501104542805517\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5224341661873748\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 51 training complete\n",
      "Cost on training data: 0.5116979801138427\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5285430684587817\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 52 training complete\n",
      "Cost on training data: 0.48386152202577537\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5027678618747203\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 53 training complete\n",
      "Cost on training data: 0.35855501568011117\n",
      "Accuracy on training data: 73 / 95\n",
      "Cost on evaluation data: 0.30999920891308724\n",
      "Accuracy on evaluation data: 45 / 55\n",
      "\n",
      "Epoch 54 training complete\n",
      "Cost on training data: 0.3834579312139333\n",
      "Accuracy on training data: 78 / 95\n",
      "Cost on evaluation data: 0.3791219873473562\n",
      "Accuracy on evaluation data: 46 / 55\n",
      "\n",
      "Epoch 55 training complete\n",
      "Cost on training data: 0.4791673952641294\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4930470200157635\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 56 training complete\n",
      "Cost on training data: 0.4227131817120184\n",
      "Accuracy on training data: 80 / 95\n",
      "Cost on evaluation data: 0.41685789076527097\n",
      "Accuracy on evaluation data: 50 / 55\n",
      "\n",
      "Epoch 57 training complete\n",
      "Cost on training data: 0.38763864998913206\n",
      "Accuracy on training data: 89 / 95\n",
      "Cost on evaluation data: 0.3770441794148506\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 58 training complete\n",
      "Cost on training data: 0.43892171933057295\n",
      "Accuracy on training data: 86 / 95\n",
      "Cost on evaluation data: 0.43442831966892376\n",
      "Accuracy on evaluation data: 52 / 55\n",
      "\n",
      "Epoch 59 training complete\n",
      "Cost on training data: 0.42652957613064385\n",
      "Accuracy on training data: 73 / 95\n",
      "Cost on evaluation data: 0.425729916588933\n",
      "Accuracy on evaluation data: 41 / 55\n",
      "\n",
      "Epoch 60 training complete\n",
      "Cost on training data: 0.4782199568103582\n",
      "Accuracy on training data: 66 / 95\n",
      "Cost on evaluation data: 0.4835994912868938\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 61 training complete\n",
      "Cost on training data: 0.3535575050671542\n",
      "Accuracy on training data: 90 / 95\n",
      "Cost on evaluation data: 0.3334221952784809\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 62 training complete\n",
      "Cost on training data: 0.6067161283832935\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6347483971527667\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 63 training complete\n",
      "Cost on training data: 0.5130181200570765\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5183082088534556\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 64 training complete\n",
      "Cost on training data: 0.4873274312384093\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4938621454174048\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 65 training complete\n",
      "Cost on training data: 0.4305047137320457\n",
      "Accuracy on training data: 76 / 95\n",
      "Cost on evaluation data: 0.42093639690615975\n",
      "Accuracy on evaluation data: 45 / 55\n",
      "\n",
      "Epoch 66 training complete\n",
      "Cost on training data: 0.435205368178903\n",
      "Accuracy on training data: 80 / 95\n",
      "Cost on evaluation data: 0.42004404815324264\n",
      "Accuracy on evaluation data: 50 / 55\n",
      "\n",
      "Epoch 67 training complete\n",
      "Cost on training data: 0.3600692818999096\n",
      "Accuracy on training data: 90 / 95\n",
      "Cost on evaluation data: 0.3370370716473347\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 68 training complete\n",
      "Cost on training data: 0.4795168126476447\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5012996328678\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 69 training complete\n",
      "Cost on training data: 0.4581786093840343\n",
      "Accuracy on training data: 69 / 95\n",
      "Cost on evaluation data: 0.46479057914753374\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 70 training complete\n",
      "Cost on training data: 0.44460245251533315\n",
      "Accuracy on training data: 88 / 95\n",
      "Cost on evaluation data: 0.44474289249764093\n",
      "Accuracy on evaluation data: 55 / 55\n",
      "\n",
      "Epoch 71 training complete\n",
      "Cost on training data: 0.4780632520013595\n",
      "Accuracy on training data: 73 / 95\n",
      "Cost on evaluation data: 0.48269859628823963\n",
      "Accuracy on evaluation data: 41 / 55\n",
      "\n",
      "Epoch 72 training complete\n",
      "Cost on training data: 0.4434102181118611\n",
      "Accuracy on training data: 77 / 95\n",
      "Cost on evaluation data: 0.44731307741795395\n",
      "Accuracy on evaluation data: 45 / 55\n",
      "\n",
      "Epoch 73 training complete\n",
      "Cost on training data: 0.4548077504478311\n",
      "Accuracy on training data: 71 / 95\n",
      "Cost on evaluation data: 0.46139774977228065\n",
      "Accuracy on evaluation data: 38 / 55\n",
      "\n",
      "Epoch 74 training complete\n",
      "Cost on training data: 0.4230291216247516\n",
      "Accuracy on training data: 88 / 95\n",
      "Cost on evaluation data: 0.4219211249268043\n",
      "Accuracy on evaluation data: 55 / 55\n",
      "\n",
      "Epoch 75 training complete\n",
      "Cost on training data: 0.43834327921927624\n",
      "Accuracy on training data: 91 / 95\n",
      "Cost on evaluation data: 0.4347982023480477\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 76 training complete\n",
      "Cost on training data: 0.5105488400627284\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5286244787964808\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 77 training complete\n",
      "Cost on training data: 0.48186845621987\n",
      "Accuracy on training data: 68 / 95\n",
      "Cost on evaluation data: 0.4854562426077191\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 78 training complete\n",
      "Cost on training data: 0.4933216985759445\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.504750269131125\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 79 training complete\n",
      "Cost on training data: 0.4852708964397552\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.49648492159979524\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 80 training complete\n",
      "Cost on training data: 0.43432984887434767\n",
      "Accuracy on training data: 86 / 95\n",
      "Cost on evaluation data: 0.4324584894002131\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 81 training complete\n",
      "Cost on training data: 0.45760237169541423\n",
      "Accuracy on training data: 71 / 95\n",
      "Cost on evaluation data: 0.4614317379770432\n",
      "Accuracy on evaluation data: 39 / 55\n",
      "\n",
      "Epoch 82 training complete\n",
      "Cost on training data: 0.48259992300870774\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4950516447340403\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 83 training complete\n",
      "Cost on training data: 0.4315926109636781\n",
      "Accuracy on training data: 92 / 95\n",
      "Cost on evaluation data: 0.4262722777130816\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 84 training complete\n",
      "Cost on training data: 0.39867676469717117\n",
      "Accuracy on training data: 92 / 95\n",
      "Cost on evaluation data: 0.38617688348619816\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 85 training complete\n",
      "Cost on training data: 0.4103043028851312\n",
      "Accuracy on training data: 83 / 95\n",
      "Cost on evaluation data: 0.40678511604213907\n",
      "Accuracy on evaluation data: 51 / 55\n",
      "\n",
      "Epoch 86 training complete\n",
      "Cost on training data: 0.4973475083412541\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5077765894285631\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 87 training complete\n",
      "Cost on training data: 0.43825961234491223\n",
      "Accuracy on training data: 79 / 95\n",
      "Cost on evaluation data: 0.439151669323995\n",
      "Accuracy on evaluation data: 47 / 55\n",
      "\n",
      "Epoch 88 training complete\n",
      "Cost on training data: 0.4061201648549185\n",
      "Accuracy on training data: 89 / 95\n",
      "Cost on evaluation data: 0.39361825404787604\n",
      "Accuracy on evaluation data: 55 / 55\n",
      "\n",
      "Epoch 89 training complete\n",
      "Cost on training data: 0.46146346155230095\n",
      "Accuracy on training data: 73 / 95\n",
      "Cost on evaluation data: 0.4587686102620881\n",
      "Accuracy on evaluation data: 41 / 55\n",
      "\n",
      "Epoch 90 training complete\n",
      "Cost on training data: 0.406857047273826\n",
      "Accuracy on training data: 83 / 95\n",
      "Cost on evaluation data: 0.402305464908556\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91 training complete\n",
      "Cost on training data: 0.43138882109178184\n",
      "Accuracy on training data: 86 / 95\n",
      "Cost on evaluation data: 0.4264622927388799\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 92 training complete\n",
      "Cost on training data: 0.4431525188665447\n",
      "Accuracy on training data: 69 / 95\n",
      "Cost on evaluation data: 0.445704855988317\n",
      "Accuracy on evaluation data: 38 / 55\n",
      "\n",
      "Epoch 93 training complete\n",
      "Cost on training data: 0.5304390271534536\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5342157423545276\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 94 training complete\n",
      "Cost on training data: 0.4796037272667612\n",
      "Accuracy on training data: 84 / 95\n",
      "Cost on evaluation data: 0.46344936310012735\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 95 training complete\n",
      "Cost on training data: 0.4177210383733021\n",
      "Accuracy on training data: 84 / 95\n",
      "Cost on evaluation data: 0.4147647873836347\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 96 training complete\n",
      "Cost on training data: 0.4305851189360443\n",
      "Accuracy on training data: 84 / 95\n",
      "Cost on evaluation data: 0.4207919951379621\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 97 training complete\n",
      "Cost on training data: 0.417398768155259\n",
      "Accuracy on training data: 89 / 95\n",
      "Cost on evaluation data: 0.4085787516432106\n",
      "Accuracy on evaluation data: 55 / 55\n",
      "\n",
      "Epoch 98 training complete\n",
      "Cost on training data: 0.4197969567500033\n",
      "Accuracy on training data: 83 / 95\n",
      "Cost on evaluation data: 0.415202050551742\n",
      "Accuracy on evaluation data: 51 / 55\n",
      "\n",
      "Epoch 99 training complete\n",
      "Cost on training data: 0.43505118494648337\n",
      "Accuracy on training data: 86 / 95\n",
      "Cost on evaluation data: 0.4271569388941475\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 100 training complete\n",
      "Cost on training data: 0.38103519573832534\n",
      "Accuracy on training data: 88 / 95\n",
      "Cost on evaluation data: 0.35932812812995546\n",
      "Accuracy on evaluation data: 52 / 55\n",
      "\n",
      "Epoch 101 training complete\n",
      "Cost on training data: 0.39766597827398814\n",
      "Accuracy on training data: 84 / 95\n",
      "Cost on evaluation data: 0.3883136596104185\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 102 training complete\n",
      "Cost on training data: 0.41397222159946145\n",
      "Accuracy on training data: 87 / 95\n",
      "Cost on evaluation data: 0.3940744859365449\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 103 training complete\n",
      "Cost on training data: 0.49406012142426364\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5096762532764652\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 104 training complete\n",
      "Cost on training data: 0.5237337716030764\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5417008693501777\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 105 training complete\n",
      "Cost on training data: 0.4078274784450705\n",
      "Accuracy on training data: 92 / 95\n",
      "Cost on evaluation data: 0.392730413089398\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 106 training complete\n",
      "Cost on training data: 0.36636625841053183\n",
      "Accuracy on training data: 79 / 95\n",
      "Cost on evaluation data: 0.3317661021644882\n",
      "Accuracy on evaluation data: 49 / 55\n",
      "\n",
      "Epoch 107 training complete\n",
      "Cost on training data: 0.3848913800485732\n",
      "Accuracy on training data: 84 / 95\n",
      "Cost on evaluation data: 0.36994144283122526\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 108 training complete\n",
      "Cost on training data: 0.47584554563152615\n",
      "Accuracy on training data: 66 / 95\n",
      "Cost on evaluation data: 0.48181393493599295\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 109 training complete\n",
      "Cost on training data: 0.4814802554842633\n",
      "Accuracy on training data: 71 / 95\n",
      "Cost on evaluation data: 0.47446764001052766\n",
      "Accuracy on evaluation data: 40 / 55\n",
      "\n",
      "Epoch 110 training complete\n",
      "Cost on training data: 0.42323824219520584\n",
      "Accuracy on training data: 85 / 95\n",
      "Cost on evaluation data: 0.4037642172265852\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 111 training complete\n",
      "Cost on training data: 0.37500201445950176\n",
      "Accuracy on training data: 88 / 95\n",
      "Cost on evaluation data: 0.3508789569761774\n",
      "Accuracy on evaluation data: 52 / 55\n",
      "\n",
      "Epoch 112 training complete\n",
      "Cost on training data: 0.4228394361112547\n",
      "Accuracy on training data: 89 / 95\n",
      "Cost on evaluation data: 0.3989177724505739\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 113 training complete\n",
      "Cost on training data: 0.5416645538699997\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5567570164649056\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 114 training complete\n",
      "Cost on training data: 0.42462304331660583\n",
      "Accuracy on training data: 84 / 95\n",
      "Cost on evaluation data: 0.4166586341728352\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 115 training complete\n",
      "Cost on training data: 0.36719834765951853\n",
      "Accuracy on training data: 86 / 95\n",
      "Cost on evaluation data: 0.33688518892822833\n",
      "Accuracy on evaluation data: 52 / 55\n",
      "\n",
      "Epoch 116 training complete\n",
      "Cost on training data: 0.4128366780714473\n",
      "Accuracy on training data: 82 / 95\n",
      "Cost on evaluation data: 0.39196644589459056\n",
      "Accuracy on evaluation data: 51 / 55\n",
      "\n",
      "Epoch 117 training complete\n",
      "Cost on training data: 0.4939169944332245\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.510005028755423\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 118 training complete\n",
      "Cost on training data: 0.4599535210724015\n",
      "Accuracy on training data: 81 / 95\n",
      "Cost on evaluation data: 0.4424053161122906\n",
      "Accuracy on evaluation data: 49 / 55\n",
      "\n",
      "Epoch 119 training complete\n",
      "Cost on training data: 0.431972174125415\n",
      "Accuracy on training data: 81 / 95\n",
      "Cost on evaluation data: 0.4191528020010821\n",
      "Accuracy on evaluation data: 49 / 55\n",
      "\n",
      "Epoch 120 training complete\n",
      "Cost on training data: 0.5425786207429432\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5562279001116709\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 121 training complete\n",
      "Cost on training data: 0.42314167159183974\n",
      "Accuracy on training data: 81 / 95\n",
      "Cost on evaluation data: 0.4083387720550377\n",
      "Accuracy on evaluation data: 51 / 55\n",
      "\n",
      "Epoch 122 training complete\n",
      "Cost on training data: 0.36578374030681826\n",
      "Accuracy on training data: 88 / 95\n",
      "Cost on evaluation data: 0.33346043419001326\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 123 training complete\n",
      "Cost on training data: 0.35249750848905276\n",
      "Accuracy on training data: 92 / 95\n",
      "Cost on evaluation data: 0.3243662636222056\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 124 training complete\n",
      "Cost on training data: 0.38919428101573855\n",
      "Accuracy on training data: 84 / 95\n",
      "Cost on evaluation data: 0.3732525430158592\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 125 training complete\n",
      "Cost on training data: 0.46569261807893747\n",
      "Accuracy on training data: 70 / 95\n",
      "Cost on evaluation data: 0.4630485019273891\n",
      "Accuracy on evaluation data: 40 / 55\n",
      "\n",
      "Epoch 126 training complete\n",
      "Cost on training data: 0.35135852083385677\n",
      "Accuracy on training data: 93 / 95\n",
      "Cost on evaluation data: 0.3285756166706743\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 127 training complete\n",
      "Cost on training data: 0.40035952057648305\n",
      "Accuracy on training data: 81 / 95\n",
      "Cost on evaluation data: 0.38683081873761666\n",
      "Accuracy on evaluation data: 50 / 55\n",
      "\n",
      "Epoch 128 training complete\n",
      "Cost on training data: 0.3576403725431176\n",
      "Accuracy on training data: 88 / 95\n",
      "Cost on evaluation data: 0.3185418775793135\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 129 training complete\n",
      "Cost on training data: 0.37188434898863787\n",
      "Accuracy on training data: 92 / 95\n",
      "Cost on evaluation data: 0.3434971748711834\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 130 training complete\n",
      "Cost on training data: 0.4135124715407818\n",
      "Accuracy on training data: 84 / 95\n",
      "Cost on evaluation data: 0.4046770810687902\n",
      "Accuracy on evaluation data: 52 / 55\n",
      "\n",
      "Epoch 131 training complete\n",
      "Cost on training data: 0.4901837235131784\n",
      "Accuracy on training data: 66 / 95\n",
      "Cost on evaluation data: 0.4980762429727543\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 132 training complete\n",
      "Cost on training data: 0.4040726733073337\n",
      "Accuracy on training data: 89 / 95\n",
      "Cost on evaluation data: 0.37954208405783035\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 133 training complete\n",
      "Cost on training data: 0.33826106841738246\n",
      "Accuracy on training data: 90 / 95\n",
      "Cost on evaluation data: 0.30764994887642694\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 134 training complete\n",
      "Cost on training data: 0.4359912547275019\n",
      "Accuracy on training data: 68 / 95\n",
      "Cost on evaluation data: 0.4393944716218437\n",
      "Accuracy on evaluation data: 40 / 55\n",
      "\n",
      "Epoch 135 training complete\n",
      "Cost on training data: 0.4525963610387573\n",
      "Accuracy on training data: 75 / 95\n",
      "Cost on evaluation data: 0.4478879672106602\n",
      "Accuracy on evaluation data: 46 / 55\n",
      "\n",
      "Epoch 136 training complete\n",
      "Cost on training data: 0.37373819905184985\n",
      "Accuracy on training data: 88 / 95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on evaluation data: 0.36228045933845215\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 137 training complete\n",
      "Cost on training data: 0.35562300138501046\n",
      "Accuracy on training data: 90 / 95\n",
      "Cost on evaluation data: 0.3226998498562627\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 138 training complete\n",
      "Cost on training data: 0.39979308914167583\n",
      "Accuracy on training data: 84 / 95\n",
      "Cost on evaluation data: 0.3650212400909059\n",
      "Accuracy on evaluation data: 52 / 55\n",
      "\n",
      "Epoch 139 training complete\n",
      "Cost on training data: 0.38278480973230544\n",
      "Accuracy on training data: 90 / 95\n",
      "Cost on evaluation data: 0.3479923991887311\n",
      "Accuracy on evaluation data: 55 / 55\n",
      "\n",
      "Epoch 140 training complete\n",
      "Cost on training data: 0.42170120389339305\n",
      "Accuracy on training data: 81 / 95\n",
      "Cost on evaluation data: 0.39174323112040216\n",
      "Accuracy on evaluation data: 51 / 55\n",
      "\n",
      "Epoch 141 training complete\n",
      "Cost on training data: 0.5315198667239183\n",
      "Accuracy on training data: 66 / 95\n",
      "Cost on evaluation data: 0.5386315629115526\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 142 training complete\n",
      "Cost on training data: 0.554240634701162\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5768345549201198\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 143 training complete\n",
      "Cost on training data: 0.34912342129816254\n",
      "Accuracy on training data: 89 / 95\n",
      "Cost on evaluation data: 0.31908468650599026\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 144 training complete\n",
      "Cost on training data: 0.33108979257202065\n",
      "Accuracy on training data: 93 / 95\n",
      "Cost on evaluation data: 0.29636977636085726\n",
      "Accuracy on evaluation data: 55 / 55\n",
      "\n",
      "Epoch 145 training complete\n",
      "Cost on training data: 0.5228985701653293\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5423342303376265\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 146 training complete\n",
      "Cost on training data: 0.5157400138744707\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5314342133112985\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 147 training complete\n",
      "Cost on training data: 0.46359302005909775\n",
      "Accuracy on training data: 80 / 95\n",
      "Cost on evaluation data: 0.4448198010599556\n",
      "Accuracy on evaluation data: 50 / 55\n",
      "\n",
      "Epoch 148 training complete\n",
      "Cost on training data: 0.31120734162330743\n",
      "Accuracy on training data: 92 / 95\n",
      "Cost on evaluation data: 0.2688539256275023\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 149 training complete\n",
      "Cost on training data: 0.3863039368996235\n",
      "Accuracy on training data: 86 / 95\n",
      "Cost on evaluation data: 0.3330502917496582\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 150 training complete\n",
      "Cost on training data: 0.43019164734873155\n",
      "Accuracy on training data: 80 / 95\n",
      "Cost on evaluation data: 0.38956896104088423\n",
      "Accuracy on evaluation data: 48 / 55\n",
      "\n",
      "Epoch 151 training complete\n",
      "Cost on training data: 0.3015121987966179\n",
      "Accuracy on training data: 93 / 95\n",
      "Cost on evaluation data: 0.25326430285936197\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 152 training complete\n",
      "Cost on training data: 0.5241585005924155\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5476902486320194\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 153 training complete\n",
      "Cost on training data: 0.49143339359057164\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5064807459378408\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 154 training complete\n",
      "Cost on training data: 0.4974547693422135\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5148106568046807\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 155 training complete\n",
      "Cost on training data: 0.5027554827695566\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5170591212701835\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 156 training complete\n",
      "Cost on training data: 0.5218962370880935\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5382020127208758\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 157 training complete\n",
      "Cost on training data: 0.4903360976557699\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5019877877934884\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 158 training complete\n",
      "Cost on training data: 0.5018217986196556\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5107974482550964\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 159 training complete\n",
      "Cost on training data: 0.5558823166237193\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5721071241295779\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 160 training complete\n",
      "Cost on training data: 0.5118230906966439\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5237554047806402\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 161 training complete\n",
      "Cost on training data: 0.4988621510716999\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5122697674554646\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 162 training complete\n",
      "Cost on training data: 0.5051513630964842\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5170101235196446\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 163 training complete\n",
      "Cost on training data: 0.4941151818868642\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5101637586341969\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 164 training complete\n",
      "Cost on training data: 0.49524649564189316\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5081743146149214\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 165 training complete\n",
      "Cost on training data: 0.5073965553902734\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5207791470917702\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 166 training complete\n",
      "Cost on training data: 0.4901344213051896\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5068609088012136\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 167 training complete\n",
      "Cost on training data: 0.4835377433650909\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4987726244493127\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 168 training complete\n",
      "Cost on training data: 0.4881325038770157\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.49659968882752464\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 169 training complete\n",
      "Cost on training data: 0.4806009398527655\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4924809355840243\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 170 training complete\n",
      "Cost on training data: 0.502010971774973\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5160333081627646\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 171 training complete\n",
      "Cost on training data: 0.5436257686869581\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5579429635149004\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 172 training complete\n",
      "Cost on training data: 0.5752614131538251\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5899322820654075\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 173 training complete\n",
      "Cost on training data: 0.5456394364602671\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5610174893680323\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 174 training complete\n",
      "Cost on training data: 0.5259624391312068\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5417478176428532\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 175 training complete\n",
      "Cost on training data: 0.5001871304672227\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5172754765878554\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 176 training complete\n",
      "Cost on training data: 0.5019819375517008\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5155910028488124\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 177 training complete\n",
      "Cost on training data: 0.5155966538313275\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5262849568319631\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 178 training complete\n",
      "Cost on training data: 0.48330498879446526\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.499509177544236\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 179 training complete\n",
      "Cost on training data: 0.48899240062777205\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5026880231522297\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 180 training complete\n",
      "Cost on training data: 0.5246545852852189\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5366664479397375\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 181 training complete\n",
      "Cost on training data: 0.4878998521777327\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5065839088471707\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182 training complete\n",
      "Cost on training data: 0.505392735730466\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5204493128737605\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 183 training complete\n",
      "Cost on training data: 0.492265728515503\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5066585135381786\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 184 training complete\n",
      "Cost on training data: 0.48441075965125774\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4978834686617198\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 185 training complete\n",
      "Cost on training data: 0.49386771934866286\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5013534461576094\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 186 training complete\n",
      "Cost on training data: 0.48366442795423636\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.49861439612839037\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 187 training complete\n",
      "Cost on training data: 0.49622482900629455\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5096291516349698\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 188 training complete\n",
      "Cost on training data: 0.48416144145320766\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4954582204960409\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 189 training complete\n",
      "Cost on training data: 0.4776684702594863\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4865003076564058\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 190 training complete\n",
      "Cost on training data: 0.4962183435230193\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5065603778928793\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 191 training complete\n",
      "Cost on training data: 0.49541749398276713\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.50522509639683\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 192 training complete\n",
      "Cost on training data: 0.48557321068186027\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5064039536810613\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 193 training complete\n",
      "Cost on training data: 0.4760325015052019\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.49176127784878904\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 194 training complete\n",
      "Cost on training data: 0.48420565538112\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.4957303367199444\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 195 training complete\n",
      "Cost on training data: 0.5043855062920147\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5166572693308776\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 196 training complete\n",
      "Cost on training data: 0.48802182769899727\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5029811285065087\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 197 training complete\n",
      "Cost on training data: 0.5063793021026033\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5193914666620786\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 198 training complete\n",
      "Cost on training data: 0.51478507914588\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5265344983922148\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 199 training complete\n",
      "Cost on training data: 0.501052070390548\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5140298097669358\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net2 = network2.load_network(\"./data/iris-4-20-7-3.dat\")\n",
    "\n",
    "# Set hyper-parameter values individually after the network\n",
    "net2.set_parameters(cost=network2.LogLikelihood,\n",
    "                    act_hidden = network2.ReLU,\n",
    "                    act_output = network2.Softmax,\n",
    "                    regularization=None, \n",
    "                    dropoutpercent=0.5)\n",
    "\n",
    "evaluation_cost10b, evaluation_accuracy10b, training_cost10b, training_accuracy10b =net2.SGD(iris_train, 200, 1, 0.05,\n",
    "         lmbda=0.0,\n",
    "         evaluation_data=iris_test, \n",
    "         monitor_evaluation_cost=True, \n",
    "         monitor_evaluation_accuracy=True,\n",
    "         monitor_training_cost=True,\n",
    "         monitor_training_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2-11) ReLU hidden + Softmax output + LogLikelihood + L2+ lmbda=3.0 +dropout=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.sizes [4, 20, 7, 3]\n",
      "Epoch 0 training complete\n",
      "Cost on training data: 1.0827071181624002\n",
      "Accuracy on training data: 62 / 95\n",
      "Cost on evaluation data: 1.3382754794907492\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 0.826804540827575\n",
      "Accuracy on training data: 80 / 95\n",
      "Cost on evaluation data: 1.0481143091932714\n",
      "Accuracy on evaluation data: 49 / 55\n",
      "\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 0.7647625684225705\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.9728494342047844\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 0.7153202833707653\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.8978762346499775\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 0.6990922616747106\n",
      "Accuracy on training data: 63 / 95\n",
      "Cost on evaluation data: 0.8179309497470073\n",
      "Accuracy on evaluation data: 38 / 55\n",
      "\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 0.6257567178592176\n",
      "Accuracy on training data: 75 / 95\n",
      "Cost on evaluation data: 0.7563609993046125\n",
      "Accuracy on evaluation data: 40 / 55\n",
      "\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 0.6472473064417914\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7798303540688731\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 0.592831651354484\n",
      "Accuracy on training data: 66 / 95\n",
      "Cost on evaluation data: 0.7002961212003049\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 0.6363739048430962\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.7531038885177772\n",
      "Accuracy on evaluation data: 34 / 55\n",
      "\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 0.6221133922784523\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.7323454065958915\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 10 training complete\n",
      "Cost on training data: 0.5574813998747119\n",
      "Accuracy on training data: 87 / 95\n",
      "Cost on evaluation data: 0.6467278177174662\n",
      "Accuracy on evaluation data: 52 / 55\n",
      "\n",
      "Epoch 11 training complete\n",
      "Cost on training data: 0.5545042233199229\n",
      "Accuracy on training data: 66 / 95\n",
      "Cost on evaluation data: 0.647222278818947\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 12 training complete\n",
      "Cost on training data: 0.5487422901058128\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.5972854043754525\n",
      "Accuracy on evaluation data: 41 / 55\n",
      "\n",
      "Epoch 13 training complete\n",
      "Cost on training data: 0.5716869736970941\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6677833608656482\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 14 training complete\n",
      "Cost on training data: 0.597499520596726\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.6991055386578227\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 15 training complete\n",
      "Cost on training data: 0.515208778396197\n",
      "Accuracy on training data: 76 / 95\n",
      "Cost on evaluation data: 0.5708158314904125\n",
      "Accuracy on evaluation data: 48 / 55\n",
      "\n",
      "Epoch 16 training complete\n",
      "Cost on training data: 0.6191877503947456\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.727897990067065\n",
      "Accuracy on evaluation data: 33 / 55\n",
      "\n",
      "Epoch 17 training complete\n",
      "Cost on training data: 0.5322004043497561\n",
      "Accuracy on training data: 74 / 95\n",
      "Cost on evaluation data: 0.6189020356156789\n",
      "Accuracy on evaluation data: 42 / 55\n",
      "\n",
      "Epoch 18 training complete\n",
      "Cost on training data: 0.5981609809058734\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7044043766680943\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 19 training complete\n",
      "Cost on training data: 0.6045187448198195\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.7124966353552603\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 20 training complete\n",
      "Cost on training data: 0.6082905404308121\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.7179590631837861\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 21 training complete\n",
      "Cost on training data: 0.6043651915665638\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.7138922107513451\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 22 training complete\n",
      "Cost on training data: 0.615717259040008\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.7297450664807081\n",
      "Accuracy on evaluation data: 34 / 55\n",
      "\n",
      "Epoch 23 training complete\n",
      "Cost on training data: 0.6486895613337443\n",
      "Accuracy on training data: 63 / 95\n",
      "Cost on evaluation data: 0.7609911718014888\n",
      "Accuracy on evaluation data: 32 / 55\n",
      "\n",
      "Epoch 24 training complete\n",
      "Cost on training data: 0.5950852625892372\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.7033599325449068\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 25 training complete\n",
      "Cost on training data: 0.5928640835035098\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.6976083692243376\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 26 training complete\n",
      "Cost on training data: 0.5370026653458939\n",
      "Accuracy on training data: 89 / 95\n",
      "Cost on evaluation data: 0.6142053941879391\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 27 training complete\n",
      "Cost on training data: 0.5252761163133142\n",
      "Accuracy on training data: 81 / 95\n",
      "Cost on evaluation data: 0.6050863076999379\n",
      "Accuracy on evaluation data: 47 / 55\n",
      "\n",
      "Epoch 28 training complete\n",
      "Cost on training data: 0.5244056691297877\n",
      "Accuracy on training data: 77 / 95\n",
      "Cost on evaluation data: 0.6048490696760481\n",
      "Accuracy on evaluation data: 46 / 55\n",
      "\n",
      "Epoch 29 training complete\n",
      "Cost on training data: 0.5633679202021807\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6584969818125406\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 30 training complete\n",
      "Cost on training data: 0.544769076218065\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.636238179131555\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 31 training complete\n",
      "Cost on training data: 0.5373657907540161\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.5907018911746764\n",
      "Accuracy on evaluation data: 42 / 55\n",
      "\n",
      "Epoch 32 training complete\n",
      "Cost on training data: 0.5469401904834735\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6055816899625526\n",
      "Accuracy on evaluation data: 42 / 55\n",
      "\n",
      "Epoch 33 training complete\n",
      "Cost on training data: 0.593495327660761\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.6994256535974039\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 34 training complete\n",
      "Cost on training data: 0.5733825374624161\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.6708581833379249\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 35 training complete\n",
      "Cost on training data: 0.5382109288396915\n",
      "Accuracy on training data: 69 / 95\n",
      "Cost on evaluation data: 0.6244438616133476\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 36 training complete\n",
      "Cost on training data: 0.4606743805682837\n",
      "Accuracy on training data: 90 / 95\n",
      "Cost on evaluation data: 0.527672166695113\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 37 training complete\n",
      "Cost on training data: 0.5685111067620462\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.6717678082312768\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 38 training complete\n",
      "Cost on training data: 0.5214156829799306\n",
      "Accuracy on training data: 88 / 95\n",
      "Cost on evaluation data: 0.5870871200447288\n",
      "Accuracy on evaluation data: 51 / 55\n",
      "\n",
      "Epoch 39 training complete\n",
      "Cost on training data: 0.5628809407657434\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6543668522458711\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 40 training complete\n",
      "Cost on training data: 0.6264428639527784\n",
      "Accuracy on training data: 62 / 95\n",
      "Cost on evaluation data: 0.7367629120899091\n",
      "Accuracy on evaluation data: 32 / 55\n",
      "\n",
      "Epoch 41 training complete\n",
      "Cost on training data: 0.5608648239933978\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6540465514286782\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 42 training complete\n",
      "Cost on training data: 0.5524150953309045\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6397740351076435\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 43 training complete\n",
      "Cost on training data: 0.6188312609778013\n",
      "Accuracy on training data: 63 / 95\n",
      "Cost on evaluation data: 0.7275142852591394\n",
      "Accuracy on evaluation data: 32 / 55\n",
      "\n",
      "Epoch 44 training complete\n",
      "Cost on training data: 0.5969494936814912\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.6999722820437206\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 45 training complete\n",
      "Cost on training data: 0.5283121573698737\n",
      "Accuracy on training data: 73 / 95\n",
      "Cost on evaluation data: 0.6163437804509214\n",
      "Accuracy on evaluation data: 44 / 55\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 training complete\n",
      "Cost on training data: 0.6722449425496445\n",
      "Accuracy on training data: 59 / 95\n",
      "Cost on evaluation data: 0.7861585630194309\n",
      "Accuracy on evaluation data: 31 / 55\n",
      "\n",
      "Epoch 47 training complete\n",
      "Cost on training data: 0.5617567094689573\n",
      "Accuracy on training data: 70 / 95\n",
      "Cost on evaluation data: 0.6509180632200113\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 48 training complete\n",
      "Cost on training data: 0.5306422428926405\n",
      "Accuracy on training data: 67 / 95\n",
      "Cost on evaluation data: 0.5756095340962823\n",
      "Accuracy on evaluation data: 44 / 55\n",
      "\n",
      "Epoch 49 training complete\n",
      "Cost on training data: 0.5761207943986053\n",
      "Accuracy on training data: 72 / 95\n",
      "Cost on evaluation data: 0.6771622979344211\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 50 training complete\n",
      "Cost on training data: 0.5347641544350911\n",
      "Accuracy on training data: 88 / 95\n",
      "Cost on evaluation data: 0.6041146947215472\n",
      "Accuracy on evaluation data: 51 / 55\n",
      "\n",
      "Epoch 51 training complete\n",
      "Cost on training data: 0.5566849161091533\n",
      "Accuracy on training data: 69 / 95\n",
      "Cost on evaluation data: 0.6511116008028893\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 52 training complete\n",
      "Cost on training data: 0.5997108621144853\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.7037820546468408\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 53 training complete\n",
      "Cost on training data: 0.6583961059841215\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.7695823787566343\n",
      "Accuracy on evaluation data: 34 / 55\n",
      "\n",
      "Epoch 54 training complete\n",
      "Cost on training data: 0.7873779527419873\n",
      "Accuracy on training data: 63 / 95\n",
      "Cost on evaluation data: 0.922400997208346\n",
      "Accuracy on evaluation data: 32 / 55\n",
      "\n",
      "Epoch 55 training complete\n",
      "Cost on training data: 0.502501626216347\n",
      "Accuracy on training data: 86 / 95\n",
      "Cost on evaluation data: 0.589535040635565\n",
      "Accuracy on evaluation data: 52 / 55\n",
      "\n",
      "Epoch 56 training complete\n",
      "Cost on training data: 0.714419646385718\n",
      "Accuracy on training data: 63 / 95\n",
      "Cost on evaluation data: 0.837613755831611\n",
      "Accuracy on evaluation data: 33 / 55\n",
      "\n",
      "Epoch 57 training complete\n",
      "Cost on training data: 0.6171258269614761\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.725820350469357\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 58 training complete\n",
      "Cost on training data: 0.5187731849634077\n",
      "Accuracy on training data: 72 / 95\n",
      "Cost on evaluation data: 0.6118996494286105\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 59 training complete\n",
      "Cost on training data: 0.5427856727505257\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6435280610266305\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 60 training complete\n",
      "Cost on training data: 0.46851792704915096\n",
      "Accuracy on training data: 84 / 95\n",
      "Cost on evaluation data: 0.5419100001570621\n",
      "Accuracy on evaluation data: 50 / 55\n",
      "\n",
      "Epoch 61 training complete\n",
      "Cost on training data: 0.5994944873927814\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.7045557509051095\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 62 training complete\n",
      "Cost on training data: 0.6075700510948343\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.7115733872317687\n",
      "Accuracy on evaluation data: 34 / 55\n",
      "\n",
      "Epoch 63 training complete\n",
      "Cost on training data: 0.49711842227752195\n",
      "Accuracy on training data: 83 / 95\n",
      "Cost on evaluation data: 0.5791348337706791\n",
      "Accuracy on evaluation data: 50 / 55\n",
      "\n",
      "Epoch 64 training complete\n",
      "Cost on training data: 0.543687282807135\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6456862809847462\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 65 training complete\n",
      "Cost on training data: 0.5793312709726803\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.6863095131292719\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 66 training complete\n",
      "Cost on training data: 0.5435730652808035\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6365388161516303\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 67 training complete\n",
      "Cost on training data: 0.483049765194835\n",
      "Accuracy on training data: 85 / 95\n",
      "Cost on evaluation data: 0.5540958783483996\n",
      "Accuracy on evaluation data: 50 / 55\n",
      "\n",
      "Epoch 68 training complete\n",
      "Cost on training data: 0.5510247993462433\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.6536363989110874\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 69 training complete\n",
      "Cost on training data: 0.5768243988073618\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.6797952698994588\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 70 training complete\n",
      "Cost on training data: 0.6156128805939229\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.7233730350508638\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 71 training complete\n",
      "Cost on training data: 0.6110981467022679\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.7212350013697348\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 72 training complete\n",
      "Cost on training data: 0.6891967147297247\n",
      "Accuracy on training data: 63 / 95\n",
      "Cost on evaluation data: 0.8073474819408359\n",
      "Accuracy on evaluation data: 32 / 55\n",
      "\n",
      "Epoch 73 training complete\n",
      "Cost on training data: 0.6111134192786142\n",
      "Accuracy on training data: 63 / 95\n",
      "Cost on evaluation data: 0.7185529536044561\n",
      "Accuracy on evaluation data: 32 / 55\n",
      "\n",
      "Epoch 74 training complete\n",
      "Cost on training data: 0.5265286881486643\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6211472047668714\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 75 training complete\n",
      "Cost on training data: 0.5397517702176394\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6341667412351532\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 76 training complete\n",
      "Cost on training data: 0.6128750415596544\n",
      "Accuracy on training data: 63 / 95\n",
      "Cost on evaluation data: 0.7241546496427937\n",
      "Accuracy on evaluation data: 33 / 55\n",
      "\n",
      "Epoch 77 training complete\n",
      "Cost on training data: 0.5970952192244172\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7024067688867373\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 78 training complete\n",
      "Cost on training data: 0.6195435507772117\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.7314702180467998\n",
      "Accuracy on evaluation data: 34 / 55\n",
      "\n",
      "Epoch 79 training complete\n",
      "Cost on training data: 0.4948434821885964\n",
      "Accuracy on training data: 76 / 95\n",
      "Cost on evaluation data: 0.5799519045570897\n",
      "Accuracy on evaluation data: 46 / 55\n",
      "\n",
      "Epoch 80 training complete\n",
      "Cost on training data: 0.5158064459333255\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6095987935364537\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 81 training complete\n",
      "Cost on training data: 0.6798512922059736\n",
      "Accuracy on training data: 62 / 95\n",
      "Cost on evaluation data: 0.8040146683292941\n",
      "Accuracy on evaluation data: 33 / 55\n",
      "\n",
      "Epoch 82 training complete\n",
      "Cost on training data: 0.5419572620297598\n",
      "Accuracy on training data: 66 / 95\n",
      "Cost on evaluation data: 0.6309883784247676\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 83 training complete\n",
      "Cost on training data: 0.7079646762801699\n",
      "Accuracy on training data: 62 / 95\n",
      "Cost on evaluation data: 0.8374248386383444\n",
      "Accuracy on evaluation data: 32 / 55\n",
      "\n",
      "Epoch 84 training complete\n",
      "Cost on training data: 0.49477473479229817\n",
      "Accuracy on training data: 75 / 95\n",
      "Cost on evaluation data: 0.579443448738454\n",
      "Accuracy on evaluation data: 44 / 55\n",
      "\n",
      "Epoch 85 training complete\n",
      "Cost on training data: 0.7061870622357116\n",
      "Accuracy on training data: 61 / 95\n",
      "Cost on evaluation data: 0.824961527436214\n",
      "Accuracy on evaluation data: 33 / 55\n",
      "\n",
      "Epoch 86 training complete\n",
      "Cost on training data: 0.5394990558164332\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6380032899893365\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 87 training complete\n",
      "Cost on training data: 0.6111811393446774\n",
      "Accuracy on training data: 63 / 95\n",
      "Cost on evaluation data: 0.7212369706004745\n",
      "Accuracy on evaluation data: 33 / 55\n",
      "\n",
      "Epoch 88 training complete\n",
      "Cost on training data: 0.6017596020321093\n",
      "Accuracy on training data: 63 / 95\n",
      "Cost on evaluation data: 0.709565294153238\n",
      "Accuracy on evaluation data: 32 / 55\n",
      "\n",
      "Epoch 89 training complete\n",
      "Cost on training data: 0.5554407874620552\n",
      "Accuracy on training data: 66 / 95\n",
      "Cost on evaluation data: 0.6518099446949384\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 90 training complete\n",
      "Cost on training data: 0.5435202972114537\n",
      "Accuracy on training data: 72 / 95\n",
      "Cost on evaluation data: 0.6415727745924438\n",
      "Accuracy on evaluation data: 41 / 55\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91 training complete\n",
      "Cost on training data: 0.6236109372990832\n",
      "Accuracy on training data: 63 / 95\n",
      "Cost on evaluation data: 0.7384479910678008\n",
      "Accuracy on evaluation data: 33 / 55\n",
      "\n",
      "Epoch 92 training complete\n",
      "Cost on training data: 0.5346724537812919\n",
      "Accuracy on training data: 72 / 95\n",
      "Cost on evaluation data: 0.6248307833317261\n",
      "Accuracy on evaluation data: 39 / 55\n",
      "\n",
      "Epoch 93 training complete\n",
      "Cost on training data: 0.4907662480130258\n",
      "Accuracy on training data: 89 / 95\n",
      "Cost on evaluation data: 0.5632978355924048\n",
      "Accuracy on evaluation data: 52 / 55\n",
      "\n",
      "Epoch 94 training complete\n",
      "Cost on training data: 0.6750371221591196\n",
      "Accuracy on training data: 62 / 95\n",
      "Cost on evaluation data: 0.7966537742801453\n",
      "Accuracy on evaluation data: 33 / 55\n",
      "\n",
      "Epoch 95 training complete\n",
      "Cost on training data: 0.6108940638929945\n",
      "Accuracy on training data: 63 / 95\n",
      "Cost on evaluation data: 0.7208310435642505\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 96 training complete\n",
      "Cost on training data: 0.6864884112251881\n",
      "Accuracy on training data: 60 / 95\n",
      "Cost on evaluation data: 0.8108001177800589\n",
      "Accuracy on evaluation data: 30 / 55\n",
      "\n",
      "Epoch 97 training complete\n",
      "Cost on training data: 0.52680967373011\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6202507818253998\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 98 training complete\n",
      "Cost on training data: 0.7090651449594864\n",
      "Accuracy on training data: 59 / 95\n",
      "Cost on evaluation data: 0.8340536910864886\n",
      "Accuracy on evaluation data: 31 / 55\n",
      "\n",
      "Epoch 99 training complete\n",
      "Cost on training data: 0.5190671213638802\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.615313640513935\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 100 training complete\n",
      "Cost on training data: 0.47106418260278193\n",
      "Accuracy on training data: 89 / 95\n",
      "Cost on evaluation data: 0.5431984850985346\n",
      "Accuracy on evaluation data: 55 / 55\n",
      "\n",
      "Epoch 101 training complete\n",
      "Cost on training data: 0.6704578625431471\n",
      "Accuracy on training data: 63 / 95\n",
      "Cost on evaluation data: 0.7856666539147203\n",
      "Accuracy on evaluation data: 33 / 55\n",
      "\n",
      "Epoch 102 training complete\n",
      "Cost on training data: 0.6560398317592099\n",
      "Accuracy on training data: 63 / 95\n",
      "Cost on evaluation data: 0.7693752931711989\n",
      "Accuracy on evaluation data: 33 / 55\n",
      "\n",
      "Epoch 103 training complete\n",
      "Cost on training data: 0.5567752158083785\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6549759941323594\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 104 training complete\n",
      "Cost on training data: 0.5490094340970957\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.645808447601566\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 105 training complete\n",
      "Cost on training data: 0.5338182171824495\n",
      "Accuracy on training data: 66 / 95\n",
      "Cost on evaluation data: 0.625920174708856\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 106 training complete\n",
      "Cost on training data: 0.48598274051046036\n",
      "Accuracy on training data: 92 / 95\n",
      "Cost on evaluation data: 0.5506117434843125\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 107 training complete\n",
      "Cost on training data: 0.5154723444066337\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6066404704944861\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 108 training complete\n",
      "Cost on training data: 0.5166099328256045\n",
      "Accuracy on training data: 73 / 95\n",
      "Cost on evaluation data: 0.605420298658753\n",
      "Accuracy on evaluation data: 40 / 55\n",
      "\n",
      "Epoch 109 training complete\n",
      "Cost on training data: 0.557787674916951\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6535130949040775\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 110 training complete\n",
      "Cost on training data: 0.5501184147612832\n",
      "Accuracy on training data: 74 / 95\n",
      "Cost on evaluation data: 0.6148457885109945\n",
      "Accuracy on evaluation data: 48 / 55\n",
      "\n",
      "Epoch 111 training complete\n",
      "Cost on training data: 0.6650147071282768\n",
      "Accuracy on training data: 62 / 95\n",
      "Cost on evaluation data: 0.786073038554322\n",
      "Accuracy on evaluation data: 33 / 55\n",
      "\n",
      "Epoch 112 training complete\n",
      "Cost on training data: 0.5506883008590376\n",
      "Accuracy on training data: 67 / 95\n",
      "Cost on evaluation data: 0.6421744162223412\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 113 training complete\n",
      "Cost on training data: 1.1065975807056585\n",
      "Accuracy on training data: 51 / 95\n",
      "Cost on evaluation data: 1.2733141298816155\n",
      "Accuracy on evaluation data: 25 / 55\n",
      "\n",
      "Epoch 114 training complete\n",
      "Cost on training data: 0.5131294781948684\n",
      "Accuracy on training data: 91 / 95\n",
      "Cost on evaluation data: 0.5859451048651305\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 115 training complete\n",
      "Cost on training data: 0.6146009622039519\n",
      "Accuracy on training data: 63 / 95\n",
      "Cost on evaluation data: 0.7260597935224901\n",
      "Accuracy on evaluation data: 32 / 55\n",
      "\n",
      "Epoch 116 training complete\n",
      "Cost on training data: 0.6663177547657299\n",
      "Accuracy on training data: 62 / 95\n",
      "Cost on evaluation data: 0.7825443540933082\n",
      "Accuracy on evaluation data: 33 / 55\n",
      "\n",
      "Epoch 117 training complete\n",
      "Cost on training data: 0.5048789496125287\n",
      "Accuracy on training data: 83 / 95\n",
      "Cost on evaluation data: 0.5808923802981506\n",
      "Accuracy on evaluation data: 50 / 55\n",
      "\n",
      "Epoch 118 training complete\n",
      "Cost on training data: 0.4888546431135081\n",
      "Accuracy on training data: 76 / 95\n",
      "Cost on evaluation data: 0.5397415423205425\n",
      "Accuracy on evaluation data: 51 / 55\n",
      "\n",
      "Epoch 119 training complete\n",
      "Cost on training data: 0.5113755146017355\n",
      "Accuracy on training data: 67 / 95\n",
      "Cost on evaluation data: 0.5974201794540708\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 120 training complete\n",
      "Cost on training data: 0.5545091324338534\n",
      "Accuracy on training data: 67 / 95\n",
      "Cost on evaluation data: 0.6056839580087516\n",
      "Accuracy on evaluation data: 44 / 55\n",
      "\n",
      "Epoch 121 training complete\n",
      "Cost on training data: 0.5040003722553176\n",
      "Accuracy on training data: 91 / 95\n",
      "Cost on evaluation data: 0.573675122322127\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 122 training complete\n",
      "Cost on training data: 0.7751564222643819\n",
      "Accuracy on training data: 52 / 95\n",
      "Cost on evaluation data: 0.9055018711781023\n",
      "Accuracy on evaluation data: 26 / 55\n",
      "\n",
      "Epoch 123 training complete\n",
      "Cost on training data: 0.5266558499923311\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.6232856854843393\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 124 training complete\n",
      "Cost on training data: 0.782665792967037\n",
      "Accuracy on training data: 56 / 95\n",
      "Cost on evaluation data: 0.9195473570320623\n",
      "Accuracy on evaluation data: 29 / 55\n",
      "\n",
      "Epoch 125 training complete\n",
      "Cost on training data: 0.5498569750603521\n",
      "Accuracy on training data: 66 / 95\n",
      "Cost on evaluation data: 0.6506468846215809\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 126 training complete\n",
      "Cost on training data: 0.5216812883511727\n",
      "Accuracy on training data: 66 / 95\n",
      "Cost on evaluation data: 0.6104011918676535\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 127 training complete\n",
      "Cost on training data: 0.6796432004579263\n",
      "Accuracy on training data: 60 / 95\n",
      "Cost on evaluation data: 0.8018055264918138\n",
      "Accuracy on evaluation data: 31 / 55\n",
      "\n",
      "Epoch 128 training complete\n",
      "Cost on training data: 0.5457550420423571\n",
      "Accuracy on training data: 66 / 95\n",
      "Cost on evaluation data: 0.6371762546342155\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 129 training complete\n",
      "Cost on training data: 0.5534469168702427\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.64173790120907\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 130 training complete\n",
      "Cost on training data: 0.5372157056129503\n",
      "Accuracy on training data: 66 / 95\n",
      "Cost on evaluation data: 0.6333821603897013\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 131 training complete\n",
      "Cost on training data: 0.5979007627893514\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.701768027072875\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 132 training complete\n",
      "Cost on training data: 0.4756411762504866\n",
      "Accuracy on training data: 81 / 95\n",
      "Cost on evaluation data: 0.5551901566793935\n",
      "Accuracy on evaluation data: 50 / 55\n",
      "\n",
      "Epoch 133 training complete\n",
      "Cost on training data: 0.42239218596134964\n",
      "Accuracy on training data: 90 / 95\n",
      "Cost on evaluation data: 0.4866421824086078\n",
      "Accuracy on evaluation data: 55 / 55\n",
      "\n",
      "Epoch 134 training complete\n",
      "Cost on training data: 0.6044937192073457\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.716912486459629\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 135 training complete\n",
      "Cost on training data: 0.7430136109817612\n",
      "Accuracy on training data: 58 / 95\n",
      "Cost on evaluation data: 0.8830810294153331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on evaluation data: 29 / 55\n",
      "\n",
      "Epoch 136 training complete\n",
      "Cost on training data: 0.5359444216731211\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6410876363607632\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 137 training complete\n",
      "Cost on training data: 0.5099883048144106\n",
      "Accuracy on training data: 66 / 95\n",
      "Cost on evaluation data: 0.6024236317992688\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 138 training complete\n",
      "Cost on training data: 0.5407888778018002\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6509094958945303\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 139 training complete\n",
      "Cost on training data: 0.5568351534419581\n",
      "Accuracy on training data: 66 / 95\n",
      "Cost on evaluation data: 0.6712215918466099\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 140 training complete\n",
      "Cost on training data: 0.4899505438276742\n",
      "Accuracy on training data: 84 / 95\n",
      "Cost on evaluation data: 0.5782628704344633\n",
      "Accuracy on evaluation data: 52 / 55\n",
      "\n",
      "Epoch 141 training complete\n",
      "Cost on training data: 0.5463758176016026\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6488056985500503\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 142 training complete\n",
      "Cost on training data: 0.4677644764805543\n",
      "Accuracy on training data: 90 / 95\n",
      "Cost on evaluation data: 0.5374388679482633\n",
      "Accuracy on evaluation data: 55 / 55\n",
      "\n",
      "Epoch 143 training complete\n",
      "Cost on training data: 0.4952349582840796\n",
      "Accuracy on training data: 75 / 95\n",
      "Cost on evaluation data: 0.5799911748904604\n",
      "Accuracy on evaluation data: 47 / 55\n",
      "\n",
      "Epoch 144 training complete\n",
      "Cost on training data: 0.7404839213121742\n",
      "Accuracy on training data: 57 / 95\n",
      "Cost on evaluation data: 0.8612024261815191\n",
      "Accuracy on evaluation data: 29 / 55\n",
      "\n",
      "Epoch 145 training complete\n",
      "Cost on training data: 0.6308641092128826\n",
      "Accuracy on training data: 63 / 95\n",
      "Cost on evaluation data: 0.7512119885603864\n",
      "Accuracy on evaluation data: 32 / 55\n",
      "\n",
      "Epoch 146 training complete\n",
      "Cost on training data: 0.5547440734564113\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6526021004168121\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 147 training complete\n",
      "Cost on training data: 0.5693026771298922\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6778597999875189\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 148 training complete\n",
      "Cost on training data: 0.47006514454747\n",
      "Accuracy on training data: 90 / 95\n",
      "Cost on evaluation data: 0.5369018776045773\n",
      "Accuracy on evaluation data: 55 / 55\n",
      "\n",
      "Epoch 149 training complete\n",
      "Cost on training data: 0.5870129387259957\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.6987112241271402\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 150 training complete\n",
      "Cost on training data: 0.5735409466355882\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6799717458274396\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 151 training complete\n",
      "Cost on training data: 0.5507334580240324\n",
      "Accuracy on training data: 72 / 95\n",
      "Cost on evaluation data: 0.6540366649703431\n",
      "Accuracy on evaluation data: 39 / 55\n",
      "\n",
      "Epoch 152 training complete\n",
      "Cost on training data: 0.5759600137600568\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6874809432622373\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 153 training complete\n",
      "Cost on training data: 0.496569250199449\n",
      "Accuracy on training data: 83 / 95\n",
      "Cost on evaluation data: 0.5849806034627689\n",
      "Accuracy on evaluation data: 50 / 55\n",
      "\n",
      "Epoch 154 training complete\n",
      "Cost on training data: 0.49138311712971083\n",
      "Accuracy on training data: 80 / 95\n",
      "Cost on evaluation data: 0.5780858109602081\n",
      "Accuracy on evaluation data: 49 / 55\n",
      "\n",
      "Epoch 155 training complete\n",
      "Cost on training data: 0.5759375930162373\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.687679772982816\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 156 training complete\n",
      "Cost on training data: 0.4652080246629663\n",
      "Accuracy on training data: 89 / 95\n",
      "Cost on evaluation data: 0.5416597895221242\n",
      "Accuracy on evaluation data: 53 / 55\n",
      "\n",
      "Epoch 157 training complete\n",
      "Cost on training data: 0.566663118437795\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6744827826194147\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 158 training complete\n",
      "Cost on training data: 0.5703911072662614\n",
      "Accuracy on training data: 66 / 95\n",
      "Cost on evaluation data: 0.6718362814205394\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 159 training complete\n",
      "Cost on training data: 0.43178924214989145\n",
      "Accuracy on training data: 89 / 95\n",
      "Cost on evaluation data: 0.49976285075015275\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 160 training complete\n",
      "Cost on training data: 0.604361324550265\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7080210440691044\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 161 training complete\n",
      "Cost on training data: 0.6033523890054628\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.7233939937365103\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 162 training complete\n",
      "Cost on training data: 0.5025798716380067\n",
      "Accuracy on training data: 82 / 95\n",
      "Cost on evaluation data: 0.5930670967896715\n",
      "Accuracy on evaluation data: 50 / 55\n",
      "\n",
      "Epoch 163 training complete\n",
      "Cost on training data: 0.46566708407205193\n",
      "Accuracy on training data: 90 / 95\n",
      "Cost on evaluation data: 0.5430830233677638\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 164 training complete\n",
      "Cost on training data: 0.6267877935450227\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.7490150435424611\n",
      "Accuracy on evaluation data: 33 / 55\n",
      "\n",
      "Epoch 165 training complete\n",
      "Cost on training data: 0.5838452348257257\n",
      "Accuracy on training data: 66 / 95\n",
      "Cost on evaluation data: 0.6960960006245168\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 166 training complete\n",
      "Cost on training data: 0.4881241578132155\n",
      "Accuracy on training data: 84 / 95\n",
      "Cost on evaluation data: 0.5782944599451604\n",
      "Accuracy on evaluation data: 52 / 55\n",
      "\n",
      "Epoch 167 training complete\n",
      "Cost on training data: 0.6182512585076224\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.7333543256365107\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 168 training complete\n",
      "Cost on training data: 0.571799755426043\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6826420595404673\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 169 training complete\n",
      "Cost on training data: 0.5468259753249688\n",
      "Accuracy on training data: 74 / 95\n",
      "Cost on evaluation data: 0.6447846271406787\n",
      "Accuracy on evaluation data: 41 / 55\n",
      "\n",
      "Epoch 170 training complete\n",
      "Cost on training data: 0.5934657925175139\n",
      "Accuracy on training data: 66 / 95\n",
      "Cost on evaluation data: 0.7035765716245441\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 171 training complete\n",
      "Cost on training data: 0.5595814475361292\n",
      "Accuracy on training data: 71 / 95\n",
      "Cost on evaluation data: 0.6551354058856929\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 172 training complete\n",
      "Cost on training data: 0.5359502359345372\n",
      "Accuracy on training data: 71 / 95\n",
      "Cost on evaluation data: 0.633956174119152\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 173 training complete\n",
      "Cost on training data: 0.5769694393212474\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6844228385754627\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 174 training complete\n",
      "Cost on training data: 0.653267386472472\n",
      "Accuracy on training data: 63 / 95\n",
      "Cost on evaluation data: 0.7766240279126341\n",
      "Accuracy on evaluation data: 34 / 55\n",
      "\n",
      "Epoch 175 training complete\n",
      "Cost on training data: 0.578018823745502\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6921230387115044\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 176 training complete\n",
      "Cost on training data: 0.5003418925402341\n",
      "Accuracy on training data: 86 / 95\n",
      "Cost on evaluation data: 0.5881135076886775\n",
      "Accuracy on evaluation data: 52 / 55\n",
      "\n",
      "Epoch 177 training complete\n",
      "Cost on training data: 0.6056255838826318\n",
      "Accuracy on training data: 66 / 95\n",
      "Cost on evaluation data: 0.7151360173909223\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 178 training complete\n",
      "Cost on training data: 0.42869943015162526\n",
      "Accuracy on training data: 90 / 95\n",
      "Cost on evaluation data: 0.501135626889301\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 179 training complete\n",
      "Cost on training data: 0.559391418902847\n",
      "Accuracy on training data: 74 / 95\n",
      "Cost on evaluation data: 0.6639185727062835\n",
      "Accuracy on evaluation data: 42 / 55\n",
      "\n",
      "Epoch 180 training complete\n",
      "Cost on training data: 0.6082678844213967\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.720625204645838\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 181 training complete\n",
      "Cost on training data: 0.5744762131673649\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6846596222786648\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182 training complete\n",
      "Cost on training data: 0.5701600219823789\n",
      "Accuracy on training data: 67 / 95\n",
      "Cost on evaluation data: 0.68084812043283\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 183 training complete\n",
      "Cost on training data: 0.43348122776356585\n",
      "Accuracy on training data: 90 / 95\n",
      "Cost on evaluation data: 0.503588964804969\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 184 training complete\n",
      "Cost on training data: 0.6654148639099703\n",
      "Accuracy on training data: 62 / 95\n",
      "Cost on evaluation data: 0.8106075010591239\n",
      "Accuracy on evaluation data: 32 / 55\n",
      "\n",
      "Epoch 185 training complete\n",
      "Cost on training data: 0.4468545692163674\n",
      "Accuracy on training data: 92 / 95\n",
      "Cost on evaluation data: 0.5143325403068921\n",
      "Accuracy on evaluation data: 55 / 55\n",
      "\n",
      "Epoch 186 training complete\n",
      "Cost on training data: 0.4532938821404492\n",
      "Accuracy on training data: 88 / 95\n",
      "Cost on evaluation data: 0.5212836428529071\n",
      "Accuracy on evaluation data: 54 / 55\n",
      "\n",
      "Epoch 187 training complete\n",
      "Cost on training data: 0.7330793323302076\n",
      "Accuracy on training data: 57 / 95\n",
      "Cost on evaluation data: 0.8884585796065771\n",
      "Accuracy on evaluation data: 29 / 55\n",
      "\n",
      "Epoch 188 training complete\n",
      "Cost on training data: 0.6370234912046528\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.77192542365276\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 189 training complete\n",
      "Cost on training data: 0.5192392104346817\n",
      "Accuracy on training data: 76 / 95\n",
      "Cost on evaluation data: 0.6137299752734745\n",
      "Accuracy on evaluation data: 47 / 55\n",
      "\n",
      "Epoch 190 training complete\n",
      "Cost on training data: 0.6542651517322279\n",
      "Accuracy on training data: 63 / 95\n",
      "Cost on evaluation data: 0.7955198778036137\n",
      "Accuracy on evaluation data: 33 / 55\n",
      "\n",
      "Epoch 191 training complete\n",
      "Cost on training data: 0.569267344119249\n",
      "Accuracy on training data: 69 / 95\n",
      "Cost on evaluation data: 0.6790154059533386\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 192 training complete\n",
      "Cost on training data: 0.5213647959985594\n",
      "Accuracy on training data: 83 / 95\n",
      "Cost on evaluation data: 0.6190224254811102\n",
      "Accuracy on evaluation data: 50 / 55\n",
      "\n",
      "Epoch 193 training complete\n",
      "Cost on training data: 0.664562569727177\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7985624510780163\n",
      "Accuracy on evaluation data: 33 / 55\n",
      "\n",
      "Epoch 194 training complete\n",
      "Cost on training data: 0.563987191488066\n",
      "Accuracy on training data: 68 / 95\n",
      "Cost on evaluation data: 0.6655529770380575\n",
      "Accuracy on evaluation data: 37 / 55\n",
      "\n",
      "Epoch 195 training complete\n",
      "Cost on training data: 0.5878738626008428\n",
      "Accuracy on training data: 66 / 95\n",
      "Cost on evaluation data: 0.7029762800917057\n",
      "Accuracy on evaluation data: 36 / 55\n",
      "\n",
      "Epoch 196 training complete\n",
      "Cost on training data: 0.6242226608194973\n",
      "Accuracy on training data: 67 / 95\n",
      "Cost on evaluation data: 0.7433191572415815\n",
      "Accuracy on evaluation data: 34 / 55\n",
      "\n",
      "Epoch 197 training complete\n",
      "Cost on training data: 0.5549931513758162\n",
      "Accuracy on training data: 73 / 95\n",
      "Cost on evaluation data: 0.66315154183289\n",
      "Accuracy on evaluation data: 41 / 55\n",
      "\n",
      "Epoch 198 training complete\n",
      "Cost on training data: 0.7329364611827192\n",
      "Accuracy on training data: 58 / 95\n",
      "Cost on evaluation data: 0.8797732921391916\n",
      "Accuracy on evaluation data: 29 / 55\n",
      "\n",
      "Epoch 199 training complete\n",
      "Cost on training data: 0.5032579716886514\n",
      "Accuracy on training data: 75 / 95\n",
      "Cost on evaluation data: 0.5996453276149785\n",
      "Accuracy on evaluation data: 45 / 55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net2 = network2.load_network(\"./data/iris-4-20-7-3.dat\")\n",
    "\n",
    "# Set hyper-parameter values individually after the network\n",
    "net2.set_parameters(cost=network2.LogLikelihood,\n",
    "                    act_hidden = network2.ReLU,\n",
    "                    act_output = network2.Softmax,\n",
    "                    regularization=\"L2\", \n",
    "                    dropoutpercent=0.1)\n",
    "\n",
    "evaluation_cost11b, evaluation_accuracy11b, training_cost11b, training_accuracy11b = net2.SGD(iris_train, 200, 1, 0.05,\n",
    "         lmbda=3.0,\n",
    "         evaluation_data=iris_test, \n",
    "         monitor_evaluation_cost=True, \n",
    "         monitor_evaluation_accuracy=True,\n",
    "         monitor_training_cost=True,\n",
    "         monitor_training_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2-12) ReLU hidden + Softmax output + LogLikelihood + L2+ lmbda=3.0 +dropout=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.sizes [4, 20, 7, 3]\n",
      "Epoch 0 training complete\n",
      "Cost on training data: 1.5405845877050528\n",
      "Accuracy on training data: 32 / 95\n",
      "Cost on evaluation data: 1.8478584684554318\n",
      "Accuracy on evaluation data: 18 / 55\n",
      "\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 1.387194054010197\n",
      "Accuracy on training data: 32 / 95\n",
      "Cost on evaluation data: 1.614331190895369\n",
      "Accuracy on evaluation data: 18 / 55\n",
      "\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 1.323340292901742\n",
      "Accuracy on training data: 32 / 95\n",
      "Cost on evaluation data: 1.490910124504148\n",
      "Accuracy on evaluation data: 18 / 55\n",
      "\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 1.2311248762537241\n",
      "Accuracy on training data: 55 / 95\n",
      "Cost on evaluation data: 1.3572407370493673\n",
      "Accuracy on evaluation data: 25 / 55\n",
      "\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 1.205818531779822\n",
      "Accuracy on training data: 63 / 95\n",
      "Cost on evaluation data: 1.3014753299779387\n",
      "Accuracy on evaluation data: 33 / 55\n",
      "\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 1.1731832101048956\n",
      "Accuracy on training data: 3 / 95\n",
      "Cost on evaluation data: 1.2446707319943526\n",
      "Accuracy on evaluation data: 5 / 55\n",
      "\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 1.0606462688950524\n",
      "Accuracy on training data: 35 / 95\n",
      "Cost on evaluation data: 1.1215416810656702\n",
      "Accuracy on evaluation data: 20 / 55\n",
      "\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 1.051570003242763\n",
      "Accuracy on training data: 63 / 95\n",
      "Cost on evaluation data: 1.1009503739296362\n",
      "Accuracy on evaluation data: 34 / 55\n",
      "\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 1.008862110757464\n",
      "Accuracy on training data: 60 / 95\n",
      "Cost on evaluation data: 1.0530347433274105\n",
      "Accuracy on evaluation data: 28 / 55\n",
      "\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 0.9160028710076816\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.9570398701233654\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 10 training complete\n",
      "Cost on training data: 0.9006050951465523\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.9438463724741366\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 11 training complete\n",
      "Cost on training data: 0.7685479371826365\n",
      "Accuracy on training data: 63 / 95\n",
      "Cost on evaluation data: 0.8291525861783866\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 12 training complete\n",
      "Cost on training data: 0.7512198303145976\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.8070963180892888\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 13 training complete\n",
      "Cost on training data: 0.7305642888536376\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7833843631797233\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 14 training complete\n",
      "Cost on training data: 0.7240028513993721\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7846482503613234\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 15 training complete\n",
      "Cost on training data: 0.7849598147525217\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.8350905431862337\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 16 training complete\n",
      "Cost on training data: 0.7416759591128589\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7963260198250739\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 17 training complete\n",
      "Cost on training data: 0.6893626634709996\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7503465073424579\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 18 training complete\n",
      "Cost on training data: 0.70780542414584\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7697609649830266\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 19 training complete\n",
      "Cost on training data: 0.6650484919561819\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7330276099087852\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 20 training complete\n",
      "Cost on training data: 0.65030972107357\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7167272767461659\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 21 training complete\n",
      "Cost on training data: 0.6646947757206649\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7292845782759958\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 22 training complete\n",
      "Cost on training data: 0.7004528797046334\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7530669680514335\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 23 training complete\n",
      "Cost on training data: 0.6435282040950843\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7032147436780267\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 24 training complete\n",
      "Cost on training data: 0.6580383275128397\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7196587942463011\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 25 training complete\n",
      "Cost on training data: 0.6170513420934524\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.6859113644761293\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 26 training complete\n",
      "Cost on training data: 0.730225575328911\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7847472137727685\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 27 training complete\n",
      "Cost on training data: 0.621757711283268\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6842944314925334\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 28 training complete\n",
      "Cost on training data: 0.61402352849455\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6782846525142979\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 29 training complete\n",
      "Cost on training data: 0.6313573359492465\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7048632039132187\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 30 training complete\n",
      "Cost on training data: 0.6423428729371126\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7136930765643267\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 31 training complete\n",
      "Cost on training data: 0.6655345375155576\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7250307779553864\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 32 training complete\n",
      "Cost on training data: 0.6376791713792969\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7056613921267538\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 33 training complete\n",
      "Cost on training data: 0.6270721470337844\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6980986505018213\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 34 training complete\n",
      "Cost on training data: 0.6877010901786285\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7622002071211208\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 35 training complete\n",
      "Cost on training data: 0.6523040245856222\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7263241948034269\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 36 training complete\n",
      "Cost on training data: 0.6435086134122379\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7128954620686254\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 37 training complete\n",
      "Cost on training data: 0.6712251207091811\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7331347692789179\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 38 training complete\n",
      "Cost on training data: 0.6270365641482922\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6924919145331785\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 39 training complete\n",
      "Cost on training data: 0.7109590894587206\n",
      "Accuracy on training data: 79 / 95\n",
      "Cost on evaluation data: 0.7646247674462168\n",
      "Accuracy on evaluation data: 51 / 55\n",
      "\n",
      "Epoch 40 training complete\n",
      "Cost on training data: 0.6642243597846627\n",
      "Accuracy on training data: 69 / 95\n",
      "Cost on evaluation data: 0.7201919291905499\n",
      "Accuracy on evaluation data: 45 / 55\n",
      "\n",
      "Epoch 41 training complete\n",
      "Cost on training data: 0.6768157497147129\n",
      "Accuracy on training data: 66 / 95\n",
      "Cost on evaluation data: 0.7307147924208641\n",
      "Accuracy on evaluation data: 43 / 55\n",
      "\n",
      "Epoch 42 training complete\n",
      "Cost on training data: 0.646178738709692\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7099296373169993\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 43 training complete\n",
      "Cost on training data: 0.6734518557926036\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.727486795285857\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 44 training complete\n",
      "Cost on training data: 0.632454532038847\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.6981384027936575\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 training complete\n",
      "Cost on training data: 0.7740813856855139\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.8224182140109603\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 46 training complete\n",
      "Cost on training data: 0.672665972671417\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7331984461108642\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 47 training complete\n",
      "Cost on training data: 0.6641406033059176\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.7263623772306359\n",
      "Accuracy on evaluation data: 41 / 55\n",
      "\n",
      "Epoch 48 training complete\n",
      "Cost on training data: 0.624591439715721\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.6892540419923008\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 49 training complete\n",
      "Cost on training data: 0.732075167943322\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7895505850154168\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 50 training complete\n",
      "Cost on training data: 0.6822298530553929\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7439072362356259\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 51 training complete\n",
      "Cost on training data: 0.6968889577692647\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7637641574262104\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 52 training complete\n",
      "Cost on training data: 0.7744053507501513\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.8401100199782233\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 53 training complete\n",
      "Cost on training data: 0.6545546982924351\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7179823920777397\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 54 training complete\n",
      "Cost on training data: 0.6565597505963319\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7187062801205948\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 55 training complete\n",
      "Cost on training data: 0.6363664769561815\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7005158464551824\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 56 training complete\n",
      "Cost on training data: 0.6332483344463856\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7022802218027027\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 57 training complete\n",
      "Cost on training data: 0.6264068993102443\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6846348266971124\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 58 training complete\n",
      "Cost on training data: 0.6644126100508222\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7273767436295682\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 59 training complete\n",
      "Cost on training data: 0.6617683345021023\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7174455125445985\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 60 training complete\n",
      "Cost on training data: 0.7891918392836264\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.8510587414095577\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 61 training complete\n",
      "Cost on training data: 0.6898415799677412\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7557359015152008\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 62 training complete\n",
      "Cost on training data: 0.7132856362004706\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7794876073473616\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 63 training complete\n",
      "Cost on training data: 0.6608732065781896\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7338912636604874\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 64 training complete\n",
      "Cost on training data: 0.6422257858900806\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7058679812328341\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 65 training complete\n",
      "Cost on training data: 0.6562689420619221\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7121632056377013\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 66 training complete\n",
      "Cost on training data: 0.6402949795581163\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7043891637100337\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 67 training complete\n",
      "Cost on training data: 0.6813568440048958\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7423972768646487\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 68 training complete\n",
      "Cost on training data: 0.6809151413920741\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7422750155093184\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 69 training complete\n",
      "Cost on training data: 0.6582941195163543\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7162177654059654\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 70 training complete\n",
      "Cost on training data: 0.6201293508989545\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6919177117987887\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 71 training complete\n",
      "Cost on training data: 0.725571729702972\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7798925465192513\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 72 training complete\n",
      "Cost on training data: 0.6483207437027514\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7090434912726398\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 73 training complete\n",
      "Cost on training data: 0.6244908974075357\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.687856774859444\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 74 training complete\n",
      "Cost on training data: 0.6268547219371476\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6949291681754457\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 75 training complete\n",
      "Cost on training data: 0.6321349247250273\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6932456367708315\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 76 training complete\n",
      "Cost on training data: 0.6230223008592845\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6937140066939137\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 77 training complete\n",
      "Cost on training data: 0.6186626477966848\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6815530494945099\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 78 training complete\n",
      "Cost on training data: 0.6189789412746977\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.693169604336367\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 79 training complete\n",
      "Cost on training data: 0.6097160219209902\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.680495474807566\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 80 training complete\n",
      "Cost on training data: 0.6389124445900259\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7133871360219847\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 81 training complete\n",
      "Cost on training data: 0.6194494486271207\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6963137019254793\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 82 training complete\n",
      "Cost on training data: 0.6296443684885928\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7011996069274685\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 83 training complete\n",
      "Cost on training data: 0.6467481814245709\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.7178261726307896\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 84 training complete\n",
      "Cost on training data: 0.6965856405438512\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.760096005025753\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 85 training complete\n",
      "Cost on training data: 0.6640332362939279\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.733010406274313\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 86 training complete\n",
      "Cost on training data: 0.6515565989075534\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7249086275267104\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 87 training complete\n",
      "Cost on training data: 0.6326675401187821\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7053379125342198\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 88 training complete\n",
      "Cost on training data: 0.6342226137346072\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.7102064015696932\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 89 training complete\n",
      "Cost on training data: 0.6446380627474799\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7112871569748431\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90 training complete\n",
      "Cost on training data: 0.6913226417383374\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7536790001777696\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 91 training complete\n",
      "Cost on training data: 0.6442825825670144\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7020030191404553\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 92 training complete\n",
      "Cost on training data: 0.6190701416989212\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6782259598644444\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 93 training complete\n",
      "Cost on training data: 0.6500471442208211\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7122476792993312\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 94 training complete\n",
      "Cost on training data: 0.622678508481102\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6852758050084983\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 95 training complete\n",
      "Cost on training data: 0.670458050584544\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7255890811264231\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 96 training complete\n",
      "Cost on training data: 0.6295186589891046\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6909688304241133\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 97 training complete\n",
      "Cost on training data: 0.6123558349371463\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6752340684165927\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 98 training complete\n",
      "Cost on training data: 0.7030579734672713\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.77006960984045\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 99 training complete\n",
      "Cost on training data: 0.7202717374109671\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7774295617346862\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 100 training complete\n",
      "Cost on training data: 0.6390025270097992\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6988270153788123\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 101 training complete\n",
      "Cost on training data: 0.6288470792936609\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6931217793786295\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 102 training complete\n",
      "Cost on training data: 0.6447573134140872\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7101479595532312\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 103 training complete\n",
      "Cost on training data: 0.6484652071986792\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7153094560371659\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 104 training complete\n",
      "Cost on training data: 0.647348569264318\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7140435762632007\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 105 training complete\n",
      "Cost on training data: 0.6483817015965456\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7168815434581931\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 106 training complete\n",
      "Cost on training data: 0.7148484153961253\n",
      "Accuracy on training data: 71 / 95\n",
      "Cost on evaluation data: 0.7799630720658441\n",
      "Accuracy on evaluation data: 45 / 55\n",
      "\n",
      "Epoch 107 training complete\n",
      "Cost on training data: 0.6669662171533604\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7391529057205242\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 108 training complete\n",
      "Cost on training data: 0.6428926939820843\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.711233388611272\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 109 training complete\n",
      "Cost on training data: 0.6511535599359007\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7198040000965819\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 110 training complete\n",
      "Cost on training data: 0.6727791347208532\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7430944997809431\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 111 training complete\n",
      "Cost on training data: 0.6389388744887085\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.707045275336021\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 112 training complete\n",
      "Cost on training data: 0.6687005210479859\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7369573754153181\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 113 training complete\n",
      "Cost on training data: 0.6179566308713976\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6855460736699164\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 114 training complete\n",
      "Cost on training data: 0.6162067745005545\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6898058680995812\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 115 training complete\n",
      "Cost on training data: 0.6149443844788032\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6848531762347576\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 116 training complete\n",
      "Cost on training data: 0.649624294781625\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7113008641874128\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 117 training complete\n",
      "Cost on training data: 0.6039140521960088\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6667367192871332\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 118 training complete\n",
      "Cost on training data: 0.6182079992299924\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6801799369805426\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 119 training complete\n",
      "Cost on training data: 0.6202455882996767\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6850617345959753\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 120 training complete\n",
      "Cost on training data: 0.6589341035939289\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7266464118265911\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 121 training complete\n",
      "Cost on training data: 0.6206711946624842\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6900071897094369\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 122 training complete\n",
      "Cost on training data: 0.6548523369823405\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.712594934175009\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 123 training complete\n",
      "Cost on training data: 0.6303132448987047\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.7054195953520448\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 124 training complete\n",
      "Cost on training data: 0.6388157062210298\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7111210146209683\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 125 training complete\n",
      "Cost on training data: 0.6207682456448316\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.6869256921345754\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 126 training complete\n",
      "Cost on training data: 0.6169603597602081\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.682488744611755\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 127 training complete\n",
      "Cost on training data: 0.6259090817744174\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6903341996233496\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 128 training complete\n",
      "Cost on training data: 0.6334080254130038\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7020933774263651\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 129 training complete\n",
      "Cost on training data: 0.6419187963964\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.7087214087259374\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 130 training complete\n",
      "Cost on training data: 0.6445124800793618\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7143738860169235\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 131 training complete\n",
      "Cost on training data: 0.653105694770483\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7200485012217179\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 132 training complete\n",
      "Cost on training data: 0.642501049571716\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7089930983457902\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 133 training complete\n",
      "Cost on training data: 0.6505625317441076\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7161168528064499\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 134 training complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on training data: 0.6521579359991876\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7236904639864641\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 135 training complete\n",
      "Cost on training data: 0.6573946115994744\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7245639617955458\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 136 training complete\n",
      "Cost on training data: 0.6325851361789714\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.7021769716915294\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 137 training complete\n",
      "Cost on training data: 0.6712012433224124\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7301699910373064\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 138 training complete\n",
      "Cost on training data: 0.6523934508582353\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.718862859916056\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 139 training complete\n",
      "Cost on training data: 0.6513676559078349\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7051725981343794\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 140 training complete\n",
      "Cost on training data: 1.2330104387452863\n",
      "Accuracy on training data: 33 / 95\n",
      "Cost on evaluation data: 1.2812828228548905\n",
      "Accuracy on evaluation data: 17 / 55\n",
      "\n",
      "Epoch 141 training complete\n",
      "Cost on training data: 0.7863267682428389\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.8378814959007448\n",
      "Accuracy on evaluation data: 41 / 55\n",
      "\n",
      "Epoch 142 training complete\n",
      "Cost on training data: 0.6606274122921906\n",
      "Accuracy on training data: 75 / 95\n",
      "Cost on evaluation data: 0.7244007560004445\n",
      "Accuracy on evaluation data: 48 / 55\n",
      "\n",
      "Epoch 143 training complete\n",
      "Cost on training data: 0.6478379703479135\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7102251720240161\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 144 training complete\n",
      "Cost on training data: 0.6305029377776937\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.7003210379675635\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 145 training complete\n",
      "Cost on training data: 0.6271652751804313\n",
      "Accuracy on training data: 63 / 95\n",
      "Cost on evaluation data: 0.6990938233567222\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 146 training complete\n",
      "Cost on training data: 0.6445002910882222\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7058854528836218\n",
      "Accuracy on evaluation data: 42 / 55\n",
      "\n",
      "Epoch 147 training complete\n",
      "Cost on training data: 0.6279296334977281\n",
      "Accuracy on training data: 66 / 95\n",
      "Cost on evaluation data: 0.6977116667396837\n",
      "Accuracy on evaluation data: 44 / 55\n",
      "\n",
      "Epoch 148 training complete\n",
      "Cost on training data: 0.6197740070475007\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6959693495531856\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 149 training complete\n",
      "Cost on training data: 0.6865363696776465\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7578262665102782\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 150 training complete\n",
      "Cost on training data: 0.6505568886814642\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.7311288177804752\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 151 training complete\n",
      "Cost on training data: 0.7165282234613412\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7933168817720937\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 152 training complete\n",
      "Cost on training data: 0.6864956674290903\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7488719025659468\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 153 training complete\n",
      "Cost on training data: 1.0597477025810405\n",
      "Accuracy on training data: 34 / 95\n",
      "Cost on evaluation data: 1.1213405545795585\n",
      "Accuracy on evaluation data: 18 / 55\n",
      "\n",
      "Epoch 154 training complete\n",
      "Cost on training data: 0.708993957658967\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7768485385523751\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 155 training complete\n",
      "Cost on training data: 0.6859646310844356\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7600927052066284\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 156 training complete\n",
      "Cost on training data: 0.6480366107376148\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7148934322437763\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 157 training complete\n",
      "Cost on training data: 0.6432412319774602\n",
      "Accuracy on training data: 63 / 95\n",
      "Cost on evaluation data: 0.7141116996163226\n",
      "Accuracy on evaluation data: 34 / 55\n",
      "\n",
      "Epoch 158 training complete\n",
      "Cost on training data: 0.6369431543608022\n",
      "Accuracy on training data: 63 / 95\n",
      "Cost on evaluation data: 0.7092202154835621\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 159 training complete\n",
      "Cost on training data: 0.6674798976699052\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7308803168369464\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 160 training complete\n",
      "Cost on training data: 0.6375706474051028\n",
      "Accuracy on training data: 63 / 95\n",
      "Cost on evaluation data: 0.7129345874281499\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 161 training complete\n",
      "Cost on training data: 0.624848586453425\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6944715889907551\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 162 training complete\n",
      "Cost on training data: 0.6378000643298807\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7065133477909552\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 163 training complete\n",
      "Cost on training data: 0.6900337044598505\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7551426373908441\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 164 training complete\n",
      "Cost on training data: 0.6287472949908623\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.7055389216751096\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 165 training complete\n",
      "Cost on training data: 0.624818877743851\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6933400262319264\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 166 training complete\n",
      "Cost on training data: 0.6276544229700212\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.697976167663636\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 167 training complete\n",
      "Cost on training data: 0.6293066751779871\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6922463332173785\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 168 training complete\n",
      "Cost on training data: 0.6235690348576562\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.692105419024749\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 169 training complete\n",
      "Cost on training data: 0.6322784504645897\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.702916640330561\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 170 training complete\n",
      "Cost on training data: 0.6247912451466429\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6918417513024594\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 171 training complete\n",
      "Cost on training data: 0.6160663442570534\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6816011700386718\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 172 training complete\n",
      "Cost on training data: 0.7439462709320023\n",
      "Accuracy on training data: 67 / 95\n",
      "Cost on evaluation data: 0.8087341103580759\n",
      "Accuracy on evaluation data: 44 / 55\n",
      "\n",
      "Epoch 173 training complete\n",
      "Cost on training data: 0.6639858172832199\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7382857403535978\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 174 training complete\n",
      "Cost on training data: 0.6638496317242031\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7343589466863985\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 175 training complete\n",
      "Cost on training data: 0.7124562952344203\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7813178692270274\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 176 training complete\n",
      "Cost on training data: 0.8358905217518495\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.9087039244379402\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 177 training complete\n",
      "Cost on training data: 0.6790237385970612\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7621197189900601\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 178 training complete\n",
      "Cost on training data: 0.6616788320669507\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7404929833666176\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179 training complete\n",
      "Cost on training data: 0.7112163170743834\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7827899152820816\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 180 training complete\n",
      "Cost on training data: 0.6327187555837265\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7024691856691228\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 181 training complete\n",
      "Cost on training data: 0.6952766710576033\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7638115691916306\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 182 training complete\n",
      "Cost on training data: 0.649755489494994\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7172725224620009\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 183 training complete\n",
      "Cost on training data: 0.6582592990363325\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7198467273385241\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 184 training complete\n",
      "Cost on training data: 0.6218615960711181\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.6901916441070264\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 185 training complete\n",
      "Cost on training data: 0.66148921831528\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7276773380514988\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 186 training complete\n",
      "Cost on training data: 0.6816080201868516\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7483154660020569\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 187 training complete\n",
      "Cost on training data: 0.6366280599936233\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6997647361131473\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 188 training complete\n",
      "Cost on training data: 0.6792198252421104\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7345762240379433\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 189 training complete\n",
      "Cost on training data: 0.6391754616322968\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6995660258215235\n",
      "Accuracy on evaluation data: 43 / 55\n",
      "\n",
      "Epoch 190 training complete\n",
      "Cost on training data: 0.6457433477034143\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7004516036543873\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 191 training complete\n",
      "Cost on training data: 0.6917196265575761\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7526820416102193\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 192 training complete\n",
      "Cost on training data: 0.6376165924412457\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.7009256122275993\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 193 training complete\n",
      "Cost on training data: 0.6495095540912614\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7112098751618255\n",
      "Accuracy on evaluation data: 42 / 55\n",
      "\n",
      "Epoch 194 training complete\n",
      "Cost on training data: 0.6801518475265572\n",
      "Accuracy on training data: 76 / 95\n",
      "Cost on evaluation data: 0.7450725293351345\n",
      "Accuracy on evaluation data: 50 / 55\n",
      "\n",
      "Epoch 195 training complete\n",
      "Cost on training data: 0.6410685251242861\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7085894338810106\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 196 training complete\n",
      "Cost on training data: 0.652776246240648\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.7230308578099099\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 197 training complete\n",
      "Cost on training data: 0.6255131391308184\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.6911525564066882\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 198 training complete\n",
      "Cost on training data: 0.6164546235445142\n",
      "Accuracy on training data: 64 / 95\n",
      "Cost on evaluation data: 0.6870744782476848\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n",
      "Epoch 199 training complete\n",
      "Cost on training data: 0.6351883807665093\n",
      "Accuracy on training data: 65 / 95\n",
      "Cost on evaluation data: 0.6974148843989927\n",
      "Accuracy on evaluation data: 35 / 55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net2 = network2.load_network(\"./data/iris-4-20-7-3.dat\")\n",
    "\n",
    "# Set hyper-parameter values individually after the network\n",
    "net2.set_parameters(cost=network2.LogLikelihood,\n",
    "                    act_hidden = network2.ReLU,\n",
    "                    act_output = network2.Softmax,\n",
    "                    regularization=\"L2\", \n",
    "                    dropoutpercent=0.5)\n",
    "\n",
    "evaluation_cost12b, evaluation_accuracy12b, training_cost12b, training_accuracy12b = net2.SGD(iris_train, 200, 1, 0.05,\n",
    "         lmbda=3.0,\n",
    "         evaluation_data=iris_test, \n",
    "         monitor_evaluation_cost=True, \n",
    "         monitor_evaluation_accuracy=True,\n",
    "         monitor_training_cost=True,\n",
    "         monitor_training_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_lst=np.linspace(0,499,500, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "lines",
         "name": "LogLikelihood - Reg: Default, drop:0.1",
         "type": "scatter",
         "uid": "78b378b4-d34d-11e8-91d4-a820660c6deb",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          0.7164908656559965,
          0.4622341006614181,
          0.46917873348657074,
          0.4697690494586621,
          0.49739563805671505,
          0.4315818691239112,
          0.4337622886123998,
          0.38596244587540984,
          0.42570316520700596,
          0.2974264932909909,
          0.35253845329622696,
          0.35619648724113057,
          0.2710454270435016,
          0.32140455659657124,
          0.3742665368333511,
          0.25080542314606175,
          0.38196588509580065,
          0.29718250744986996,
          0.30155054061296593,
          0.2677209851194511,
          0.5572495528069465,
          0.5058145999038826,
          0.4885488239783015,
          0.4831317385066748,
          0.47954295471655356,
          0.4779945090452997,
          0.476914110181013,
          0.4752594562993959,
          0.47394873563394413,
          0.4725939707260362,
          0.4732396917922601,
          0.4725803984233273,
          0.4715187875206039,
          0.471637277176977,
          0.4707012770465192,
          0.47068075059415243,
          0.46981441115356937,
          0.4698102754963191,
          0.4697951379530958,
          0.4690476466133142,
          0.4683995698560495,
          0.46847471857349227,
          0.46792373851558305,
          0.46743996831847473,
          0.4669846244109398,
          0.4671246916974736,
          0.46669858935797176,
          0.4668454410151359,
          0.4664517444353516,
          0.46608633750514117,
          0.46634956432276736,
          0.46601712179775634,
          0.46570090632170624,
          0.4657761171953198,
          0.4655472097653393,
          0.46526708639403636,
          0.46544733451148357,
          0.4651736955085655,
          0.46492357616251245,
          0.46510190919247035,
          0.4648638051519727,
          0.46463231377921616,
          0.46441997743645497,
          0.4646998253612009,
          0.46450596973088065,
          0.46431018526986617,
          0.46412162228702597,
          0.4639494709619405,
          0.4638272122447828,
          0.463673000781709,
          0.4635257707216055,
          0.4636685825220047,
          0.4635694585715622,
          0.46343859786550917,
          0.4617534245687541,
          0.4616147687936419,
          1.8723620979900741,
          0.4820832507146503,
          0.4791533486677235,
          0.4780271914387797,
          0.4755846341608667,
          0.4737067099387562,
          0.4547532470057133,
          0.37147263810031517,
          0.385113442008069,
          0.47173470440739135,
          0.37145750938333505,
          0.36742037141949757,
          0.46631270221304005,
          0.43422092319965994,
          0.24773822813649568,
          0.4974168126945388,
          0.367444301193115,
          0.23481457547934434,
          0.44869850428527647,
          0.3942306944677563,
          0.37234021747542584,
          0.33682784948715133,
          0.29317220744438227,
          0.35016766991649756,
          0.29876024535528883,
          0.19634290347161987,
          0.27762788493745977,
          0.26044057483155686,
          0.19301747275913983,
          0.33977750105458465,
          0.22818273647392714,
          0.34996534162432386,
          0.352369746047956,
          0.29150431335282345,
          0.3931645888570352,
          0.3171112860755622,
          0.24830922668169256,
          0.3579918819235349,
          0.5132885713679756,
          0.37870248160161385,
          0.2808233889516715,
          0.2932556895613408,
          0.14630852266693029,
          0.3536521650968762,
          0.31075026186292265,
          0.2996245758648527,
          0.22479804217133448,
          0.25939239957466204,
          0.17667490138784495,
          0.24959305895008166,
          0.24772715770434584,
          0.20400218663186445,
          0.28779879642178985,
          0.18349238442053561,
          0.15053316417303195,
          0.3043833282513741,
          0.19690621344549641,
          0.1590753483803434,
          0.22588918031789088,
          0.16265484170254427,
          0.1032281538643379,
          0.21888442699526264,
          0.23074542526381409,
          0.295730938458594,
          0.19290285011536087,
          0.29143386692776724,
          0.2700043443039865,
          0.2794775444902732,
          0.18736288957913172,
          0.3709518954805212,
          0.2947718747607091,
          0.27315649429921884,
          0.193810386096638,
          0.17854533610159634,
          0.14978452675033752,
          0.22930759879287627,
          0.1683279422790566,
          0.13730822471801507,
          0.15192172825140757,
          0.17124841064055107,
          0.0925569163475596,
          0.1373347718686275,
          0.181884579939211,
          0.21444457453646693,
          0.17626488697239986,
          0.1636801053603809,
          0.15673404282421055,
          0.1088521452377157,
          0.12103159836889618,
          0.1373503342129476,
          0.30104930709212907,
          0.18281016796938743,
          0.2130354540410634,
          0.1287468994436223,
          0.29912003564487316,
          0.12054977203655601,
          0.14147800923537351,
          0.21235635844616266,
          0.09152529468636407,
          0.7101372949195631,
          0.17295400176663356,
          0.16587426866975513,
          0.1949611404818274,
          0.18482144539364748,
          0.45090574434665537,
          0.08958039012752726,
          0.249053814732886,
          0.3378400491415598,
          0.11163592585479833,
          0.10423722679811123,
          0.1575058440221353,
          0.3259257407677643,
          0.24510299526291093,
          0.1527373662058138,
          0.15242926229432538,
          0.21850544244235998,
          0.17836415160045657,
          0.1401045216562318,
          0.10348405645640216,
          0.17259922620217985,
          0.18066709037530918,
          0.08772878935622289,
          0.25651565703277573,
          0.21773404499120436
         ]
        },
        {
         "mode": "lines",
         "name": "LogLikelihood - Reg: Default, drop:0.5",
         "type": "scatter",
         "uid": "78b37a6c-d34d-11e8-a89a-a820660c6deb",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          1.1629585567984275,
          1.1097333394900661,
          1.1006733710386802,
          1.0980600871259705,
          1.098328998314477,
          1.0987950863863687,
          1.098727087051333,
          1.1004312500835485,
          1.0989992155323531,
          1.0986117525614607,
          1.0980019157490715,
          1.0983562568523417,
          1.0996112197668364,
          1.1021395944113368,
          1.0632689073643629,
          0.8080247999508664,
          0.8246603403293616,
          0.6929984274934152,
          0.8492300414544807,
          0.6687621859615464,
          0.6406120786078874,
          0.6253050897522108,
          0.6012346782159245,
          0.5564411606672567,
          0.5399277942956558,
          0.539630830956343,
          0.5421933929398896,
          0.49665422280376403,
          0.5181397729965925,
          0.4780716991569214,
          0.43211549216302936,
          0.5240198241968946,
          0.5269971707494066,
          0.5109061754215813,
          0.45654062187389366,
          0.47318699847827683,
          0.4231202435451687,
          0.47550759111147756,
          0.46305748376062467,
          0.5129579362838796,
          0.4502518200920423,
          0.4830072790104559,
          0.43053643134339636,
          0.5124255732136886,
          0.4715483073759084,
          0.5205337571318357,
          0.4222334394696623,
          0.46534569142964644,
          0.452448666749315,
          0.3665300995513667,
          0.501104542805517,
          0.5116979801138427,
          0.48386152202577537,
          0.35855501568011117,
          0.3834579312139333,
          0.4791673952641294,
          0.4227131817120184,
          0.38763864998913206,
          0.43892171933057295,
          0.42652957613064385,
          0.4782199568103582,
          0.3535575050671542,
          0.6067161283832935,
          0.5130181200570765,
          0.4873274312384093,
          0.4305047137320457,
          0.435205368178903,
          0.3600692818999096,
          0.4795168126476447,
          0.4581786093840343,
          0.44460245251533315,
          0.4780632520013595,
          0.4434102181118611,
          0.4548077504478311,
          0.4230291216247516,
          0.43834327921927624,
          0.5105488400627284,
          0.48186845621987,
          0.4933216985759445,
          0.4852708964397552,
          0.43432984887434767,
          0.45760237169541423,
          0.48259992300870774,
          0.4315926109636781,
          0.39867676469717117,
          0.4103043028851312,
          0.4973475083412541,
          0.43825961234491223,
          0.4061201648549185,
          0.46146346155230095,
          0.406857047273826,
          0.43138882109178184,
          0.4431525188665447,
          0.5304390271534536,
          0.4796037272667612,
          0.4177210383733021,
          0.4305851189360443,
          0.417398768155259,
          0.4197969567500033,
          0.43505118494648337,
          0.38103519573832534,
          0.39766597827398814,
          0.41397222159946145,
          0.49406012142426364,
          0.5237337716030764,
          0.4078274784450705,
          0.36636625841053183,
          0.3848913800485732,
          0.47584554563152615,
          0.4814802554842633,
          0.42323824219520584,
          0.37500201445950176,
          0.4228394361112547,
          0.5416645538699997,
          0.42462304331660583,
          0.36719834765951853,
          0.4128366780714473,
          0.4939169944332245,
          0.4599535210724015,
          0.431972174125415,
          0.5425786207429432,
          0.42314167159183974,
          0.36578374030681826,
          0.35249750848905276,
          0.38919428101573855,
          0.46569261807893747,
          0.35135852083385677,
          0.40035952057648305,
          0.3576403725431176,
          0.37188434898863787,
          0.4135124715407818,
          0.4901837235131784,
          0.4040726733073337,
          0.33826106841738246,
          0.4359912547275019,
          0.4525963610387573,
          0.37373819905184985,
          0.35562300138501046,
          0.39979308914167583,
          0.38278480973230544,
          0.42170120389339305,
          0.5315198667239183,
          0.554240634701162,
          0.34912342129816254,
          0.33108979257202065,
          0.5228985701653293,
          0.5157400138744707,
          0.46359302005909775,
          0.31120734162330743,
          0.3863039368996235,
          0.43019164734873155,
          0.3015121987966179,
          0.5241585005924155,
          0.49143339359057164,
          0.4974547693422135,
          0.5027554827695566,
          0.5218962370880935,
          0.4903360976557699,
          0.5018217986196556,
          0.5558823166237193,
          0.5118230906966439,
          0.4988621510716999,
          0.5051513630964842,
          0.4941151818868642,
          0.49524649564189316,
          0.5073965553902734,
          0.4901344213051896,
          0.4835377433650909,
          0.4881325038770157,
          0.4806009398527655,
          0.502010971774973,
          0.5436257686869581,
          0.5752614131538251,
          0.5456394364602671,
          0.5259624391312068,
          0.5001871304672227,
          0.5019819375517008,
          0.5155966538313275,
          0.48330498879446526,
          0.48899240062777205,
          0.5246545852852189,
          0.4878998521777327,
          0.505392735730466,
          0.492265728515503,
          0.48441075965125774,
          0.49386771934866286,
          0.48366442795423636,
          0.49622482900629455,
          0.48416144145320766,
          0.4776684702594863,
          0.4962183435230193,
          0.49541749398276713,
          0.48557321068186027,
          0.4760325015052019,
          0.48420565538112,
          0.5043855062920147,
          0.48802182769899727,
          0.5063793021026033,
          0.51478507914588,
          0.501052070390548
         ]
        },
        {
         "mode": "lines",
         "name": "LogLikelihood - Reg: L2, lmbda:3.0, drop:0.1",
         "type": "scatter",
         "uid": "78b37b5c-d34d-11e8-bf88-a820660c6deb",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          1.0827071181624002,
          0.826804540827575,
          0.7647625684225705,
          0.7153202833707653,
          0.6990922616747106,
          0.6257567178592176,
          0.6472473064417914,
          0.592831651354484,
          0.6363739048430962,
          0.6221133922784523,
          0.5574813998747119,
          0.5545042233199229,
          0.5487422901058128,
          0.5716869736970941,
          0.597499520596726,
          0.515208778396197,
          0.6191877503947456,
          0.5322004043497561,
          0.5981609809058734,
          0.6045187448198195,
          0.6082905404308121,
          0.6043651915665638,
          0.615717259040008,
          0.6486895613337443,
          0.5950852625892372,
          0.5928640835035098,
          0.5370026653458939,
          0.5252761163133142,
          0.5244056691297877,
          0.5633679202021807,
          0.544769076218065,
          0.5373657907540161,
          0.5469401904834735,
          0.593495327660761,
          0.5733825374624161,
          0.5382109288396915,
          0.4606743805682837,
          0.5685111067620462,
          0.5214156829799306,
          0.5628809407657434,
          0.6264428639527784,
          0.5608648239933978,
          0.5524150953309045,
          0.6188312609778013,
          0.5969494936814912,
          0.5283121573698737,
          0.6722449425496445,
          0.5617567094689573,
          0.5306422428926405,
          0.5761207943986053,
          0.5347641544350911,
          0.5566849161091533,
          0.5997108621144853,
          0.6583961059841215,
          0.7873779527419873,
          0.502501626216347,
          0.714419646385718,
          0.6171258269614761,
          0.5187731849634077,
          0.5427856727505257,
          0.46851792704915096,
          0.5994944873927814,
          0.6075700510948343,
          0.49711842227752195,
          0.543687282807135,
          0.5793312709726803,
          0.5435730652808035,
          0.483049765194835,
          0.5510247993462433,
          0.5768243988073618,
          0.6156128805939229,
          0.6110981467022679,
          0.6891967147297247,
          0.6111134192786142,
          0.5265286881486643,
          0.5397517702176394,
          0.6128750415596544,
          0.5970952192244172,
          0.6195435507772117,
          0.4948434821885964,
          0.5158064459333255,
          0.6798512922059736,
          0.5419572620297598,
          0.7079646762801699,
          0.49477473479229817,
          0.7061870622357116,
          0.5394990558164332,
          0.6111811393446774,
          0.6017596020321093,
          0.5554407874620552,
          0.5435202972114537,
          0.6236109372990832,
          0.5346724537812919,
          0.4907662480130258,
          0.6750371221591196,
          0.6108940638929945,
          0.6864884112251881,
          0.52680967373011,
          0.7090651449594864,
          0.5190671213638802,
          0.47106418260278193,
          0.6704578625431471,
          0.6560398317592099,
          0.5567752158083785,
          0.5490094340970957,
          0.5338182171824495,
          0.48598274051046036,
          0.5154723444066337,
          0.5166099328256045,
          0.557787674916951,
          0.5501184147612832,
          0.6650147071282768,
          0.5506883008590376,
          1.1065975807056585,
          0.5131294781948684,
          0.6146009622039519,
          0.6663177547657299,
          0.5048789496125287,
          0.4888546431135081,
          0.5113755146017355,
          0.5545091324338534,
          0.5040003722553176,
          0.7751564222643819,
          0.5266558499923311,
          0.782665792967037,
          0.5498569750603521,
          0.5216812883511727,
          0.6796432004579263,
          0.5457550420423571,
          0.5534469168702427,
          0.5372157056129503,
          0.5979007627893514,
          0.4756411762504866,
          0.42239218596134964,
          0.6044937192073457,
          0.7430136109817612,
          0.5359444216731211,
          0.5099883048144106,
          0.5407888778018002,
          0.5568351534419581,
          0.4899505438276742,
          0.5463758176016026,
          0.4677644764805543,
          0.4952349582840796,
          0.7404839213121742,
          0.6308641092128826,
          0.5547440734564113,
          0.5693026771298922,
          0.47006514454747,
          0.5870129387259957,
          0.5735409466355882,
          0.5507334580240324,
          0.5759600137600568,
          0.496569250199449,
          0.49138311712971083,
          0.5759375930162373,
          0.4652080246629663,
          0.566663118437795,
          0.5703911072662614,
          0.43178924214989145,
          0.604361324550265,
          0.6033523890054628,
          0.5025798716380067,
          0.46566708407205193,
          0.6267877935450227,
          0.5838452348257257,
          0.4881241578132155,
          0.6182512585076224,
          0.571799755426043,
          0.5468259753249688,
          0.5934657925175139,
          0.5595814475361292,
          0.5359502359345372,
          0.5769694393212474,
          0.653267386472472,
          0.578018823745502,
          0.5003418925402341,
          0.6056255838826318,
          0.42869943015162526,
          0.559391418902847,
          0.6082678844213967,
          0.5744762131673649,
          0.5701600219823789,
          0.43348122776356585,
          0.6654148639099703,
          0.4468545692163674,
          0.4532938821404492,
          0.7330793323302076,
          0.6370234912046528,
          0.5192392104346817,
          0.6542651517322279,
          0.569267344119249,
          0.5213647959985594,
          0.664562569727177,
          0.563987191488066,
          0.5878738626008428,
          0.6242226608194973,
          0.5549931513758162,
          0.7329364611827192,
          0.5032579716886514
         ]
        },
        {
         "mode": "lines",
         "name": "LogLikelihood - Reg: L2, lmbda:3.0, drop:0.5",
         "type": "scatter",
         "uid": "78b37c2e-d34d-11e8-8fe7-a820660c6deb",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          1.5405845877050528,
          1.387194054010197,
          1.323340292901742,
          1.2311248762537241,
          1.205818531779822,
          1.1731832101048956,
          1.0606462688950524,
          1.051570003242763,
          1.008862110757464,
          0.9160028710076816,
          0.9006050951465523,
          0.7685479371826365,
          0.7512198303145976,
          0.7305642888536376,
          0.7240028513993721,
          0.7849598147525217,
          0.7416759591128589,
          0.6893626634709996,
          0.70780542414584,
          0.6650484919561819,
          0.65030972107357,
          0.6646947757206649,
          0.7004528797046334,
          0.6435282040950843,
          0.6580383275128397,
          0.6170513420934524,
          0.730225575328911,
          0.621757711283268,
          0.61402352849455,
          0.6313573359492465,
          0.6423428729371126,
          0.6655345375155576,
          0.6376791713792969,
          0.6270721470337844,
          0.6877010901786285,
          0.6523040245856222,
          0.6435086134122379,
          0.6712251207091811,
          0.6270365641482922,
          0.7109590894587206,
          0.6642243597846627,
          0.6768157497147129,
          0.646178738709692,
          0.6734518557926036,
          0.632454532038847,
          0.7740813856855139,
          0.672665972671417,
          0.6641406033059176,
          0.624591439715721,
          0.732075167943322,
          0.6822298530553929,
          0.6968889577692647,
          0.7744053507501513,
          0.6545546982924351,
          0.6565597505963319,
          0.6363664769561815,
          0.6332483344463856,
          0.6264068993102443,
          0.6644126100508222,
          0.6617683345021023,
          0.7891918392836264,
          0.6898415799677412,
          0.7132856362004706,
          0.6608732065781896,
          0.6422257858900806,
          0.6562689420619221,
          0.6402949795581163,
          0.6813568440048958,
          0.6809151413920741,
          0.6582941195163543,
          0.6201293508989545,
          0.725571729702972,
          0.6483207437027514,
          0.6244908974075357,
          0.6268547219371476,
          0.6321349247250273,
          0.6230223008592845,
          0.6186626477966848,
          0.6189789412746977,
          0.6097160219209902,
          0.6389124445900259,
          0.6194494486271207,
          0.6296443684885928,
          0.6467481814245709,
          0.6965856405438512,
          0.6640332362939279,
          0.6515565989075534,
          0.6326675401187821,
          0.6342226137346072,
          0.6446380627474799,
          0.6913226417383374,
          0.6442825825670144,
          0.6190701416989212,
          0.6500471442208211,
          0.622678508481102,
          0.670458050584544,
          0.6295186589891046,
          0.6123558349371463,
          0.7030579734672713,
          0.7202717374109671,
          0.6390025270097992,
          0.6288470792936609,
          0.6447573134140872,
          0.6484652071986792,
          0.647348569264318,
          0.6483817015965456,
          0.7148484153961253,
          0.6669662171533604,
          0.6428926939820843,
          0.6511535599359007,
          0.6727791347208532,
          0.6389388744887085,
          0.6687005210479859,
          0.6179566308713976,
          0.6162067745005545,
          0.6149443844788032,
          0.649624294781625,
          0.6039140521960088,
          0.6182079992299924,
          0.6202455882996767,
          0.6589341035939289,
          0.6206711946624842,
          0.6548523369823405,
          0.6303132448987047,
          0.6388157062210298,
          0.6207682456448316,
          0.6169603597602081,
          0.6259090817744174,
          0.6334080254130038,
          0.6419187963964,
          0.6445124800793618,
          0.653105694770483,
          0.642501049571716,
          0.6505625317441076,
          0.6521579359991876,
          0.6573946115994744,
          0.6325851361789714,
          0.6712012433224124,
          0.6523934508582353,
          0.6513676559078349,
          1.2330104387452863,
          0.7863267682428389,
          0.6606274122921906,
          0.6478379703479135,
          0.6305029377776937,
          0.6271652751804313,
          0.6445002910882222,
          0.6279296334977281,
          0.6197740070475007,
          0.6865363696776465,
          0.6505568886814642,
          0.7165282234613412,
          0.6864956674290903,
          1.0597477025810405,
          0.708993957658967,
          0.6859646310844356,
          0.6480366107376148,
          0.6432412319774602,
          0.6369431543608022,
          0.6674798976699052,
          0.6375706474051028,
          0.624848586453425,
          0.6378000643298807,
          0.6900337044598505,
          0.6287472949908623,
          0.624818877743851,
          0.6276544229700212,
          0.6293066751779871,
          0.6235690348576562,
          0.6322784504645897,
          0.6247912451466429,
          0.6160663442570534,
          0.7439462709320023,
          0.6639858172832199,
          0.6638496317242031,
          0.7124562952344203,
          0.8358905217518495,
          0.6790237385970612,
          0.6616788320669507,
          0.7112163170743834,
          0.6327187555837265,
          0.6952766710576033,
          0.649755489494994,
          0.6582592990363325,
          0.6218615960711181,
          0.66148921831528,
          0.6816080201868516,
          0.6366280599936233,
          0.6792198252421104,
          0.6391754616322968,
          0.6457433477034143,
          0.6917196265575761,
          0.6376165924412457,
          0.6495095540912614,
          0.6801518475265572,
          0.6410685251242861,
          0.652776246240648,
          0.6255131391308184,
          0.6164546235445142,
          0.6351883807665093
         ]
        }
       ],
       "layout": {
        "title": "Cost: hidden: ReLU / Output: Softmax / Network: 4-20-7-3 (Train: 95 instances)",
        "xaxis": {
         "title": "Epoch"
        },
        "yaxis": {
         "title": "Cost"
        }
       }
      },
      "text/html": [
       "<div id=\"1a55fb75-76f1-45d2-9367-5cc7d70cb1ed\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "        Plotly.plot(\n",
       "            '1a55fb75-76f1-45d2-9367-5cc7d70cb1ed',\n",
       "            [{\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: Default, drop:0.1\", \"x\": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 251.0, 252.0, 253.0, 254.0, 255.0, 256.0, 257.0, 258.0, 259.0, 260.0, 261.0, 262.0, 263.0, 264.0, 265.0, 266.0, 267.0, 268.0, 269.0, 270.0, 271.0, 272.0, 273.0, 274.0, 275.0, 276.0, 277.0, 278.0, 279.0, 280.0, 281.0, 282.0, 283.0, 284.0, 285.0, 286.0, 287.0, 288.0, 289.0, 290.0, 291.0, 292.0, 293.0, 294.0, 295.0, 296.0, 297.0, 298.0, 299.0, 300.0, 301.0, 302.0, 303.0, 304.0, 305.0, 306.0, 307.0, 308.0, 309.0, 310.0, 311.0, 312.0, 313.0, 314.0, 315.0, 316.0, 317.0, 318.0, 319.0, 320.0, 321.0, 322.0, 323.0, 324.0, 325.0, 326.0, 327.0, 328.0, 329.0, 330.0, 331.0, 332.0, 333.0, 334.0, 335.0, 336.0, 337.0, 338.0, 339.0, 340.0, 341.0, 342.0, 343.0, 344.0, 345.0, 346.0, 347.0, 348.0, 349.0, 350.0, 351.0, 352.0, 353.0, 354.0, 355.0, 356.0, 357.0, 358.0, 359.0, 360.0, 361.0, 362.0, 363.0, 364.0, 365.0, 366.0, 367.0, 368.0, 369.0, 370.0, 371.0, 372.0, 373.0, 374.0, 375.0, 376.0, 377.0, 378.0, 379.0, 380.0, 381.0, 382.0, 383.0, 384.0, 385.0, 386.0, 387.0, 388.0, 389.0, 390.0, 391.0, 392.0, 393.0, 394.0, 395.0, 396.0, 397.0, 398.0, 399.0, 400.0, 401.0, 402.0, 403.0, 404.0, 405.0, 406.0, 407.0, 408.0, 409.0, 410.0, 411.0, 412.0, 413.0, 414.0, 415.0, 416.0, 417.0, 418.0, 419.0, 420.0, 421.0, 422.0, 423.0, 424.0, 425.0, 426.0, 427.0, 428.0, 429.0, 430.0, 431.0, 432.0, 433.0, 434.0, 435.0, 436.0, 437.0, 438.0, 439.0, 440.0, 441.0, 442.0, 443.0, 444.0, 445.0, 446.0, 447.0, 448.0, 449.0, 450.0, 451.0, 452.0, 453.0, 454.0, 455.0, 456.0, 457.0, 458.0, 459.0, 460.0, 461.0, 462.0, 463.0, 464.0, 465.0, 466.0, 467.0, 468.0, 469.0, 470.0, 471.0, 472.0, 473.0, 474.0, 475.0, 476.0, 477.0, 478.0, 479.0, 480.0, 481.0, 482.0, 483.0, 484.0, 485.0, 486.0, 487.0, 488.0, 489.0, 490.0, 491.0, 492.0, 493.0, 494.0, 495.0, 496.0, 497.0, 498.0, 499.0], \"y\": [0.7164908656559965, 0.4622341006614181, 0.46917873348657074, 0.4697690494586621, 0.49739563805671505, 0.4315818691239112, 0.4337622886123998, 0.38596244587540984, 0.42570316520700596, 0.2974264932909909, 0.35253845329622696, 0.35619648724113057, 0.2710454270435016, 0.32140455659657124, 0.3742665368333511, 0.25080542314606175, 0.38196588509580065, 0.29718250744986996, 0.30155054061296593, 0.2677209851194511, 0.5572495528069465, 0.5058145999038826, 0.4885488239783015, 0.4831317385066748, 0.47954295471655356, 0.4779945090452997, 0.476914110181013, 0.4752594562993959, 0.47394873563394413, 0.4725939707260362, 0.4732396917922601, 0.4725803984233273, 0.4715187875206039, 0.471637277176977, 0.4707012770465192, 0.47068075059415243, 0.46981441115356937, 0.4698102754963191, 0.4697951379530958, 0.4690476466133142, 0.4683995698560495, 0.46847471857349227, 0.46792373851558305, 0.46743996831847473, 0.4669846244109398, 0.4671246916974736, 0.46669858935797176, 0.4668454410151359, 0.4664517444353516, 0.46608633750514117, 0.46634956432276736, 0.46601712179775634, 0.46570090632170624, 0.4657761171953198, 0.4655472097653393, 0.46526708639403636, 0.46544733451148357, 0.4651736955085655, 0.46492357616251245, 0.46510190919247035, 0.4648638051519727, 0.46463231377921616, 0.46441997743645497, 0.4646998253612009, 0.46450596973088065, 0.46431018526986617, 0.46412162228702597, 0.4639494709619405, 0.4638272122447828, 0.463673000781709, 0.4635257707216055, 0.4636685825220047, 0.4635694585715622, 0.46343859786550917, 0.4617534245687541, 0.4616147687936419, 1.8723620979900741, 0.4820832507146503, 0.4791533486677235, 0.4780271914387797, 0.4755846341608667, 0.4737067099387562, 0.4547532470057133, 0.37147263810031517, 0.385113442008069, 0.47173470440739135, 0.37145750938333505, 0.36742037141949757, 0.46631270221304005, 0.43422092319965994, 0.24773822813649568, 0.4974168126945388, 0.367444301193115, 0.23481457547934434, 0.44869850428527647, 0.3942306944677563, 0.37234021747542584, 0.33682784948715133, 0.29317220744438227, 0.35016766991649756, 0.29876024535528883, 0.19634290347161987, 0.27762788493745977, 0.26044057483155686, 0.19301747275913983, 0.33977750105458465, 0.22818273647392714, 0.34996534162432386, 0.352369746047956, 0.29150431335282345, 0.3931645888570352, 0.3171112860755622, 0.24830922668169256, 0.3579918819235349, 0.5132885713679756, 0.37870248160161385, 0.2808233889516715, 0.2932556895613408, 0.14630852266693029, 0.3536521650968762, 0.31075026186292265, 0.2996245758648527, 0.22479804217133448, 0.25939239957466204, 0.17667490138784495, 0.24959305895008166, 0.24772715770434584, 0.20400218663186445, 0.28779879642178985, 0.18349238442053561, 0.15053316417303195, 0.3043833282513741, 0.19690621344549641, 0.1590753483803434, 0.22588918031789088, 0.16265484170254427, 0.1032281538643379, 0.21888442699526264, 0.23074542526381409, 0.295730938458594, 0.19290285011536087, 0.29143386692776724, 0.2700043443039865, 0.2794775444902732, 0.18736288957913172, 0.3709518954805212, 0.2947718747607091, 0.27315649429921884, 0.193810386096638, 0.17854533610159634, 0.14978452675033752, 0.22930759879287627, 0.1683279422790566, 0.13730822471801507, 0.15192172825140757, 0.17124841064055107, 0.0925569163475596, 0.1373347718686275, 0.181884579939211, 0.21444457453646693, 0.17626488697239986, 0.1636801053603809, 0.15673404282421055, 0.1088521452377157, 0.12103159836889618, 0.1373503342129476, 0.30104930709212907, 0.18281016796938743, 0.2130354540410634, 0.1287468994436223, 0.29912003564487316, 0.12054977203655601, 0.14147800923537351, 0.21235635844616266, 0.09152529468636407, 0.7101372949195631, 0.17295400176663356, 0.16587426866975513, 0.1949611404818274, 0.18482144539364748, 0.45090574434665537, 0.08958039012752726, 0.249053814732886, 0.3378400491415598, 0.11163592585479833, 0.10423722679811123, 0.1575058440221353, 0.3259257407677643, 0.24510299526291093, 0.1527373662058138, 0.15242926229432538, 0.21850544244235998, 0.17836415160045657, 0.1401045216562318, 0.10348405645640216, 0.17259922620217985, 0.18066709037530918, 0.08772878935622289, 0.25651565703277573, 0.21773404499120436], \"type\": \"scatter\", \"uid\": \"78b378b4-d34d-11e8-91d4-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: Default, drop:0.5\", \"x\": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 251.0, 252.0, 253.0, 254.0, 255.0, 256.0, 257.0, 258.0, 259.0, 260.0, 261.0, 262.0, 263.0, 264.0, 265.0, 266.0, 267.0, 268.0, 269.0, 270.0, 271.0, 272.0, 273.0, 274.0, 275.0, 276.0, 277.0, 278.0, 279.0, 280.0, 281.0, 282.0, 283.0, 284.0, 285.0, 286.0, 287.0, 288.0, 289.0, 290.0, 291.0, 292.0, 293.0, 294.0, 295.0, 296.0, 297.0, 298.0, 299.0, 300.0, 301.0, 302.0, 303.0, 304.0, 305.0, 306.0, 307.0, 308.0, 309.0, 310.0, 311.0, 312.0, 313.0, 314.0, 315.0, 316.0, 317.0, 318.0, 319.0, 320.0, 321.0, 322.0, 323.0, 324.0, 325.0, 326.0, 327.0, 328.0, 329.0, 330.0, 331.0, 332.0, 333.0, 334.0, 335.0, 336.0, 337.0, 338.0, 339.0, 340.0, 341.0, 342.0, 343.0, 344.0, 345.0, 346.0, 347.0, 348.0, 349.0, 350.0, 351.0, 352.0, 353.0, 354.0, 355.0, 356.0, 357.0, 358.0, 359.0, 360.0, 361.0, 362.0, 363.0, 364.0, 365.0, 366.0, 367.0, 368.0, 369.0, 370.0, 371.0, 372.0, 373.0, 374.0, 375.0, 376.0, 377.0, 378.0, 379.0, 380.0, 381.0, 382.0, 383.0, 384.0, 385.0, 386.0, 387.0, 388.0, 389.0, 390.0, 391.0, 392.0, 393.0, 394.0, 395.0, 396.0, 397.0, 398.0, 399.0, 400.0, 401.0, 402.0, 403.0, 404.0, 405.0, 406.0, 407.0, 408.0, 409.0, 410.0, 411.0, 412.0, 413.0, 414.0, 415.0, 416.0, 417.0, 418.0, 419.0, 420.0, 421.0, 422.0, 423.0, 424.0, 425.0, 426.0, 427.0, 428.0, 429.0, 430.0, 431.0, 432.0, 433.0, 434.0, 435.0, 436.0, 437.0, 438.0, 439.0, 440.0, 441.0, 442.0, 443.0, 444.0, 445.0, 446.0, 447.0, 448.0, 449.0, 450.0, 451.0, 452.0, 453.0, 454.0, 455.0, 456.0, 457.0, 458.0, 459.0, 460.0, 461.0, 462.0, 463.0, 464.0, 465.0, 466.0, 467.0, 468.0, 469.0, 470.0, 471.0, 472.0, 473.0, 474.0, 475.0, 476.0, 477.0, 478.0, 479.0, 480.0, 481.0, 482.0, 483.0, 484.0, 485.0, 486.0, 487.0, 488.0, 489.0, 490.0, 491.0, 492.0, 493.0, 494.0, 495.0, 496.0, 497.0, 498.0, 499.0], \"y\": [1.1629585567984275, 1.1097333394900661, 1.1006733710386802, 1.0980600871259705, 1.098328998314477, 1.0987950863863687, 1.098727087051333, 1.1004312500835485, 1.0989992155323531, 1.0986117525614607, 1.0980019157490715, 1.0983562568523417, 1.0996112197668364, 1.1021395944113368, 1.0632689073643629, 0.8080247999508664, 0.8246603403293616, 0.6929984274934152, 0.8492300414544807, 0.6687621859615464, 0.6406120786078874, 0.6253050897522108, 0.6012346782159245, 0.5564411606672567, 0.5399277942956558, 0.539630830956343, 0.5421933929398896, 0.49665422280376403, 0.5181397729965925, 0.4780716991569214, 0.43211549216302936, 0.5240198241968946, 0.5269971707494066, 0.5109061754215813, 0.45654062187389366, 0.47318699847827683, 0.4231202435451687, 0.47550759111147756, 0.46305748376062467, 0.5129579362838796, 0.4502518200920423, 0.4830072790104559, 0.43053643134339636, 0.5124255732136886, 0.4715483073759084, 0.5205337571318357, 0.4222334394696623, 0.46534569142964644, 0.452448666749315, 0.3665300995513667, 0.501104542805517, 0.5116979801138427, 0.48386152202577537, 0.35855501568011117, 0.3834579312139333, 0.4791673952641294, 0.4227131817120184, 0.38763864998913206, 0.43892171933057295, 0.42652957613064385, 0.4782199568103582, 0.3535575050671542, 0.6067161283832935, 0.5130181200570765, 0.4873274312384093, 0.4305047137320457, 0.435205368178903, 0.3600692818999096, 0.4795168126476447, 0.4581786093840343, 0.44460245251533315, 0.4780632520013595, 0.4434102181118611, 0.4548077504478311, 0.4230291216247516, 0.43834327921927624, 0.5105488400627284, 0.48186845621987, 0.4933216985759445, 0.4852708964397552, 0.43432984887434767, 0.45760237169541423, 0.48259992300870774, 0.4315926109636781, 0.39867676469717117, 0.4103043028851312, 0.4973475083412541, 0.43825961234491223, 0.4061201648549185, 0.46146346155230095, 0.406857047273826, 0.43138882109178184, 0.4431525188665447, 0.5304390271534536, 0.4796037272667612, 0.4177210383733021, 0.4305851189360443, 0.417398768155259, 0.4197969567500033, 0.43505118494648337, 0.38103519573832534, 0.39766597827398814, 0.41397222159946145, 0.49406012142426364, 0.5237337716030764, 0.4078274784450705, 0.36636625841053183, 0.3848913800485732, 0.47584554563152615, 0.4814802554842633, 0.42323824219520584, 0.37500201445950176, 0.4228394361112547, 0.5416645538699997, 0.42462304331660583, 0.36719834765951853, 0.4128366780714473, 0.4939169944332245, 0.4599535210724015, 0.431972174125415, 0.5425786207429432, 0.42314167159183974, 0.36578374030681826, 0.35249750848905276, 0.38919428101573855, 0.46569261807893747, 0.35135852083385677, 0.40035952057648305, 0.3576403725431176, 0.37188434898863787, 0.4135124715407818, 0.4901837235131784, 0.4040726733073337, 0.33826106841738246, 0.4359912547275019, 0.4525963610387573, 0.37373819905184985, 0.35562300138501046, 0.39979308914167583, 0.38278480973230544, 0.42170120389339305, 0.5315198667239183, 0.554240634701162, 0.34912342129816254, 0.33108979257202065, 0.5228985701653293, 0.5157400138744707, 0.46359302005909775, 0.31120734162330743, 0.3863039368996235, 0.43019164734873155, 0.3015121987966179, 0.5241585005924155, 0.49143339359057164, 0.4974547693422135, 0.5027554827695566, 0.5218962370880935, 0.4903360976557699, 0.5018217986196556, 0.5558823166237193, 0.5118230906966439, 0.4988621510716999, 0.5051513630964842, 0.4941151818868642, 0.49524649564189316, 0.5073965553902734, 0.4901344213051896, 0.4835377433650909, 0.4881325038770157, 0.4806009398527655, 0.502010971774973, 0.5436257686869581, 0.5752614131538251, 0.5456394364602671, 0.5259624391312068, 0.5001871304672227, 0.5019819375517008, 0.5155966538313275, 0.48330498879446526, 0.48899240062777205, 0.5246545852852189, 0.4878998521777327, 0.505392735730466, 0.492265728515503, 0.48441075965125774, 0.49386771934866286, 0.48366442795423636, 0.49622482900629455, 0.48416144145320766, 0.4776684702594863, 0.4962183435230193, 0.49541749398276713, 0.48557321068186027, 0.4760325015052019, 0.48420565538112, 0.5043855062920147, 0.48802182769899727, 0.5063793021026033, 0.51478507914588, 0.501052070390548], \"type\": \"scatter\", \"uid\": \"78b37a6c-d34d-11e8-a89a-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: L2, lmbda:3.0, drop:0.1\", \"x\": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 251.0, 252.0, 253.0, 254.0, 255.0, 256.0, 257.0, 258.0, 259.0, 260.0, 261.0, 262.0, 263.0, 264.0, 265.0, 266.0, 267.0, 268.0, 269.0, 270.0, 271.0, 272.0, 273.0, 274.0, 275.0, 276.0, 277.0, 278.0, 279.0, 280.0, 281.0, 282.0, 283.0, 284.0, 285.0, 286.0, 287.0, 288.0, 289.0, 290.0, 291.0, 292.0, 293.0, 294.0, 295.0, 296.0, 297.0, 298.0, 299.0, 300.0, 301.0, 302.0, 303.0, 304.0, 305.0, 306.0, 307.0, 308.0, 309.0, 310.0, 311.0, 312.0, 313.0, 314.0, 315.0, 316.0, 317.0, 318.0, 319.0, 320.0, 321.0, 322.0, 323.0, 324.0, 325.0, 326.0, 327.0, 328.0, 329.0, 330.0, 331.0, 332.0, 333.0, 334.0, 335.0, 336.0, 337.0, 338.0, 339.0, 340.0, 341.0, 342.0, 343.0, 344.0, 345.0, 346.0, 347.0, 348.0, 349.0, 350.0, 351.0, 352.0, 353.0, 354.0, 355.0, 356.0, 357.0, 358.0, 359.0, 360.0, 361.0, 362.0, 363.0, 364.0, 365.0, 366.0, 367.0, 368.0, 369.0, 370.0, 371.0, 372.0, 373.0, 374.0, 375.0, 376.0, 377.0, 378.0, 379.0, 380.0, 381.0, 382.0, 383.0, 384.0, 385.0, 386.0, 387.0, 388.0, 389.0, 390.0, 391.0, 392.0, 393.0, 394.0, 395.0, 396.0, 397.0, 398.0, 399.0, 400.0, 401.0, 402.0, 403.0, 404.0, 405.0, 406.0, 407.0, 408.0, 409.0, 410.0, 411.0, 412.0, 413.0, 414.0, 415.0, 416.0, 417.0, 418.0, 419.0, 420.0, 421.0, 422.0, 423.0, 424.0, 425.0, 426.0, 427.0, 428.0, 429.0, 430.0, 431.0, 432.0, 433.0, 434.0, 435.0, 436.0, 437.0, 438.0, 439.0, 440.0, 441.0, 442.0, 443.0, 444.0, 445.0, 446.0, 447.0, 448.0, 449.0, 450.0, 451.0, 452.0, 453.0, 454.0, 455.0, 456.0, 457.0, 458.0, 459.0, 460.0, 461.0, 462.0, 463.0, 464.0, 465.0, 466.0, 467.0, 468.0, 469.0, 470.0, 471.0, 472.0, 473.0, 474.0, 475.0, 476.0, 477.0, 478.0, 479.0, 480.0, 481.0, 482.0, 483.0, 484.0, 485.0, 486.0, 487.0, 488.0, 489.0, 490.0, 491.0, 492.0, 493.0, 494.0, 495.0, 496.0, 497.0, 498.0, 499.0], \"y\": [1.0827071181624002, 0.826804540827575, 0.7647625684225705, 0.7153202833707653, 0.6990922616747106, 0.6257567178592176, 0.6472473064417914, 0.592831651354484, 0.6363739048430962, 0.6221133922784523, 0.5574813998747119, 0.5545042233199229, 0.5487422901058128, 0.5716869736970941, 0.597499520596726, 0.515208778396197, 0.6191877503947456, 0.5322004043497561, 0.5981609809058734, 0.6045187448198195, 0.6082905404308121, 0.6043651915665638, 0.615717259040008, 0.6486895613337443, 0.5950852625892372, 0.5928640835035098, 0.5370026653458939, 0.5252761163133142, 0.5244056691297877, 0.5633679202021807, 0.544769076218065, 0.5373657907540161, 0.5469401904834735, 0.593495327660761, 0.5733825374624161, 0.5382109288396915, 0.4606743805682837, 0.5685111067620462, 0.5214156829799306, 0.5628809407657434, 0.6264428639527784, 0.5608648239933978, 0.5524150953309045, 0.6188312609778013, 0.5969494936814912, 0.5283121573698737, 0.6722449425496445, 0.5617567094689573, 0.5306422428926405, 0.5761207943986053, 0.5347641544350911, 0.5566849161091533, 0.5997108621144853, 0.6583961059841215, 0.7873779527419873, 0.502501626216347, 0.714419646385718, 0.6171258269614761, 0.5187731849634077, 0.5427856727505257, 0.46851792704915096, 0.5994944873927814, 0.6075700510948343, 0.49711842227752195, 0.543687282807135, 0.5793312709726803, 0.5435730652808035, 0.483049765194835, 0.5510247993462433, 0.5768243988073618, 0.6156128805939229, 0.6110981467022679, 0.6891967147297247, 0.6111134192786142, 0.5265286881486643, 0.5397517702176394, 0.6128750415596544, 0.5970952192244172, 0.6195435507772117, 0.4948434821885964, 0.5158064459333255, 0.6798512922059736, 0.5419572620297598, 0.7079646762801699, 0.49477473479229817, 0.7061870622357116, 0.5394990558164332, 0.6111811393446774, 0.6017596020321093, 0.5554407874620552, 0.5435202972114537, 0.6236109372990832, 0.5346724537812919, 0.4907662480130258, 0.6750371221591196, 0.6108940638929945, 0.6864884112251881, 0.52680967373011, 0.7090651449594864, 0.5190671213638802, 0.47106418260278193, 0.6704578625431471, 0.6560398317592099, 0.5567752158083785, 0.5490094340970957, 0.5338182171824495, 0.48598274051046036, 0.5154723444066337, 0.5166099328256045, 0.557787674916951, 0.5501184147612832, 0.6650147071282768, 0.5506883008590376, 1.1065975807056585, 0.5131294781948684, 0.6146009622039519, 0.6663177547657299, 0.5048789496125287, 0.4888546431135081, 0.5113755146017355, 0.5545091324338534, 0.5040003722553176, 0.7751564222643819, 0.5266558499923311, 0.782665792967037, 0.5498569750603521, 0.5216812883511727, 0.6796432004579263, 0.5457550420423571, 0.5534469168702427, 0.5372157056129503, 0.5979007627893514, 0.4756411762504866, 0.42239218596134964, 0.6044937192073457, 0.7430136109817612, 0.5359444216731211, 0.5099883048144106, 0.5407888778018002, 0.5568351534419581, 0.4899505438276742, 0.5463758176016026, 0.4677644764805543, 0.4952349582840796, 0.7404839213121742, 0.6308641092128826, 0.5547440734564113, 0.5693026771298922, 0.47006514454747, 0.5870129387259957, 0.5735409466355882, 0.5507334580240324, 0.5759600137600568, 0.496569250199449, 0.49138311712971083, 0.5759375930162373, 0.4652080246629663, 0.566663118437795, 0.5703911072662614, 0.43178924214989145, 0.604361324550265, 0.6033523890054628, 0.5025798716380067, 0.46566708407205193, 0.6267877935450227, 0.5838452348257257, 0.4881241578132155, 0.6182512585076224, 0.571799755426043, 0.5468259753249688, 0.5934657925175139, 0.5595814475361292, 0.5359502359345372, 0.5769694393212474, 0.653267386472472, 0.578018823745502, 0.5003418925402341, 0.6056255838826318, 0.42869943015162526, 0.559391418902847, 0.6082678844213967, 0.5744762131673649, 0.5701600219823789, 0.43348122776356585, 0.6654148639099703, 0.4468545692163674, 0.4532938821404492, 0.7330793323302076, 0.6370234912046528, 0.5192392104346817, 0.6542651517322279, 0.569267344119249, 0.5213647959985594, 0.664562569727177, 0.563987191488066, 0.5878738626008428, 0.6242226608194973, 0.5549931513758162, 0.7329364611827192, 0.5032579716886514], \"type\": \"scatter\", \"uid\": \"78b37b5c-d34d-11e8-bf88-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: L2, lmbda:3.0, drop:0.5\", \"x\": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 251.0, 252.0, 253.0, 254.0, 255.0, 256.0, 257.0, 258.0, 259.0, 260.0, 261.0, 262.0, 263.0, 264.0, 265.0, 266.0, 267.0, 268.0, 269.0, 270.0, 271.0, 272.0, 273.0, 274.0, 275.0, 276.0, 277.0, 278.0, 279.0, 280.0, 281.0, 282.0, 283.0, 284.0, 285.0, 286.0, 287.0, 288.0, 289.0, 290.0, 291.0, 292.0, 293.0, 294.0, 295.0, 296.0, 297.0, 298.0, 299.0, 300.0, 301.0, 302.0, 303.0, 304.0, 305.0, 306.0, 307.0, 308.0, 309.0, 310.0, 311.0, 312.0, 313.0, 314.0, 315.0, 316.0, 317.0, 318.0, 319.0, 320.0, 321.0, 322.0, 323.0, 324.0, 325.0, 326.0, 327.0, 328.0, 329.0, 330.0, 331.0, 332.0, 333.0, 334.0, 335.0, 336.0, 337.0, 338.0, 339.0, 340.0, 341.0, 342.0, 343.0, 344.0, 345.0, 346.0, 347.0, 348.0, 349.0, 350.0, 351.0, 352.0, 353.0, 354.0, 355.0, 356.0, 357.0, 358.0, 359.0, 360.0, 361.0, 362.0, 363.0, 364.0, 365.0, 366.0, 367.0, 368.0, 369.0, 370.0, 371.0, 372.0, 373.0, 374.0, 375.0, 376.0, 377.0, 378.0, 379.0, 380.0, 381.0, 382.0, 383.0, 384.0, 385.0, 386.0, 387.0, 388.0, 389.0, 390.0, 391.0, 392.0, 393.0, 394.0, 395.0, 396.0, 397.0, 398.0, 399.0, 400.0, 401.0, 402.0, 403.0, 404.0, 405.0, 406.0, 407.0, 408.0, 409.0, 410.0, 411.0, 412.0, 413.0, 414.0, 415.0, 416.0, 417.0, 418.0, 419.0, 420.0, 421.0, 422.0, 423.0, 424.0, 425.0, 426.0, 427.0, 428.0, 429.0, 430.0, 431.0, 432.0, 433.0, 434.0, 435.0, 436.0, 437.0, 438.0, 439.0, 440.0, 441.0, 442.0, 443.0, 444.0, 445.0, 446.0, 447.0, 448.0, 449.0, 450.0, 451.0, 452.0, 453.0, 454.0, 455.0, 456.0, 457.0, 458.0, 459.0, 460.0, 461.0, 462.0, 463.0, 464.0, 465.0, 466.0, 467.0, 468.0, 469.0, 470.0, 471.0, 472.0, 473.0, 474.0, 475.0, 476.0, 477.0, 478.0, 479.0, 480.0, 481.0, 482.0, 483.0, 484.0, 485.0, 486.0, 487.0, 488.0, 489.0, 490.0, 491.0, 492.0, 493.0, 494.0, 495.0, 496.0, 497.0, 498.0, 499.0], \"y\": [1.5405845877050528, 1.387194054010197, 1.323340292901742, 1.2311248762537241, 1.205818531779822, 1.1731832101048956, 1.0606462688950524, 1.051570003242763, 1.008862110757464, 0.9160028710076816, 0.9006050951465523, 0.7685479371826365, 0.7512198303145976, 0.7305642888536376, 0.7240028513993721, 0.7849598147525217, 0.7416759591128589, 0.6893626634709996, 0.70780542414584, 0.6650484919561819, 0.65030972107357, 0.6646947757206649, 0.7004528797046334, 0.6435282040950843, 0.6580383275128397, 0.6170513420934524, 0.730225575328911, 0.621757711283268, 0.61402352849455, 0.6313573359492465, 0.6423428729371126, 0.6655345375155576, 0.6376791713792969, 0.6270721470337844, 0.6877010901786285, 0.6523040245856222, 0.6435086134122379, 0.6712251207091811, 0.6270365641482922, 0.7109590894587206, 0.6642243597846627, 0.6768157497147129, 0.646178738709692, 0.6734518557926036, 0.632454532038847, 0.7740813856855139, 0.672665972671417, 0.6641406033059176, 0.624591439715721, 0.732075167943322, 0.6822298530553929, 0.6968889577692647, 0.7744053507501513, 0.6545546982924351, 0.6565597505963319, 0.6363664769561815, 0.6332483344463856, 0.6264068993102443, 0.6644126100508222, 0.6617683345021023, 0.7891918392836264, 0.6898415799677412, 0.7132856362004706, 0.6608732065781896, 0.6422257858900806, 0.6562689420619221, 0.6402949795581163, 0.6813568440048958, 0.6809151413920741, 0.6582941195163543, 0.6201293508989545, 0.725571729702972, 0.6483207437027514, 0.6244908974075357, 0.6268547219371476, 0.6321349247250273, 0.6230223008592845, 0.6186626477966848, 0.6189789412746977, 0.6097160219209902, 0.6389124445900259, 0.6194494486271207, 0.6296443684885928, 0.6467481814245709, 0.6965856405438512, 0.6640332362939279, 0.6515565989075534, 0.6326675401187821, 0.6342226137346072, 0.6446380627474799, 0.6913226417383374, 0.6442825825670144, 0.6190701416989212, 0.6500471442208211, 0.622678508481102, 0.670458050584544, 0.6295186589891046, 0.6123558349371463, 0.7030579734672713, 0.7202717374109671, 0.6390025270097992, 0.6288470792936609, 0.6447573134140872, 0.6484652071986792, 0.647348569264318, 0.6483817015965456, 0.7148484153961253, 0.6669662171533604, 0.6428926939820843, 0.6511535599359007, 0.6727791347208532, 0.6389388744887085, 0.6687005210479859, 0.6179566308713976, 0.6162067745005545, 0.6149443844788032, 0.649624294781625, 0.6039140521960088, 0.6182079992299924, 0.6202455882996767, 0.6589341035939289, 0.6206711946624842, 0.6548523369823405, 0.6303132448987047, 0.6388157062210298, 0.6207682456448316, 0.6169603597602081, 0.6259090817744174, 0.6334080254130038, 0.6419187963964, 0.6445124800793618, 0.653105694770483, 0.642501049571716, 0.6505625317441076, 0.6521579359991876, 0.6573946115994744, 0.6325851361789714, 0.6712012433224124, 0.6523934508582353, 0.6513676559078349, 1.2330104387452863, 0.7863267682428389, 0.6606274122921906, 0.6478379703479135, 0.6305029377776937, 0.6271652751804313, 0.6445002910882222, 0.6279296334977281, 0.6197740070475007, 0.6865363696776465, 0.6505568886814642, 0.7165282234613412, 0.6864956674290903, 1.0597477025810405, 0.708993957658967, 0.6859646310844356, 0.6480366107376148, 0.6432412319774602, 0.6369431543608022, 0.6674798976699052, 0.6375706474051028, 0.624848586453425, 0.6378000643298807, 0.6900337044598505, 0.6287472949908623, 0.624818877743851, 0.6276544229700212, 0.6293066751779871, 0.6235690348576562, 0.6322784504645897, 0.6247912451466429, 0.6160663442570534, 0.7439462709320023, 0.6639858172832199, 0.6638496317242031, 0.7124562952344203, 0.8358905217518495, 0.6790237385970612, 0.6616788320669507, 0.7112163170743834, 0.6327187555837265, 0.6952766710576033, 0.649755489494994, 0.6582592990363325, 0.6218615960711181, 0.66148921831528, 0.6816080201868516, 0.6366280599936233, 0.6792198252421104, 0.6391754616322968, 0.6457433477034143, 0.6917196265575761, 0.6376165924412457, 0.6495095540912614, 0.6801518475265572, 0.6410685251242861, 0.652776246240648, 0.6255131391308184, 0.6164546235445142, 0.6351883807665093], \"type\": \"scatter\", \"uid\": \"78b37c2e-d34d-11e8-8fe7-a820660c6deb\"}],\n",
       "            {\"title\": \"Cost: hidden: ReLU / Output: Softmax / Network: 4-20-7-3 (Train: 95 instances)\", \"xaxis\": {\"title\": \"Epoch\"}, \"yaxis\": {\"title\": \"Cost\"}},\n",
       "            {\"showLink\": true, \"linkText\": \"Export to plot.ly\"}\n",
       "        ).then(function () {return Plotly.addFrames('1a55fb75-76f1-45d2-9367-5cc7d70cb1ed',{});}).then(function(){Plotly.animate('1a55fb75-76f1-45d2-9367-5cc7d70cb1ed');})\n",
       "        });</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"1a55fb75-76f1-45d2-9367-5cc7d70cb1ed\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "        Plotly.plot(\n",
       "            '1a55fb75-76f1-45d2-9367-5cc7d70cb1ed',\n",
       "            [{\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: Default, drop:0.1\", \"x\": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 251.0, 252.0, 253.0, 254.0, 255.0, 256.0, 257.0, 258.0, 259.0, 260.0, 261.0, 262.0, 263.0, 264.0, 265.0, 266.0, 267.0, 268.0, 269.0, 270.0, 271.0, 272.0, 273.0, 274.0, 275.0, 276.0, 277.0, 278.0, 279.0, 280.0, 281.0, 282.0, 283.0, 284.0, 285.0, 286.0, 287.0, 288.0, 289.0, 290.0, 291.0, 292.0, 293.0, 294.0, 295.0, 296.0, 297.0, 298.0, 299.0, 300.0, 301.0, 302.0, 303.0, 304.0, 305.0, 306.0, 307.0, 308.0, 309.0, 310.0, 311.0, 312.0, 313.0, 314.0, 315.0, 316.0, 317.0, 318.0, 319.0, 320.0, 321.0, 322.0, 323.0, 324.0, 325.0, 326.0, 327.0, 328.0, 329.0, 330.0, 331.0, 332.0, 333.0, 334.0, 335.0, 336.0, 337.0, 338.0, 339.0, 340.0, 341.0, 342.0, 343.0, 344.0, 345.0, 346.0, 347.0, 348.0, 349.0, 350.0, 351.0, 352.0, 353.0, 354.0, 355.0, 356.0, 357.0, 358.0, 359.0, 360.0, 361.0, 362.0, 363.0, 364.0, 365.0, 366.0, 367.0, 368.0, 369.0, 370.0, 371.0, 372.0, 373.0, 374.0, 375.0, 376.0, 377.0, 378.0, 379.0, 380.0, 381.0, 382.0, 383.0, 384.0, 385.0, 386.0, 387.0, 388.0, 389.0, 390.0, 391.0, 392.0, 393.0, 394.0, 395.0, 396.0, 397.0, 398.0, 399.0, 400.0, 401.0, 402.0, 403.0, 404.0, 405.0, 406.0, 407.0, 408.0, 409.0, 410.0, 411.0, 412.0, 413.0, 414.0, 415.0, 416.0, 417.0, 418.0, 419.0, 420.0, 421.0, 422.0, 423.0, 424.0, 425.0, 426.0, 427.0, 428.0, 429.0, 430.0, 431.0, 432.0, 433.0, 434.0, 435.0, 436.0, 437.0, 438.0, 439.0, 440.0, 441.0, 442.0, 443.0, 444.0, 445.0, 446.0, 447.0, 448.0, 449.0, 450.0, 451.0, 452.0, 453.0, 454.0, 455.0, 456.0, 457.0, 458.0, 459.0, 460.0, 461.0, 462.0, 463.0, 464.0, 465.0, 466.0, 467.0, 468.0, 469.0, 470.0, 471.0, 472.0, 473.0, 474.0, 475.0, 476.0, 477.0, 478.0, 479.0, 480.0, 481.0, 482.0, 483.0, 484.0, 485.0, 486.0, 487.0, 488.0, 489.0, 490.0, 491.0, 492.0, 493.0, 494.0, 495.0, 496.0, 497.0, 498.0, 499.0], \"y\": [0.7164908656559965, 0.4622341006614181, 0.46917873348657074, 0.4697690494586621, 0.49739563805671505, 0.4315818691239112, 0.4337622886123998, 0.38596244587540984, 0.42570316520700596, 0.2974264932909909, 0.35253845329622696, 0.35619648724113057, 0.2710454270435016, 0.32140455659657124, 0.3742665368333511, 0.25080542314606175, 0.38196588509580065, 0.29718250744986996, 0.30155054061296593, 0.2677209851194511, 0.5572495528069465, 0.5058145999038826, 0.4885488239783015, 0.4831317385066748, 0.47954295471655356, 0.4779945090452997, 0.476914110181013, 0.4752594562993959, 0.47394873563394413, 0.4725939707260362, 0.4732396917922601, 0.4725803984233273, 0.4715187875206039, 0.471637277176977, 0.4707012770465192, 0.47068075059415243, 0.46981441115356937, 0.4698102754963191, 0.4697951379530958, 0.4690476466133142, 0.4683995698560495, 0.46847471857349227, 0.46792373851558305, 0.46743996831847473, 0.4669846244109398, 0.4671246916974736, 0.46669858935797176, 0.4668454410151359, 0.4664517444353516, 0.46608633750514117, 0.46634956432276736, 0.46601712179775634, 0.46570090632170624, 0.4657761171953198, 0.4655472097653393, 0.46526708639403636, 0.46544733451148357, 0.4651736955085655, 0.46492357616251245, 0.46510190919247035, 0.4648638051519727, 0.46463231377921616, 0.46441997743645497, 0.4646998253612009, 0.46450596973088065, 0.46431018526986617, 0.46412162228702597, 0.4639494709619405, 0.4638272122447828, 0.463673000781709, 0.4635257707216055, 0.4636685825220047, 0.4635694585715622, 0.46343859786550917, 0.4617534245687541, 0.4616147687936419, 1.8723620979900741, 0.4820832507146503, 0.4791533486677235, 0.4780271914387797, 0.4755846341608667, 0.4737067099387562, 0.4547532470057133, 0.37147263810031517, 0.385113442008069, 0.47173470440739135, 0.37145750938333505, 0.36742037141949757, 0.46631270221304005, 0.43422092319965994, 0.24773822813649568, 0.4974168126945388, 0.367444301193115, 0.23481457547934434, 0.44869850428527647, 0.3942306944677563, 0.37234021747542584, 0.33682784948715133, 0.29317220744438227, 0.35016766991649756, 0.29876024535528883, 0.19634290347161987, 0.27762788493745977, 0.26044057483155686, 0.19301747275913983, 0.33977750105458465, 0.22818273647392714, 0.34996534162432386, 0.352369746047956, 0.29150431335282345, 0.3931645888570352, 0.3171112860755622, 0.24830922668169256, 0.3579918819235349, 0.5132885713679756, 0.37870248160161385, 0.2808233889516715, 0.2932556895613408, 0.14630852266693029, 0.3536521650968762, 0.31075026186292265, 0.2996245758648527, 0.22479804217133448, 0.25939239957466204, 0.17667490138784495, 0.24959305895008166, 0.24772715770434584, 0.20400218663186445, 0.28779879642178985, 0.18349238442053561, 0.15053316417303195, 0.3043833282513741, 0.19690621344549641, 0.1590753483803434, 0.22588918031789088, 0.16265484170254427, 0.1032281538643379, 0.21888442699526264, 0.23074542526381409, 0.295730938458594, 0.19290285011536087, 0.29143386692776724, 0.2700043443039865, 0.2794775444902732, 0.18736288957913172, 0.3709518954805212, 0.2947718747607091, 0.27315649429921884, 0.193810386096638, 0.17854533610159634, 0.14978452675033752, 0.22930759879287627, 0.1683279422790566, 0.13730822471801507, 0.15192172825140757, 0.17124841064055107, 0.0925569163475596, 0.1373347718686275, 0.181884579939211, 0.21444457453646693, 0.17626488697239986, 0.1636801053603809, 0.15673404282421055, 0.1088521452377157, 0.12103159836889618, 0.1373503342129476, 0.30104930709212907, 0.18281016796938743, 0.2130354540410634, 0.1287468994436223, 0.29912003564487316, 0.12054977203655601, 0.14147800923537351, 0.21235635844616266, 0.09152529468636407, 0.7101372949195631, 0.17295400176663356, 0.16587426866975513, 0.1949611404818274, 0.18482144539364748, 0.45090574434665537, 0.08958039012752726, 0.249053814732886, 0.3378400491415598, 0.11163592585479833, 0.10423722679811123, 0.1575058440221353, 0.3259257407677643, 0.24510299526291093, 0.1527373662058138, 0.15242926229432538, 0.21850544244235998, 0.17836415160045657, 0.1401045216562318, 0.10348405645640216, 0.17259922620217985, 0.18066709037530918, 0.08772878935622289, 0.25651565703277573, 0.21773404499120436], \"type\": \"scatter\", \"uid\": \"78b378b4-d34d-11e8-91d4-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: Default, drop:0.5\", \"x\": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 251.0, 252.0, 253.0, 254.0, 255.0, 256.0, 257.0, 258.0, 259.0, 260.0, 261.0, 262.0, 263.0, 264.0, 265.0, 266.0, 267.0, 268.0, 269.0, 270.0, 271.0, 272.0, 273.0, 274.0, 275.0, 276.0, 277.0, 278.0, 279.0, 280.0, 281.0, 282.0, 283.0, 284.0, 285.0, 286.0, 287.0, 288.0, 289.0, 290.0, 291.0, 292.0, 293.0, 294.0, 295.0, 296.0, 297.0, 298.0, 299.0, 300.0, 301.0, 302.0, 303.0, 304.0, 305.0, 306.0, 307.0, 308.0, 309.0, 310.0, 311.0, 312.0, 313.0, 314.0, 315.0, 316.0, 317.0, 318.0, 319.0, 320.0, 321.0, 322.0, 323.0, 324.0, 325.0, 326.0, 327.0, 328.0, 329.0, 330.0, 331.0, 332.0, 333.0, 334.0, 335.0, 336.0, 337.0, 338.0, 339.0, 340.0, 341.0, 342.0, 343.0, 344.0, 345.0, 346.0, 347.0, 348.0, 349.0, 350.0, 351.0, 352.0, 353.0, 354.0, 355.0, 356.0, 357.0, 358.0, 359.0, 360.0, 361.0, 362.0, 363.0, 364.0, 365.0, 366.0, 367.0, 368.0, 369.0, 370.0, 371.0, 372.0, 373.0, 374.0, 375.0, 376.0, 377.0, 378.0, 379.0, 380.0, 381.0, 382.0, 383.0, 384.0, 385.0, 386.0, 387.0, 388.0, 389.0, 390.0, 391.0, 392.0, 393.0, 394.0, 395.0, 396.0, 397.0, 398.0, 399.0, 400.0, 401.0, 402.0, 403.0, 404.0, 405.0, 406.0, 407.0, 408.0, 409.0, 410.0, 411.0, 412.0, 413.0, 414.0, 415.0, 416.0, 417.0, 418.0, 419.0, 420.0, 421.0, 422.0, 423.0, 424.0, 425.0, 426.0, 427.0, 428.0, 429.0, 430.0, 431.0, 432.0, 433.0, 434.0, 435.0, 436.0, 437.0, 438.0, 439.0, 440.0, 441.0, 442.0, 443.0, 444.0, 445.0, 446.0, 447.0, 448.0, 449.0, 450.0, 451.0, 452.0, 453.0, 454.0, 455.0, 456.0, 457.0, 458.0, 459.0, 460.0, 461.0, 462.0, 463.0, 464.0, 465.0, 466.0, 467.0, 468.0, 469.0, 470.0, 471.0, 472.0, 473.0, 474.0, 475.0, 476.0, 477.0, 478.0, 479.0, 480.0, 481.0, 482.0, 483.0, 484.0, 485.0, 486.0, 487.0, 488.0, 489.0, 490.0, 491.0, 492.0, 493.0, 494.0, 495.0, 496.0, 497.0, 498.0, 499.0], \"y\": [1.1629585567984275, 1.1097333394900661, 1.1006733710386802, 1.0980600871259705, 1.098328998314477, 1.0987950863863687, 1.098727087051333, 1.1004312500835485, 1.0989992155323531, 1.0986117525614607, 1.0980019157490715, 1.0983562568523417, 1.0996112197668364, 1.1021395944113368, 1.0632689073643629, 0.8080247999508664, 0.8246603403293616, 0.6929984274934152, 0.8492300414544807, 0.6687621859615464, 0.6406120786078874, 0.6253050897522108, 0.6012346782159245, 0.5564411606672567, 0.5399277942956558, 0.539630830956343, 0.5421933929398896, 0.49665422280376403, 0.5181397729965925, 0.4780716991569214, 0.43211549216302936, 0.5240198241968946, 0.5269971707494066, 0.5109061754215813, 0.45654062187389366, 0.47318699847827683, 0.4231202435451687, 0.47550759111147756, 0.46305748376062467, 0.5129579362838796, 0.4502518200920423, 0.4830072790104559, 0.43053643134339636, 0.5124255732136886, 0.4715483073759084, 0.5205337571318357, 0.4222334394696623, 0.46534569142964644, 0.452448666749315, 0.3665300995513667, 0.501104542805517, 0.5116979801138427, 0.48386152202577537, 0.35855501568011117, 0.3834579312139333, 0.4791673952641294, 0.4227131817120184, 0.38763864998913206, 0.43892171933057295, 0.42652957613064385, 0.4782199568103582, 0.3535575050671542, 0.6067161283832935, 0.5130181200570765, 0.4873274312384093, 0.4305047137320457, 0.435205368178903, 0.3600692818999096, 0.4795168126476447, 0.4581786093840343, 0.44460245251533315, 0.4780632520013595, 0.4434102181118611, 0.4548077504478311, 0.4230291216247516, 0.43834327921927624, 0.5105488400627284, 0.48186845621987, 0.4933216985759445, 0.4852708964397552, 0.43432984887434767, 0.45760237169541423, 0.48259992300870774, 0.4315926109636781, 0.39867676469717117, 0.4103043028851312, 0.4973475083412541, 0.43825961234491223, 0.4061201648549185, 0.46146346155230095, 0.406857047273826, 0.43138882109178184, 0.4431525188665447, 0.5304390271534536, 0.4796037272667612, 0.4177210383733021, 0.4305851189360443, 0.417398768155259, 0.4197969567500033, 0.43505118494648337, 0.38103519573832534, 0.39766597827398814, 0.41397222159946145, 0.49406012142426364, 0.5237337716030764, 0.4078274784450705, 0.36636625841053183, 0.3848913800485732, 0.47584554563152615, 0.4814802554842633, 0.42323824219520584, 0.37500201445950176, 0.4228394361112547, 0.5416645538699997, 0.42462304331660583, 0.36719834765951853, 0.4128366780714473, 0.4939169944332245, 0.4599535210724015, 0.431972174125415, 0.5425786207429432, 0.42314167159183974, 0.36578374030681826, 0.35249750848905276, 0.38919428101573855, 0.46569261807893747, 0.35135852083385677, 0.40035952057648305, 0.3576403725431176, 0.37188434898863787, 0.4135124715407818, 0.4901837235131784, 0.4040726733073337, 0.33826106841738246, 0.4359912547275019, 0.4525963610387573, 0.37373819905184985, 0.35562300138501046, 0.39979308914167583, 0.38278480973230544, 0.42170120389339305, 0.5315198667239183, 0.554240634701162, 0.34912342129816254, 0.33108979257202065, 0.5228985701653293, 0.5157400138744707, 0.46359302005909775, 0.31120734162330743, 0.3863039368996235, 0.43019164734873155, 0.3015121987966179, 0.5241585005924155, 0.49143339359057164, 0.4974547693422135, 0.5027554827695566, 0.5218962370880935, 0.4903360976557699, 0.5018217986196556, 0.5558823166237193, 0.5118230906966439, 0.4988621510716999, 0.5051513630964842, 0.4941151818868642, 0.49524649564189316, 0.5073965553902734, 0.4901344213051896, 0.4835377433650909, 0.4881325038770157, 0.4806009398527655, 0.502010971774973, 0.5436257686869581, 0.5752614131538251, 0.5456394364602671, 0.5259624391312068, 0.5001871304672227, 0.5019819375517008, 0.5155966538313275, 0.48330498879446526, 0.48899240062777205, 0.5246545852852189, 0.4878998521777327, 0.505392735730466, 0.492265728515503, 0.48441075965125774, 0.49386771934866286, 0.48366442795423636, 0.49622482900629455, 0.48416144145320766, 0.4776684702594863, 0.4962183435230193, 0.49541749398276713, 0.48557321068186027, 0.4760325015052019, 0.48420565538112, 0.5043855062920147, 0.48802182769899727, 0.5063793021026033, 0.51478507914588, 0.501052070390548], \"type\": \"scatter\", \"uid\": \"78b37a6c-d34d-11e8-a89a-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: L2, lmbda:3.0, drop:0.1\", \"x\": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 251.0, 252.0, 253.0, 254.0, 255.0, 256.0, 257.0, 258.0, 259.0, 260.0, 261.0, 262.0, 263.0, 264.0, 265.0, 266.0, 267.0, 268.0, 269.0, 270.0, 271.0, 272.0, 273.0, 274.0, 275.0, 276.0, 277.0, 278.0, 279.0, 280.0, 281.0, 282.0, 283.0, 284.0, 285.0, 286.0, 287.0, 288.0, 289.0, 290.0, 291.0, 292.0, 293.0, 294.0, 295.0, 296.0, 297.0, 298.0, 299.0, 300.0, 301.0, 302.0, 303.0, 304.0, 305.0, 306.0, 307.0, 308.0, 309.0, 310.0, 311.0, 312.0, 313.0, 314.0, 315.0, 316.0, 317.0, 318.0, 319.0, 320.0, 321.0, 322.0, 323.0, 324.0, 325.0, 326.0, 327.0, 328.0, 329.0, 330.0, 331.0, 332.0, 333.0, 334.0, 335.0, 336.0, 337.0, 338.0, 339.0, 340.0, 341.0, 342.0, 343.0, 344.0, 345.0, 346.0, 347.0, 348.0, 349.0, 350.0, 351.0, 352.0, 353.0, 354.0, 355.0, 356.0, 357.0, 358.0, 359.0, 360.0, 361.0, 362.0, 363.0, 364.0, 365.0, 366.0, 367.0, 368.0, 369.0, 370.0, 371.0, 372.0, 373.0, 374.0, 375.0, 376.0, 377.0, 378.0, 379.0, 380.0, 381.0, 382.0, 383.0, 384.0, 385.0, 386.0, 387.0, 388.0, 389.0, 390.0, 391.0, 392.0, 393.0, 394.0, 395.0, 396.0, 397.0, 398.0, 399.0, 400.0, 401.0, 402.0, 403.0, 404.0, 405.0, 406.0, 407.0, 408.0, 409.0, 410.0, 411.0, 412.0, 413.0, 414.0, 415.0, 416.0, 417.0, 418.0, 419.0, 420.0, 421.0, 422.0, 423.0, 424.0, 425.0, 426.0, 427.0, 428.0, 429.0, 430.0, 431.0, 432.0, 433.0, 434.0, 435.0, 436.0, 437.0, 438.0, 439.0, 440.0, 441.0, 442.0, 443.0, 444.0, 445.0, 446.0, 447.0, 448.0, 449.0, 450.0, 451.0, 452.0, 453.0, 454.0, 455.0, 456.0, 457.0, 458.0, 459.0, 460.0, 461.0, 462.0, 463.0, 464.0, 465.0, 466.0, 467.0, 468.0, 469.0, 470.0, 471.0, 472.0, 473.0, 474.0, 475.0, 476.0, 477.0, 478.0, 479.0, 480.0, 481.0, 482.0, 483.0, 484.0, 485.0, 486.0, 487.0, 488.0, 489.0, 490.0, 491.0, 492.0, 493.0, 494.0, 495.0, 496.0, 497.0, 498.0, 499.0], \"y\": [1.0827071181624002, 0.826804540827575, 0.7647625684225705, 0.7153202833707653, 0.6990922616747106, 0.6257567178592176, 0.6472473064417914, 0.592831651354484, 0.6363739048430962, 0.6221133922784523, 0.5574813998747119, 0.5545042233199229, 0.5487422901058128, 0.5716869736970941, 0.597499520596726, 0.515208778396197, 0.6191877503947456, 0.5322004043497561, 0.5981609809058734, 0.6045187448198195, 0.6082905404308121, 0.6043651915665638, 0.615717259040008, 0.6486895613337443, 0.5950852625892372, 0.5928640835035098, 0.5370026653458939, 0.5252761163133142, 0.5244056691297877, 0.5633679202021807, 0.544769076218065, 0.5373657907540161, 0.5469401904834735, 0.593495327660761, 0.5733825374624161, 0.5382109288396915, 0.4606743805682837, 0.5685111067620462, 0.5214156829799306, 0.5628809407657434, 0.6264428639527784, 0.5608648239933978, 0.5524150953309045, 0.6188312609778013, 0.5969494936814912, 0.5283121573698737, 0.6722449425496445, 0.5617567094689573, 0.5306422428926405, 0.5761207943986053, 0.5347641544350911, 0.5566849161091533, 0.5997108621144853, 0.6583961059841215, 0.7873779527419873, 0.502501626216347, 0.714419646385718, 0.6171258269614761, 0.5187731849634077, 0.5427856727505257, 0.46851792704915096, 0.5994944873927814, 0.6075700510948343, 0.49711842227752195, 0.543687282807135, 0.5793312709726803, 0.5435730652808035, 0.483049765194835, 0.5510247993462433, 0.5768243988073618, 0.6156128805939229, 0.6110981467022679, 0.6891967147297247, 0.6111134192786142, 0.5265286881486643, 0.5397517702176394, 0.6128750415596544, 0.5970952192244172, 0.6195435507772117, 0.4948434821885964, 0.5158064459333255, 0.6798512922059736, 0.5419572620297598, 0.7079646762801699, 0.49477473479229817, 0.7061870622357116, 0.5394990558164332, 0.6111811393446774, 0.6017596020321093, 0.5554407874620552, 0.5435202972114537, 0.6236109372990832, 0.5346724537812919, 0.4907662480130258, 0.6750371221591196, 0.6108940638929945, 0.6864884112251881, 0.52680967373011, 0.7090651449594864, 0.5190671213638802, 0.47106418260278193, 0.6704578625431471, 0.6560398317592099, 0.5567752158083785, 0.5490094340970957, 0.5338182171824495, 0.48598274051046036, 0.5154723444066337, 0.5166099328256045, 0.557787674916951, 0.5501184147612832, 0.6650147071282768, 0.5506883008590376, 1.1065975807056585, 0.5131294781948684, 0.6146009622039519, 0.6663177547657299, 0.5048789496125287, 0.4888546431135081, 0.5113755146017355, 0.5545091324338534, 0.5040003722553176, 0.7751564222643819, 0.5266558499923311, 0.782665792967037, 0.5498569750603521, 0.5216812883511727, 0.6796432004579263, 0.5457550420423571, 0.5534469168702427, 0.5372157056129503, 0.5979007627893514, 0.4756411762504866, 0.42239218596134964, 0.6044937192073457, 0.7430136109817612, 0.5359444216731211, 0.5099883048144106, 0.5407888778018002, 0.5568351534419581, 0.4899505438276742, 0.5463758176016026, 0.4677644764805543, 0.4952349582840796, 0.7404839213121742, 0.6308641092128826, 0.5547440734564113, 0.5693026771298922, 0.47006514454747, 0.5870129387259957, 0.5735409466355882, 0.5507334580240324, 0.5759600137600568, 0.496569250199449, 0.49138311712971083, 0.5759375930162373, 0.4652080246629663, 0.566663118437795, 0.5703911072662614, 0.43178924214989145, 0.604361324550265, 0.6033523890054628, 0.5025798716380067, 0.46566708407205193, 0.6267877935450227, 0.5838452348257257, 0.4881241578132155, 0.6182512585076224, 0.571799755426043, 0.5468259753249688, 0.5934657925175139, 0.5595814475361292, 0.5359502359345372, 0.5769694393212474, 0.653267386472472, 0.578018823745502, 0.5003418925402341, 0.6056255838826318, 0.42869943015162526, 0.559391418902847, 0.6082678844213967, 0.5744762131673649, 0.5701600219823789, 0.43348122776356585, 0.6654148639099703, 0.4468545692163674, 0.4532938821404492, 0.7330793323302076, 0.6370234912046528, 0.5192392104346817, 0.6542651517322279, 0.569267344119249, 0.5213647959985594, 0.664562569727177, 0.563987191488066, 0.5878738626008428, 0.6242226608194973, 0.5549931513758162, 0.7329364611827192, 0.5032579716886514], \"type\": \"scatter\", \"uid\": \"78b37b5c-d34d-11e8-bf88-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: L2, lmbda:3.0, drop:0.5\", \"x\": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 251.0, 252.0, 253.0, 254.0, 255.0, 256.0, 257.0, 258.0, 259.0, 260.0, 261.0, 262.0, 263.0, 264.0, 265.0, 266.0, 267.0, 268.0, 269.0, 270.0, 271.0, 272.0, 273.0, 274.0, 275.0, 276.0, 277.0, 278.0, 279.0, 280.0, 281.0, 282.0, 283.0, 284.0, 285.0, 286.0, 287.0, 288.0, 289.0, 290.0, 291.0, 292.0, 293.0, 294.0, 295.0, 296.0, 297.0, 298.0, 299.0, 300.0, 301.0, 302.0, 303.0, 304.0, 305.0, 306.0, 307.0, 308.0, 309.0, 310.0, 311.0, 312.0, 313.0, 314.0, 315.0, 316.0, 317.0, 318.0, 319.0, 320.0, 321.0, 322.0, 323.0, 324.0, 325.0, 326.0, 327.0, 328.0, 329.0, 330.0, 331.0, 332.0, 333.0, 334.0, 335.0, 336.0, 337.0, 338.0, 339.0, 340.0, 341.0, 342.0, 343.0, 344.0, 345.0, 346.0, 347.0, 348.0, 349.0, 350.0, 351.0, 352.0, 353.0, 354.0, 355.0, 356.0, 357.0, 358.0, 359.0, 360.0, 361.0, 362.0, 363.0, 364.0, 365.0, 366.0, 367.0, 368.0, 369.0, 370.0, 371.0, 372.0, 373.0, 374.0, 375.0, 376.0, 377.0, 378.0, 379.0, 380.0, 381.0, 382.0, 383.0, 384.0, 385.0, 386.0, 387.0, 388.0, 389.0, 390.0, 391.0, 392.0, 393.0, 394.0, 395.0, 396.0, 397.0, 398.0, 399.0, 400.0, 401.0, 402.0, 403.0, 404.0, 405.0, 406.0, 407.0, 408.0, 409.0, 410.0, 411.0, 412.0, 413.0, 414.0, 415.0, 416.0, 417.0, 418.0, 419.0, 420.0, 421.0, 422.0, 423.0, 424.0, 425.0, 426.0, 427.0, 428.0, 429.0, 430.0, 431.0, 432.0, 433.0, 434.0, 435.0, 436.0, 437.0, 438.0, 439.0, 440.0, 441.0, 442.0, 443.0, 444.0, 445.0, 446.0, 447.0, 448.0, 449.0, 450.0, 451.0, 452.0, 453.0, 454.0, 455.0, 456.0, 457.0, 458.0, 459.0, 460.0, 461.0, 462.0, 463.0, 464.0, 465.0, 466.0, 467.0, 468.0, 469.0, 470.0, 471.0, 472.0, 473.0, 474.0, 475.0, 476.0, 477.0, 478.0, 479.0, 480.0, 481.0, 482.0, 483.0, 484.0, 485.0, 486.0, 487.0, 488.0, 489.0, 490.0, 491.0, 492.0, 493.0, 494.0, 495.0, 496.0, 497.0, 498.0, 499.0], \"y\": [1.5405845877050528, 1.387194054010197, 1.323340292901742, 1.2311248762537241, 1.205818531779822, 1.1731832101048956, 1.0606462688950524, 1.051570003242763, 1.008862110757464, 0.9160028710076816, 0.9006050951465523, 0.7685479371826365, 0.7512198303145976, 0.7305642888536376, 0.7240028513993721, 0.7849598147525217, 0.7416759591128589, 0.6893626634709996, 0.70780542414584, 0.6650484919561819, 0.65030972107357, 0.6646947757206649, 0.7004528797046334, 0.6435282040950843, 0.6580383275128397, 0.6170513420934524, 0.730225575328911, 0.621757711283268, 0.61402352849455, 0.6313573359492465, 0.6423428729371126, 0.6655345375155576, 0.6376791713792969, 0.6270721470337844, 0.6877010901786285, 0.6523040245856222, 0.6435086134122379, 0.6712251207091811, 0.6270365641482922, 0.7109590894587206, 0.6642243597846627, 0.6768157497147129, 0.646178738709692, 0.6734518557926036, 0.632454532038847, 0.7740813856855139, 0.672665972671417, 0.6641406033059176, 0.624591439715721, 0.732075167943322, 0.6822298530553929, 0.6968889577692647, 0.7744053507501513, 0.6545546982924351, 0.6565597505963319, 0.6363664769561815, 0.6332483344463856, 0.6264068993102443, 0.6644126100508222, 0.6617683345021023, 0.7891918392836264, 0.6898415799677412, 0.7132856362004706, 0.6608732065781896, 0.6422257858900806, 0.6562689420619221, 0.6402949795581163, 0.6813568440048958, 0.6809151413920741, 0.6582941195163543, 0.6201293508989545, 0.725571729702972, 0.6483207437027514, 0.6244908974075357, 0.6268547219371476, 0.6321349247250273, 0.6230223008592845, 0.6186626477966848, 0.6189789412746977, 0.6097160219209902, 0.6389124445900259, 0.6194494486271207, 0.6296443684885928, 0.6467481814245709, 0.6965856405438512, 0.6640332362939279, 0.6515565989075534, 0.6326675401187821, 0.6342226137346072, 0.6446380627474799, 0.6913226417383374, 0.6442825825670144, 0.6190701416989212, 0.6500471442208211, 0.622678508481102, 0.670458050584544, 0.6295186589891046, 0.6123558349371463, 0.7030579734672713, 0.7202717374109671, 0.6390025270097992, 0.6288470792936609, 0.6447573134140872, 0.6484652071986792, 0.647348569264318, 0.6483817015965456, 0.7148484153961253, 0.6669662171533604, 0.6428926939820843, 0.6511535599359007, 0.6727791347208532, 0.6389388744887085, 0.6687005210479859, 0.6179566308713976, 0.6162067745005545, 0.6149443844788032, 0.649624294781625, 0.6039140521960088, 0.6182079992299924, 0.6202455882996767, 0.6589341035939289, 0.6206711946624842, 0.6548523369823405, 0.6303132448987047, 0.6388157062210298, 0.6207682456448316, 0.6169603597602081, 0.6259090817744174, 0.6334080254130038, 0.6419187963964, 0.6445124800793618, 0.653105694770483, 0.642501049571716, 0.6505625317441076, 0.6521579359991876, 0.6573946115994744, 0.6325851361789714, 0.6712012433224124, 0.6523934508582353, 0.6513676559078349, 1.2330104387452863, 0.7863267682428389, 0.6606274122921906, 0.6478379703479135, 0.6305029377776937, 0.6271652751804313, 0.6445002910882222, 0.6279296334977281, 0.6197740070475007, 0.6865363696776465, 0.6505568886814642, 0.7165282234613412, 0.6864956674290903, 1.0597477025810405, 0.708993957658967, 0.6859646310844356, 0.6480366107376148, 0.6432412319774602, 0.6369431543608022, 0.6674798976699052, 0.6375706474051028, 0.624848586453425, 0.6378000643298807, 0.6900337044598505, 0.6287472949908623, 0.624818877743851, 0.6276544229700212, 0.6293066751779871, 0.6235690348576562, 0.6322784504645897, 0.6247912451466429, 0.6160663442570534, 0.7439462709320023, 0.6639858172832199, 0.6638496317242031, 0.7124562952344203, 0.8358905217518495, 0.6790237385970612, 0.6616788320669507, 0.7112163170743834, 0.6327187555837265, 0.6952766710576033, 0.649755489494994, 0.6582592990363325, 0.6218615960711181, 0.66148921831528, 0.6816080201868516, 0.6366280599936233, 0.6792198252421104, 0.6391754616322968, 0.6457433477034143, 0.6917196265575761, 0.6376165924412457, 0.6495095540912614, 0.6801518475265572, 0.6410685251242861, 0.652776246240648, 0.6255131391308184, 0.6164546235445142, 0.6351883807665093], \"type\": \"scatter\", \"uid\": \"78b37c2e-d34d-11e8-8fe7-a820660c6deb\"}],\n",
       "            {\"title\": \"Cost: hidden: ReLU / Output: Softmax / Network: 4-20-7-3 (Train: 95 instances)\", \"xaxis\": {\"title\": \"Epoch\"}, \"yaxis\": {\"title\": \"Cost\"}},\n",
       "            {\"showLink\": true, \"linkText\": \"Export to plot.ly\"}\n",
       "        ).then(function () {return Plotly.addFrames('1a55fb75-76f1-45d2-9367-5cc7d70cb1ed',{});}).then(function(){Plotly.animate('1a55fb75-76f1-45d2-9367-5cc7d70cb1ed');})\n",
       "        });</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Set up plotly data\n",
    "trace0 = go.Scatter(\n",
    "    x = epoch_lst,\n",
    "    y = training_cost9b,\n",
    "    mode = 'lines',\n",
    "    name = 'LogLikelihood - Reg: Default, drop:0.1'\n",
    ")\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x = epoch_lst,\n",
    "    y = training_cost10b,\n",
    "    mode = 'lines',\n",
    "    name = 'LogLikelihood - Reg: Default, drop:0.5'\n",
    ")\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "    x = epoch_lst,\n",
    "    y = training_cost11b,\n",
    "    mode = 'lines',\n",
    "    name = 'LogLikelihood - Reg: L2, lmbda:3.0, drop:0.1'\n",
    ")\n",
    "\n",
    "trace3 = go.Scatter(\n",
    "    x = epoch_lst,\n",
    "    y = training_cost12b,\n",
    "    mode = 'lines',\n",
    "    name = 'LogLikelihood - Reg: L2, lmbda:3.0, drop:0.5'\n",
    ")\n",
    "\n",
    "data = [trace0, trace1, trace2, trace3]\n",
    "\n",
    "# Edit the layout\n",
    "layout = dict(title = 'Cost: hidden: ReLU / Output: Softmax / Network: 4-20-7-3 (Train: 95 instances)',\n",
    "              xaxis = dict(title = 'Epoch'),\n",
    "              yaxis = dict(title = 'Cost'),\n",
    "              )\n",
    "\n",
    "#Plot \n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "lines",
         "name": "LogLikelihood - Reg: Default, drop:0.1",
         "type": "scatter",
         "uid": "78ca98a8-d34d-11e8-b0b1-a820660c6deb",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          64,
          63,
          71,
          65,
          64,
          67,
          72,
          74,
          71,
          86,
          81,
          80,
          92,
          83,
          75,
          93,
          81,
          86,
          84,
          88,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          33,
          65,
          65,
          65,
          65,
          65,
          65,
          85,
          87,
          65,
          65,
          85,
          65,
          73,
          89,
          65,
          81,
          92,
          65,
          77,
          80,
          80,
          85,
          78,
          82,
          86,
          90,
          84,
          92,
          84,
          93,
          81,
          80,
          86,
          67,
          85,
          91,
          78,
          65,
          83,
          83,
          83,
          89,
          80,
          81,
          72,
          89,
          80,
          87,
          82,
          83,
          88,
          81,
          90,
          93,
          80,
          87,
          89,
          86,
          90,
          93,
          87,
          87,
          83,
          93,
          85,
          85,
          83,
          94,
          77,
          83,
          84,
          87,
          89,
          87,
          88,
          90,
          88,
          89,
          87,
          94,
          90,
          90,
          87,
          92,
          88,
          94,
          93,
          94,
          89,
          83,
          87,
          87,
          89,
          85,
          89,
          92,
          87,
          93,
          71,
          89,
          88,
          86,
          90,
          85,
          94,
          93,
          82,
          93,
          91,
          90,
          80,
          87,
          93,
          93,
          87,
          87,
          88,
          93,
          87,
          87,
          94,
          85,
          86
         ]
        },
        {
         "mode": "lines",
         "name": "LogLikelihood - Reg: Default, drop:0.5",
         "type": "scatter",
         "uid": "78ca9b48-d34d-11e8-ab33-a820660c6deb",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          32,
          32,
          32,
          33,
          33,
          32,
          32,
          32,
          32,
          32,
          33,
          32,
          33,
          33,
          40,
          64,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          87,
          65,
          70,
          88,
          65,
          65,
          65,
          71,
          67,
          72,
          65,
          66,
          65,
          75,
          73,
          82,
          66,
          66,
          65,
          88,
          66,
          75,
          92,
          65,
          65,
          65,
          73,
          78,
          65,
          80,
          89,
          86,
          73,
          66,
          90,
          65,
          65,
          65,
          76,
          80,
          90,
          65,
          69,
          88,
          73,
          77,
          71,
          88,
          91,
          65,
          68,
          65,
          65,
          86,
          71,
          65,
          92,
          92,
          83,
          65,
          79,
          89,
          73,
          83,
          86,
          69,
          65,
          84,
          84,
          84,
          89,
          83,
          86,
          88,
          84,
          87,
          65,
          65,
          92,
          79,
          84,
          66,
          71,
          85,
          88,
          89,
          65,
          84,
          86,
          82,
          65,
          81,
          81,
          65,
          81,
          88,
          92,
          84,
          70,
          93,
          81,
          88,
          92,
          84,
          66,
          89,
          90,
          68,
          75,
          88,
          90,
          84,
          90,
          81,
          66,
          65,
          89,
          93,
          65,
          65,
          80,
          92,
          86,
          80,
          93,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65
         ]
        },
        {
         "mode": "lines",
         "name": "LogLikelihood - Reg: L2, lmbda:3.0, drop:0.1",
         "type": "scatter",
         "uid": "78ca9ce2-d34d-11e8-89b4-a820660c6deb",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          62,
          80,
          65,
          65,
          63,
          75,
          65,
          66,
          64,
          64,
          87,
          66,
          64,
          65,
          64,
          76,
          64,
          74,
          65,
          64,
          64,
          64,
          64,
          63,
          64,
          64,
          89,
          81,
          77,
          65,
          65,
          65,
          65,
          64,
          64,
          69,
          90,
          64,
          88,
          65,
          62,
          65,
          65,
          63,
          64,
          73,
          59,
          70,
          67,
          72,
          88,
          69,
          64,
          64,
          63,
          86,
          63,
          64,
          72,
          65,
          84,
          64,
          64,
          83,
          65,
          64,
          65,
          85,
          64,
          64,
          64,
          64,
          63,
          63,
          65,
          65,
          63,
          65,
          64,
          76,
          65,
          62,
          66,
          62,
          75,
          61,
          65,
          63,
          63,
          66,
          72,
          63,
          72,
          89,
          62,
          63,
          60,
          65,
          59,
          64,
          89,
          63,
          63,
          65,
          65,
          66,
          92,
          65,
          73,
          65,
          74,
          62,
          67,
          51,
          91,
          63,
          62,
          83,
          76,
          67,
          67,
          91,
          52,
          64,
          56,
          66,
          66,
          60,
          66,
          65,
          66,
          65,
          81,
          90,
          64,
          58,
          65,
          66,
          65,
          66,
          84,
          65,
          90,
          75,
          57,
          63,
          65,
          65,
          90,
          64,
          65,
          72,
          65,
          83,
          80,
          64,
          89,
          65,
          66,
          89,
          65,
          64,
          82,
          90,
          64,
          66,
          84,
          64,
          65,
          74,
          66,
          71,
          71,
          65,
          63,
          65,
          86,
          66,
          90,
          74,
          65,
          65,
          67,
          90,
          62,
          92,
          88,
          57,
          64,
          76,
          63,
          69,
          83,
          65,
          68,
          66,
          67,
          73,
          58,
          75
         ]
        },
        {
         "mode": "lines",
         "name": "LogLikelihood - Reg: L2, lmbda:3.0, drop:0.5",
         "type": "scatter",
         "uid": "78ca9e5e-d34d-11e8-ac13-a820660c6deb",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          32,
          32,
          32,
          55,
          63,
          3,
          35,
          63,
          60,
          65,
          65,
          63,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          64,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          79,
          69,
          66,
          65,
          65,
          64,
          65,
          65,
          64,
          64,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          64,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          64,
          65,
          65,
          65,
          65,
          64,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          71,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          64,
          65,
          64,
          64,
          65,
          65,
          64,
          65,
          65,
          65,
          65,
          65,
          65,
          64,
          65,
          65,
          65,
          33,
          64,
          75,
          65,
          64,
          63,
          65,
          66,
          65,
          65,
          64,
          65,
          65,
          34,
          65,
          65,
          65,
          63,
          63,
          65,
          63,
          65,
          65,
          65,
          64,
          65,
          65,
          65,
          64,
          64,
          65,
          65,
          67,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          64,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          64,
          65,
          76,
          65,
          65,
          64,
          64,
          65
         ]
        }
       ],
       "layout": {
        "title": "Accuracy: Hidden: ReLU / Output: Softmax / Network: 4-20-7-3 (Train: 95 instances)",
        "xaxis": {
         "title": "Epoch"
        },
        "yaxis": {
         "title": "Accuracy"
        }
       }
      },
      "text/html": [
       "<div id=\"ff0bd7d6-63e0-4272-a4e2-8c22ab5dfeb5\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "        Plotly.plot(\n",
       "            'ff0bd7d6-63e0-4272-a4e2-8c22ab5dfeb5',\n",
       "            [{\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: Default, drop:0.1\", \"x\": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 251.0, 252.0, 253.0, 254.0, 255.0, 256.0, 257.0, 258.0, 259.0, 260.0, 261.0, 262.0, 263.0, 264.0, 265.0, 266.0, 267.0, 268.0, 269.0, 270.0, 271.0, 272.0, 273.0, 274.0, 275.0, 276.0, 277.0, 278.0, 279.0, 280.0, 281.0, 282.0, 283.0, 284.0, 285.0, 286.0, 287.0, 288.0, 289.0, 290.0, 291.0, 292.0, 293.0, 294.0, 295.0, 296.0, 297.0, 298.0, 299.0, 300.0, 301.0, 302.0, 303.0, 304.0, 305.0, 306.0, 307.0, 308.0, 309.0, 310.0, 311.0, 312.0, 313.0, 314.0, 315.0, 316.0, 317.0, 318.0, 319.0, 320.0, 321.0, 322.0, 323.0, 324.0, 325.0, 326.0, 327.0, 328.0, 329.0, 330.0, 331.0, 332.0, 333.0, 334.0, 335.0, 336.0, 337.0, 338.0, 339.0, 340.0, 341.0, 342.0, 343.0, 344.0, 345.0, 346.0, 347.0, 348.0, 349.0, 350.0, 351.0, 352.0, 353.0, 354.0, 355.0, 356.0, 357.0, 358.0, 359.0, 360.0, 361.0, 362.0, 363.0, 364.0, 365.0, 366.0, 367.0, 368.0, 369.0, 370.0, 371.0, 372.0, 373.0, 374.0, 375.0, 376.0, 377.0, 378.0, 379.0, 380.0, 381.0, 382.0, 383.0, 384.0, 385.0, 386.0, 387.0, 388.0, 389.0, 390.0, 391.0, 392.0, 393.0, 394.0, 395.0, 396.0, 397.0, 398.0, 399.0, 400.0, 401.0, 402.0, 403.0, 404.0, 405.0, 406.0, 407.0, 408.0, 409.0, 410.0, 411.0, 412.0, 413.0, 414.0, 415.0, 416.0, 417.0, 418.0, 419.0, 420.0, 421.0, 422.0, 423.0, 424.0, 425.0, 426.0, 427.0, 428.0, 429.0, 430.0, 431.0, 432.0, 433.0, 434.0, 435.0, 436.0, 437.0, 438.0, 439.0, 440.0, 441.0, 442.0, 443.0, 444.0, 445.0, 446.0, 447.0, 448.0, 449.0, 450.0, 451.0, 452.0, 453.0, 454.0, 455.0, 456.0, 457.0, 458.0, 459.0, 460.0, 461.0, 462.0, 463.0, 464.0, 465.0, 466.0, 467.0, 468.0, 469.0, 470.0, 471.0, 472.0, 473.0, 474.0, 475.0, 476.0, 477.0, 478.0, 479.0, 480.0, 481.0, 482.0, 483.0, 484.0, 485.0, 486.0, 487.0, 488.0, 489.0, 490.0, 491.0, 492.0, 493.0, 494.0, 495.0, 496.0, 497.0, 498.0, 499.0], \"y\": [64, 63, 71, 65, 64, 67, 72, 74, 71, 86, 81, 80, 92, 83, 75, 93, 81, 86, 84, 88, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 33, 65, 65, 65, 65, 65, 65, 85, 87, 65, 65, 85, 65, 73, 89, 65, 81, 92, 65, 77, 80, 80, 85, 78, 82, 86, 90, 84, 92, 84, 93, 81, 80, 86, 67, 85, 91, 78, 65, 83, 83, 83, 89, 80, 81, 72, 89, 80, 87, 82, 83, 88, 81, 90, 93, 80, 87, 89, 86, 90, 93, 87, 87, 83, 93, 85, 85, 83, 94, 77, 83, 84, 87, 89, 87, 88, 90, 88, 89, 87, 94, 90, 90, 87, 92, 88, 94, 93, 94, 89, 83, 87, 87, 89, 85, 89, 92, 87, 93, 71, 89, 88, 86, 90, 85, 94, 93, 82, 93, 91, 90, 80, 87, 93, 93, 87, 87, 88, 93, 87, 87, 94, 85, 86], \"type\": \"scatter\", \"uid\": \"78ca98a8-d34d-11e8-b0b1-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: Default, drop:0.5\", \"x\": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 251.0, 252.0, 253.0, 254.0, 255.0, 256.0, 257.0, 258.0, 259.0, 260.0, 261.0, 262.0, 263.0, 264.0, 265.0, 266.0, 267.0, 268.0, 269.0, 270.0, 271.0, 272.0, 273.0, 274.0, 275.0, 276.0, 277.0, 278.0, 279.0, 280.0, 281.0, 282.0, 283.0, 284.0, 285.0, 286.0, 287.0, 288.0, 289.0, 290.0, 291.0, 292.0, 293.0, 294.0, 295.0, 296.0, 297.0, 298.0, 299.0, 300.0, 301.0, 302.0, 303.0, 304.0, 305.0, 306.0, 307.0, 308.0, 309.0, 310.0, 311.0, 312.0, 313.0, 314.0, 315.0, 316.0, 317.0, 318.0, 319.0, 320.0, 321.0, 322.0, 323.0, 324.0, 325.0, 326.0, 327.0, 328.0, 329.0, 330.0, 331.0, 332.0, 333.0, 334.0, 335.0, 336.0, 337.0, 338.0, 339.0, 340.0, 341.0, 342.0, 343.0, 344.0, 345.0, 346.0, 347.0, 348.0, 349.0, 350.0, 351.0, 352.0, 353.0, 354.0, 355.0, 356.0, 357.0, 358.0, 359.0, 360.0, 361.0, 362.0, 363.0, 364.0, 365.0, 366.0, 367.0, 368.0, 369.0, 370.0, 371.0, 372.0, 373.0, 374.0, 375.0, 376.0, 377.0, 378.0, 379.0, 380.0, 381.0, 382.0, 383.0, 384.0, 385.0, 386.0, 387.0, 388.0, 389.0, 390.0, 391.0, 392.0, 393.0, 394.0, 395.0, 396.0, 397.0, 398.0, 399.0, 400.0, 401.0, 402.0, 403.0, 404.0, 405.0, 406.0, 407.0, 408.0, 409.0, 410.0, 411.0, 412.0, 413.0, 414.0, 415.0, 416.0, 417.0, 418.0, 419.0, 420.0, 421.0, 422.0, 423.0, 424.0, 425.0, 426.0, 427.0, 428.0, 429.0, 430.0, 431.0, 432.0, 433.0, 434.0, 435.0, 436.0, 437.0, 438.0, 439.0, 440.0, 441.0, 442.0, 443.0, 444.0, 445.0, 446.0, 447.0, 448.0, 449.0, 450.0, 451.0, 452.0, 453.0, 454.0, 455.0, 456.0, 457.0, 458.0, 459.0, 460.0, 461.0, 462.0, 463.0, 464.0, 465.0, 466.0, 467.0, 468.0, 469.0, 470.0, 471.0, 472.0, 473.0, 474.0, 475.0, 476.0, 477.0, 478.0, 479.0, 480.0, 481.0, 482.0, 483.0, 484.0, 485.0, 486.0, 487.0, 488.0, 489.0, 490.0, 491.0, 492.0, 493.0, 494.0, 495.0, 496.0, 497.0, 498.0, 499.0], \"y\": [32, 32, 32, 33, 33, 32, 32, 32, 32, 32, 33, 32, 33, 33, 40, 64, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 87, 65, 70, 88, 65, 65, 65, 71, 67, 72, 65, 66, 65, 75, 73, 82, 66, 66, 65, 88, 66, 75, 92, 65, 65, 65, 73, 78, 65, 80, 89, 86, 73, 66, 90, 65, 65, 65, 76, 80, 90, 65, 69, 88, 73, 77, 71, 88, 91, 65, 68, 65, 65, 86, 71, 65, 92, 92, 83, 65, 79, 89, 73, 83, 86, 69, 65, 84, 84, 84, 89, 83, 86, 88, 84, 87, 65, 65, 92, 79, 84, 66, 71, 85, 88, 89, 65, 84, 86, 82, 65, 81, 81, 65, 81, 88, 92, 84, 70, 93, 81, 88, 92, 84, 66, 89, 90, 68, 75, 88, 90, 84, 90, 81, 66, 65, 89, 93, 65, 65, 80, 92, 86, 80, 93, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65], \"type\": \"scatter\", \"uid\": \"78ca9b48-d34d-11e8-ab33-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: L2, lmbda:3.0, drop:0.1\", \"x\": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 251.0, 252.0, 253.0, 254.0, 255.0, 256.0, 257.0, 258.0, 259.0, 260.0, 261.0, 262.0, 263.0, 264.0, 265.0, 266.0, 267.0, 268.0, 269.0, 270.0, 271.0, 272.0, 273.0, 274.0, 275.0, 276.0, 277.0, 278.0, 279.0, 280.0, 281.0, 282.0, 283.0, 284.0, 285.0, 286.0, 287.0, 288.0, 289.0, 290.0, 291.0, 292.0, 293.0, 294.0, 295.0, 296.0, 297.0, 298.0, 299.0, 300.0, 301.0, 302.0, 303.0, 304.0, 305.0, 306.0, 307.0, 308.0, 309.0, 310.0, 311.0, 312.0, 313.0, 314.0, 315.0, 316.0, 317.0, 318.0, 319.0, 320.0, 321.0, 322.0, 323.0, 324.0, 325.0, 326.0, 327.0, 328.0, 329.0, 330.0, 331.0, 332.0, 333.0, 334.0, 335.0, 336.0, 337.0, 338.0, 339.0, 340.0, 341.0, 342.0, 343.0, 344.0, 345.0, 346.0, 347.0, 348.0, 349.0, 350.0, 351.0, 352.0, 353.0, 354.0, 355.0, 356.0, 357.0, 358.0, 359.0, 360.0, 361.0, 362.0, 363.0, 364.0, 365.0, 366.0, 367.0, 368.0, 369.0, 370.0, 371.0, 372.0, 373.0, 374.0, 375.0, 376.0, 377.0, 378.0, 379.0, 380.0, 381.0, 382.0, 383.0, 384.0, 385.0, 386.0, 387.0, 388.0, 389.0, 390.0, 391.0, 392.0, 393.0, 394.0, 395.0, 396.0, 397.0, 398.0, 399.0, 400.0, 401.0, 402.0, 403.0, 404.0, 405.0, 406.0, 407.0, 408.0, 409.0, 410.0, 411.0, 412.0, 413.0, 414.0, 415.0, 416.0, 417.0, 418.0, 419.0, 420.0, 421.0, 422.0, 423.0, 424.0, 425.0, 426.0, 427.0, 428.0, 429.0, 430.0, 431.0, 432.0, 433.0, 434.0, 435.0, 436.0, 437.0, 438.0, 439.0, 440.0, 441.0, 442.0, 443.0, 444.0, 445.0, 446.0, 447.0, 448.0, 449.0, 450.0, 451.0, 452.0, 453.0, 454.0, 455.0, 456.0, 457.0, 458.0, 459.0, 460.0, 461.0, 462.0, 463.0, 464.0, 465.0, 466.0, 467.0, 468.0, 469.0, 470.0, 471.0, 472.0, 473.0, 474.0, 475.0, 476.0, 477.0, 478.0, 479.0, 480.0, 481.0, 482.0, 483.0, 484.0, 485.0, 486.0, 487.0, 488.0, 489.0, 490.0, 491.0, 492.0, 493.0, 494.0, 495.0, 496.0, 497.0, 498.0, 499.0], \"y\": [62, 80, 65, 65, 63, 75, 65, 66, 64, 64, 87, 66, 64, 65, 64, 76, 64, 74, 65, 64, 64, 64, 64, 63, 64, 64, 89, 81, 77, 65, 65, 65, 65, 64, 64, 69, 90, 64, 88, 65, 62, 65, 65, 63, 64, 73, 59, 70, 67, 72, 88, 69, 64, 64, 63, 86, 63, 64, 72, 65, 84, 64, 64, 83, 65, 64, 65, 85, 64, 64, 64, 64, 63, 63, 65, 65, 63, 65, 64, 76, 65, 62, 66, 62, 75, 61, 65, 63, 63, 66, 72, 63, 72, 89, 62, 63, 60, 65, 59, 64, 89, 63, 63, 65, 65, 66, 92, 65, 73, 65, 74, 62, 67, 51, 91, 63, 62, 83, 76, 67, 67, 91, 52, 64, 56, 66, 66, 60, 66, 65, 66, 65, 81, 90, 64, 58, 65, 66, 65, 66, 84, 65, 90, 75, 57, 63, 65, 65, 90, 64, 65, 72, 65, 83, 80, 64, 89, 65, 66, 89, 65, 64, 82, 90, 64, 66, 84, 64, 65, 74, 66, 71, 71, 65, 63, 65, 86, 66, 90, 74, 65, 65, 67, 90, 62, 92, 88, 57, 64, 76, 63, 69, 83, 65, 68, 66, 67, 73, 58, 75], \"type\": \"scatter\", \"uid\": \"78ca9ce2-d34d-11e8-89b4-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: L2, lmbda:3.0, drop:0.5\", \"x\": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 251.0, 252.0, 253.0, 254.0, 255.0, 256.0, 257.0, 258.0, 259.0, 260.0, 261.0, 262.0, 263.0, 264.0, 265.0, 266.0, 267.0, 268.0, 269.0, 270.0, 271.0, 272.0, 273.0, 274.0, 275.0, 276.0, 277.0, 278.0, 279.0, 280.0, 281.0, 282.0, 283.0, 284.0, 285.0, 286.0, 287.0, 288.0, 289.0, 290.0, 291.0, 292.0, 293.0, 294.0, 295.0, 296.0, 297.0, 298.0, 299.0, 300.0, 301.0, 302.0, 303.0, 304.0, 305.0, 306.0, 307.0, 308.0, 309.0, 310.0, 311.0, 312.0, 313.0, 314.0, 315.0, 316.0, 317.0, 318.0, 319.0, 320.0, 321.0, 322.0, 323.0, 324.0, 325.0, 326.0, 327.0, 328.0, 329.0, 330.0, 331.0, 332.0, 333.0, 334.0, 335.0, 336.0, 337.0, 338.0, 339.0, 340.0, 341.0, 342.0, 343.0, 344.0, 345.0, 346.0, 347.0, 348.0, 349.0, 350.0, 351.0, 352.0, 353.0, 354.0, 355.0, 356.0, 357.0, 358.0, 359.0, 360.0, 361.0, 362.0, 363.0, 364.0, 365.0, 366.0, 367.0, 368.0, 369.0, 370.0, 371.0, 372.0, 373.0, 374.0, 375.0, 376.0, 377.0, 378.0, 379.0, 380.0, 381.0, 382.0, 383.0, 384.0, 385.0, 386.0, 387.0, 388.0, 389.0, 390.0, 391.0, 392.0, 393.0, 394.0, 395.0, 396.0, 397.0, 398.0, 399.0, 400.0, 401.0, 402.0, 403.0, 404.0, 405.0, 406.0, 407.0, 408.0, 409.0, 410.0, 411.0, 412.0, 413.0, 414.0, 415.0, 416.0, 417.0, 418.0, 419.0, 420.0, 421.0, 422.0, 423.0, 424.0, 425.0, 426.0, 427.0, 428.0, 429.0, 430.0, 431.0, 432.0, 433.0, 434.0, 435.0, 436.0, 437.0, 438.0, 439.0, 440.0, 441.0, 442.0, 443.0, 444.0, 445.0, 446.0, 447.0, 448.0, 449.0, 450.0, 451.0, 452.0, 453.0, 454.0, 455.0, 456.0, 457.0, 458.0, 459.0, 460.0, 461.0, 462.0, 463.0, 464.0, 465.0, 466.0, 467.0, 468.0, 469.0, 470.0, 471.0, 472.0, 473.0, 474.0, 475.0, 476.0, 477.0, 478.0, 479.0, 480.0, 481.0, 482.0, 483.0, 484.0, 485.0, 486.0, 487.0, 488.0, 489.0, 490.0, 491.0, 492.0, 493.0, 494.0, 495.0, 496.0, 497.0, 498.0, 499.0], \"y\": [32, 32, 32, 55, 63, 3, 35, 63, 60, 65, 65, 63, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 64, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 79, 69, 66, 65, 65, 64, 65, 65, 64, 64, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 64, 65, 65, 65, 65, 65, 65, 65, 65, 65, 64, 65, 65, 65, 65, 64, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 71, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 64, 65, 64, 64, 65, 65, 64, 65, 65, 65, 65, 65, 65, 64, 65, 65, 65, 33, 64, 75, 65, 64, 63, 65, 66, 65, 65, 64, 65, 65, 34, 65, 65, 65, 63, 63, 65, 63, 65, 65, 65, 64, 65, 65, 65, 64, 64, 65, 65, 67, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 64, 65, 65, 65, 65, 65, 65, 65, 64, 65, 76, 65, 65, 64, 64, 65], \"type\": \"scatter\", \"uid\": \"78ca9e5e-d34d-11e8-ac13-a820660c6deb\"}],\n",
       "            {\"title\": \"Accuracy: Hidden: ReLU / Output: Softmax / Network: 4-20-7-3 (Train: 95 instances)\", \"xaxis\": {\"title\": \"Epoch\"}, \"yaxis\": {\"title\": \"Accuracy\"}},\n",
       "            {\"showLink\": true, \"linkText\": \"Export to plot.ly\"}\n",
       "        ).then(function () {return Plotly.addFrames('ff0bd7d6-63e0-4272-a4e2-8c22ab5dfeb5',{});}).then(function(){Plotly.animate('ff0bd7d6-63e0-4272-a4e2-8c22ab5dfeb5');})\n",
       "        });</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"ff0bd7d6-63e0-4272-a4e2-8c22ab5dfeb5\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "        Plotly.plot(\n",
       "            'ff0bd7d6-63e0-4272-a4e2-8c22ab5dfeb5',\n",
       "            [{\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: Default, drop:0.1\", \"x\": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 251.0, 252.0, 253.0, 254.0, 255.0, 256.0, 257.0, 258.0, 259.0, 260.0, 261.0, 262.0, 263.0, 264.0, 265.0, 266.0, 267.0, 268.0, 269.0, 270.0, 271.0, 272.0, 273.0, 274.0, 275.0, 276.0, 277.0, 278.0, 279.0, 280.0, 281.0, 282.0, 283.0, 284.0, 285.0, 286.0, 287.0, 288.0, 289.0, 290.0, 291.0, 292.0, 293.0, 294.0, 295.0, 296.0, 297.0, 298.0, 299.0, 300.0, 301.0, 302.0, 303.0, 304.0, 305.0, 306.0, 307.0, 308.0, 309.0, 310.0, 311.0, 312.0, 313.0, 314.0, 315.0, 316.0, 317.0, 318.0, 319.0, 320.0, 321.0, 322.0, 323.0, 324.0, 325.0, 326.0, 327.0, 328.0, 329.0, 330.0, 331.0, 332.0, 333.0, 334.0, 335.0, 336.0, 337.0, 338.0, 339.0, 340.0, 341.0, 342.0, 343.0, 344.0, 345.0, 346.0, 347.0, 348.0, 349.0, 350.0, 351.0, 352.0, 353.0, 354.0, 355.0, 356.0, 357.0, 358.0, 359.0, 360.0, 361.0, 362.0, 363.0, 364.0, 365.0, 366.0, 367.0, 368.0, 369.0, 370.0, 371.0, 372.0, 373.0, 374.0, 375.0, 376.0, 377.0, 378.0, 379.0, 380.0, 381.0, 382.0, 383.0, 384.0, 385.0, 386.0, 387.0, 388.0, 389.0, 390.0, 391.0, 392.0, 393.0, 394.0, 395.0, 396.0, 397.0, 398.0, 399.0, 400.0, 401.0, 402.0, 403.0, 404.0, 405.0, 406.0, 407.0, 408.0, 409.0, 410.0, 411.0, 412.0, 413.0, 414.0, 415.0, 416.0, 417.0, 418.0, 419.0, 420.0, 421.0, 422.0, 423.0, 424.0, 425.0, 426.0, 427.0, 428.0, 429.0, 430.0, 431.0, 432.0, 433.0, 434.0, 435.0, 436.0, 437.0, 438.0, 439.0, 440.0, 441.0, 442.0, 443.0, 444.0, 445.0, 446.0, 447.0, 448.0, 449.0, 450.0, 451.0, 452.0, 453.0, 454.0, 455.0, 456.0, 457.0, 458.0, 459.0, 460.0, 461.0, 462.0, 463.0, 464.0, 465.0, 466.0, 467.0, 468.0, 469.0, 470.0, 471.0, 472.0, 473.0, 474.0, 475.0, 476.0, 477.0, 478.0, 479.0, 480.0, 481.0, 482.0, 483.0, 484.0, 485.0, 486.0, 487.0, 488.0, 489.0, 490.0, 491.0, 492.0, 493.0, 494.0, 495.0, 496.0, 497.0, 498.0, 499.0], \"y\": [64, 63, 71, 65, 64, 67, 72, 74, 71, 86, 81, 80, 92, 83, 75, 93, 81, 86, 84, 88, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 33, 65, 65, 65, 65, 65, 65, 85, 87, 65, 65, 85, 65, 73, 89, 65, 81, 92, 65, 77, 80, 80, 85, 78, 82, 86, 90, 84, 92, 84, 93, 81, 80, 86, 67, 85, 91, 78, 65, 83, 83, 83, 89, 80, 81, 72, 89, 80, 87, 82, 83, 88, 81, 90, 93, 80, 87, 89, 86, 90, 93, 87, 87, 83, 93, 85, 85, 83, 94, 77, 83, 84, 87, 89, 87, 88, 90, 88, 89, 87, 94, 90, 90, 87, 92, 88, 94, 93, 94, 89, 83, 87, 87, 89, 85, 89, 92, 87, 93, 71, 89, 88, 86, 90, 85, 94, 93, 82, 93, 91, 90, 80, 87, 93, 93, 87, 87, 88, 93, 87, 87, 94, 85, 86], \"type\": \"scatter\", \"uid\": \"78ca98a8-d34d-11e8-b0b1-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: Default, drop:0.5\", \"x\": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 251.0, 252.0, 253.0, 254.0, 255.0, 256.0, 257.0, 258.0, 259.0, 260.0, 261.0, 262.0, 263.0, 264.0, 265.0, 266.0, 267.0, 268.0, 269.0, 270.0, 271.0, 272.0, 273.0, 274.0, 275.0, 276.0, 277.0, 278.0, 279.0, 280.0, 281.0, 282.0, 283.0, 284.0, 285.0, 286.0, 287.0, 288.0, 289.0, 290.0, 291.0, 292.0, 293.0, 294.0, 295.0, 296.0, 297.0, 298.0, 299.0, 300.0, 301.0, 302.0, 303.0, 304.0, 305.0, 306.0, 307.0, 308.0, 309.0, 310.0, 311.0, 312.0, 313.0, 314.0, 315.0, 316.0, 317.0, 318.0, 319.0, 320.0, 321.0, 322.0, 323.0, 324.0, 325.0, 326.0, 327.0, 328.0, 329.0, 330.0, 331.0, 332.0, 333.0, 334.0, 335.0, 336.0, 337.0, 338.0, 339.0, 340.0, 341.0, 342.0, 343.0, 344.0, 345.0, 346.0, 347.0, 348.0, 349.0, 350.0, 351.0, 352.0, 353.0, 354.0, 355.0, 356.0, 357.0, 358.0, 359.0, 360.0, 361.0, 362.0, 363.0, 364.0, 365.0, 366.0, 367.0, 368.0, 369.0, 370.0, 371.0, 372.0, 373.0, 374.0, 375.0, 376.0, 377.0, 378.0, 379.0, 380.0, 381.0, 382.0, 383.0, 384.0, 385.0, 386.0, 387.0, 388.0, 389.0, 390.0, 391.0, 392.0, 393.0, 394.0, 395.0, 396.0, 397.0, 398.0, 399.0, 400.0, 401.0, 402.0, 403.0, 404.0, 405.0, 406.0, 407.0, 408.0, 409.0, 410.0, 411.0, 412.0, 413.0, 414.0, 415.0, 416.0, 417.0, 418.0, 419.0, 420.0, 421.0, 422.0, 423.0, 424.0, 425.0, 426.0, 427.0, 428.0, 429.0, 430.0, 431.0, 432.0, 433.0, 434.0, 435.0, 436.0, 437.0, 438.0, 439.0, 440.0, 441.0, 442.0, 443.0, 444.0, 445.0, 446.0, 447.0, 448.0, 449.0, 450.0, 451.0, 452.0, 453.0, 454.0, 455.0, 456.0, 457.0, 458.0, 459.0, 460.0, 461.0, 462.0, 463.0, 464.0, 465.0, 466.0, 467.0, 468.0, 469.0, 470.0, 471.0, 472.0, 473.0, 474.0, 475.0, 476.0, 477.0, 478.0, 479.0, 480.0, 481.0, 482.0, 483.0, 484.0, 485.0, 486.0, 487.0, 488.0, 489.0, 490.0, 491.0, 492.0, 493.0, 494.0, 495.0, 496.0, 497.0, 498.0, 499.0], \"y\": [32, 32, 32, 33, 33, 32, 32, 32, 32, 32, 33, 32, 33, 33, 40, 64, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 87, 65, 70, 88, 65, 65, 65, 71, 67, 72, 65, 66, 65, 75, 73, 82, 66, 66, 65, 88, 66, 75, 92, 65, 65, 65, 73, 78, 65, 80, 89, 86, 73, 66, 90, 65, 65, 65, 76, 80, 90, 65, 69, 88, 73, 77, 71, 88, 91, 65, 68, 65, 65, 86, 71, 65, 92, 92, 83, 65, 79, 89, 73, 83, 86, 69, 65, 84, 84, 84, 89, 83, 86, 88, 84, 87, 65, 65, 92, 79, 84, 66, 71, 85, 88, 89, 65, 84, 86, 82, 65, 81, 81, 65, 81, 88, 92, 84, 70, 93, 81, 88, 92, 84, 66, 89, 90, 68, 75, 88, 90, 84, 90, 81, 66, 65, 89, 93, 65, 65, 80, 92, 86, 80, 93, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65], \"type\": \"scatter\", \"uid\": \"78ca9b48-d34d-11e8-ab33-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: L2, lmbda:3.0, drop:0.1\", \"x\": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 251.0, 252.0, 253.0, 254.0, 255.0, 256.0, 257.0, 258.0, 259.0, 260.0, 261.0, 262.0, 263.0, 264.0, 265.0, 266.0, 267.0, 268.0, 269.0, 270.0, 271.0, 272.0, 273.0, 274.0, 275.0, 276.0, 277.0, 278.0, 279.0, 280.0, 281.0, 282.0, 283.0, 284.0, 285.0, 286.0, 287.0, 288.0, 289.0, 290.0, 291.0, 292.0, 293.0, 294.0, 295.0, 296.0, 297.0, 298.0, 299.0, 300.0, 301.0, 302.0, 303.0, 304.0, 305.0, 306.0, 307.0, 308.0, 309.0, 310.0, 311.0, 312.0, 313.0, 314.0, 315.0, 316.0, 317.0, 318.0, 319.0, 320.0, 321.0, 322.0, 323.0, 324.0, 325.0, 326.0, 327.0, 328.0, 329.0, 330.0, 331.0, 332.0, 333.0, 334.0, 335.0, 336.0, 337.0, 338.0, 339.0, 340.0, 341.0, 342.0, 343.0, 344.0, 345.0, 346.0, 347.0, 348.0, 349.0, 350.0, 351.0, 352.0, 353.0, 354.0, 355.0, 356.0, 357.0, 358.0, 359.0, 360.0, 361.0, 362.0, 363.0, 364.0, 365.0, 366.0, 367.0, 368.0, 369.0, 370.0, 371.0, 372.0, 373.0, 374.0, 375.0, 376.0, 377.0, 378.0, 379.0, 380.0, 381.0, 382.0, 383.0, 384.0, 385.0, 386.0, 387.0, 388.0, 389.0, 390.0, 391.0, 392.0, 393.0, 394.0, 395.0, 396.0, 397.0, 398.0, 399.0, 400.0, 401.0, 402.0, 403.0, 404.0, 405.0, 406.0, 407.0, 408.0, 409.0, 410.0, 411.0, 412.0, 413.0, 414.0, 415.0, 416.0, 417.0, 418.0, 419.0, 420.0, 421.0, 422.0, 423.0, 424.0, 425.0, 426.0, 427.0, 428.0, 429.0, 430.0, 431.0, 432.0, 433.0, 434.0, 435.0, 436.0, 437.0, 438.0, 439.0, 440.0, 441.0, 442.0, 443.0, 444.0, 445.0, 446.0, 447.0, 448.0, 449.0, 450.0, 451.0, 452.0, 453.0, 454.0, 455.0, 456.0, 457.0, 458.0, 459.0, 460.0, 461.0, 462.0, 463.0, 464.0, 465.0, 466.0, 467.0, 468.0, 469.0, 470.0, 471.0, 472.0, 473.0, 474.0, 475.0, 476.0, 477.0, 478.0, 479.0, 480.0, 481.0, 482.0, 483.0, 484.0, 485.0, 486.0, 487.0, 488.0, 489.0, 490.0, 491.0, 492.0, 493.0, 494.0, 495.0, 496.0, 497.0, 498.0, 499.0], \"y\": [62, 80, 65, 65, 63, 75, 65, 66, 64, 64, 87, 66, 64, 65, 64, 76, 64, 74, 65, 64, 64, 64, 64, 63, 64, 64, 89, 81, 77, 65, 65, 65, 65, 64, 64, 69, 90, 64, 88, 65, 62, 65, 65, 63, 64, 73, 59, 70, 67, 72, 88, 69, 64, 64, 63, 86, 63, 64, 72, 65, 84, 64, 64, 83, 65, 64, 65, 85, 64, 64, 64, 64, 63, 63, 65, 65, 63, 65, 64, 76, 65, 62, 66, 62, 75, 61, 65, 63, 63, 66, 72, 63, 72, 89, 62, 63, 60, 65, 59, 64, 89, 63, 63, 65, 65, 66, 92, 65, 73, 65, 74, 62, 67, 51, 91, 63, 62, 83, 76, 67, 67, 91, 52, 64, 56, 66, 66, 60, 66, 65, 66, 65, 81, 90, 64, 58, 65, 66, 65, 66, 84, 65, 90, 75, 57, 63, 65, 65, 90, 64, 65, 72, 65, 83, 80, 64, 89, 65, 66, 89, 65, 64, 82, 90, 64, 66, 84, 64, 65, 74, 66, 71, 71, 65, 63, 65, 86, 66, 90, 74, 65, 65, 67, 90, 62, 92, 88, 57, 64, 76, 63, 69, 83, 65, 68, 66, 67, 73, 58, 75], \"type\": \"scatter\", \"uid\": \"78ca9ce2-d34d-11e8-89b4-a820660c6deb\"}, {\"mode\": \"lines\", \"name\": \"LogLikelihood - Reg: L2, lmbda:3.0, drop:0.5\", \"x\": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 251.0, 252.0, 253.0, 254.0, 255.0, 256.0, 257.0, 258.0, 259.0, 260.0, 261.0, 262.0, 263.0, 264.0, 265.0, 266.0, 267.0, 268.0, 269.0, 270.0, 271.0, 272.0, 273.0, 274.0, 275.0, 276.0, 277.0, 278.0, 279.0, 280.0, 281.0, 282.0, 283.0, 284.0, 285.0, 286.0, 287.0, 288.0, 289.0, 290.0, 291.0, 292.0, 293.0, 294.0, 295.0, 296.0, 297.0, 298.0, 299.0, 300.0, 301.0, 302.0, 303.0, 304.0, 305.0, 306.0, 307.0, 308.0, 309.0, 310.0, 311.0, 312.0, 313.0, 314.0, 315.0, 316.0, 317.0, 318.0, 319.0, 320.0, 321.0, 322.0, 323.0, 324.0, 325.0, 326.0, 327.0, 328.0, 329.0, 330.0, 331.0, 332.0, 333.0, 334.0, 335.0, 336.0, 337.0, 338.0, 339.0, 340.0, 341.0, 342.0, 343.0, 344.0, 345.0, 346.0, 347.0, 348.0, 349.0, 350.0, 351.0, 352.0, 353.0, 354.0, 355.0, 356.0, 357.0, 358.0, 359.0, 360.0, 361.0, 362.0, 363.0, 364.0, 365.0, 366.0, 367.0, 368.0, 369.0, 370.0, 371.0, 372.0, 373.0, 374.0, 375.0, 376.0, 377.0, 378.0, 379.0, 380.0, 381.0, 382.0, 383.0, 384.0, 385.0, 386.0, 387.0, 388.0, 389.0, 390.0, 391.0, 392.0, 393.0, 394.0, 395.0, 396.0, 397.0, 398.0, 399.0, 400.0, 401.0, 402.0, 403.0, 404.0, 405.0, 406.0, 407.0, 408.0, 409.0, 410.0, 411.0, 412.0, 413.0, 414.0, 415.0, 416.0, 417.0, 418.0, 419.0, 420.0, 421.0, 422.0, 423.0, 424.0, 425.0, 426.0, 427.0, 428.0, 429.0, 430.0, 431.0, 432.0, 433.0, 434.0, 435.0, 436.0, 437.0, 438.0, 439.0, 440.0, 441.0, 442.0, 443.0, 444.0, 445.0, 446.0, 447.0, 448.0, 449.0, 450.0, 451.0, 452.0, 453.0, 454.0, 455.0, 456.0, 457.0, 458.0, 459.0, 460.0, 461.0, 462.0, 463.0, 464.0, 465.0, 466.0, 467.0, 468.0, 469.0, 470.0, 471.0, 472.0, 473.0, 474.0, 475.0, 476.0, 477.0, 478.0, 479.0, 480.0, 481.0, 482.0, 483.0, 484.0, 485.0, 486.0, 487.0, 488.0, 489.0, 490.0, 491.0, 492.0, 493.0, 494.0, 495.0, 496.0, 497.0, 498.0, 499.0], \"y\": [32, 32, 32, 55, 63, 3, 35, 63, 60, 65, 65, 63, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 64, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 79, 69, 66, 65, 65, 64, 65, 65, 64, 64, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 64, 65, 65, 65, 65, 65, 65, 65, 65, 65, 64, 65, 65, 65, 65, 64, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 71, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 64, 65, 64, 64, 65, 65, 64, 65, 65, 65, 65, 65, 65, 64, 65, 65, 65, 33, 64, 75, 65, 64, 63, 65, 66, 65, 65, 64, 65, 65, 34, 65, 65, 65, 63, 63, 65, 63, 65, 65, 65, 64, 65, 65, 65, 64, 64, 65, 65, 67, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 64, 65, 65, 65, 65, 65, 65, 65, 64, 65, 76, 65, 65, 64, 64, 65], \"type\": \"scatter\", \"uid\": \"78ca9e5e-d34d-11e8-ac13-a820660c6deb\"}],\n",
       "            {\"title\": \"Accuracy: Hidden: ReLU / Output: Softmax / Network: 4-20-7-3 (Train: 95 instances)\", \"xaxis\": {\"title\": \"Epoch\"}, \"yaxis\": {\"title\": \"Accuracy\"}},\n",
       "            {\"showLink\": true, \"linkText\": \"Export to plot.ly\"}\n",
       "        ).then(function () {return Plotly.addFrames('ff0bd7d6-63e0-4272-a4e2-8c22ab5dfeb5',{});}).then(function(){Plotly.animate('ff0bd7d6-63e0-4272-a4e2-8c22ab5dfeb5');})\n",
       "        });</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Set up plotly data\n",
    "trace0 = go.Scatter(\n",
    "    x = epoch_lst,\n",
    "    y = training_accuracy9b,\n",
    "    mode = 'lines',\n",
    "    name = 'LogLikelihood - Reg: Default, drop:0.1'\n",
    ")\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x = epoch_lst,\n",
    "    y = training_accuracy10b,\n",
    "    mode = 'lines',\n",
    "    name = 'LogLikelihood - Reg: Default, drop:0.5'\n",
    ")\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "    x = epoch_lst,\n",
    "    y = training_accuracy11b,\n",
    "    mode = 'lines',\n",
    "    name = 'LogLikelihood - Reg: L2, lmbda:3.0, drop:0.1'\n",
    ")\n",
    "\n",
    "trace3 = go.Scatter(\n",
    "    x = epoch_lst,\n",
    "    y = training_accuracy12b,\n",
    "    mode = 'lines',\n",
    "    name = 'LogLikelihood - Reg: L2, lmbda:3.0, drop:0.5'\n",
    ")\n",
    "\n",
    "data = [trace0, trace1, trace2, trace3]\n",
    "\n",
    "# Edit the layout\n",
    "layout = dict(title = 'Accuracy: Hidden: ReLU / Output: Softmax / Network: 4-20-7-3 (Train: 95 instances)',\n",
    "              xaxis = dict(title = 'Epoch'),\n",
    "              yaxis = dict(title = 'Accuracy'),\n",
    "              )\n",
    "\n",
    "#Plot \n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
